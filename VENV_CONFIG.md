# ğŸ”§ CPUè°ƒè¯•ç¯å¢ƒé…ç½®æŒ‡å—

> ä¸“ä¸ºä»£ç éªŒè¯è®¾è®¡çš„CPU-onlyç¯å¢ƒï¼Œé¿å…GPUé…ç½®å¤æ‚æ€§

## ğŸ¯ ç¯å¢ƒæ¦‚è¿°

| ç»„ä»¶ | ç‰ˆæœ¬ | ç”¨é€” | å¤‡æ³¨ |
|------|------|------|------|
| Python | 3.9-3.10 | è¿è¡Œç¯å¢ƒ | æ”¯æŒPyTorchå’ŒPaddlePaddle |
| PyTorch | 2.6.0+cpu | æ·±åº¦å­¦ä¹ æ¡†æ¶ | CPUä¸“ç”¨ç‰ˆæœ¬ |
| PaddlePaddle | 2.6.0+cpu | æ·±åº¦å­¦ä¹ æ¡†æ¶ | CPUä¸“ç”¨ç‰ˆæœ¬ |
| å†…å­˜éœ€æ±‚ | â‰¥ 4GB | è¿è¡Œè¦æ±‚ | æ”¯æŒbatch_size=32 |

## ğŸš€ ä¸€é”®å®‰è£…

### æ–¹æ¡ˆ1: Pythonè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨è - AI Agentå‹å¥½ï¼‰

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv ml-debug
source ml-debug/bin/activate  # Linux/Mac
# æˆ– ml-debug\Scripts\activate  # Windows

# å‡çº§pip
python -m pip install --upgrade pip

# å®‰è£…CPUç‰ˆæœ¬ä¾èµ–
pip install -r requirements-cpu.txt

# éªŒè¯å®‰è£…
python -c "import torch; print('âœ… PyTorch CPU OK')"
python -c "import paddle; print('âœ… PaddlePaddle CPU OK')"
```

### æ–¹æ¡ˆ2: Condaç¯å¢ƒï¼ˆå¯é€‰ï¼‰

```bash
# åˆ›å»ºå¹¶æ¿€æ´»ç¯å¢ƒ
conda create -n ml-debug python=3.10 -y
conda activate ml-debug

# å®‰è£…CPUç‰ˆæœ¬ä¾èµ–
pip install -r requirements-cpu.txt

# éªŒè¯å®‰è£…
python -c "import torch; print('âœ… PyTorch CPU OK')"
python -c "import paddle; print('âœ… PaddlePaddle CPU OK')"
```

## ğŸ“‹ è¯¦ç»†å®‰è£…æ­¥éª¤

### 1. Pythonç¯å¢ƒå‡†å¤‡ï¼ˆåŸºäºML.mdç‰ˆæœ¬çº¦æŸï¼‰

```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬ï¼ˆå‚è€ƒML.mdç‰ˆæœ¬å…¼å®¹æ€§ç« èŠ‚ï¼‰
python --version  # æœŸæœ›: Python 3.9-3.10ï¼ˆé¿å…3.11æµ‹è¯•ç‰ˆï¼‰

# éªŒè¯CPUæ¶æ„ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚å‚è€ƒï¼‰
python -c "import platform; print(f'CPU: {platform.processor()}')"

# æ£€æŸ¥å†…å­˜éœ€æ±‚ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚æ•°æ®ï¼‰
python -c "
import psutil
mem_gb = psutil.virtual_memory().total / 1024**3
print(f'ç³»ç»Ÿå†…å­˜: {mem_gb:.1f}GB (æœ€ä½è¦æ±‚: 4GB)')
if mem_gb < 4:
    print('âš ï¸ å†…å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨æ›´å°batch_size')
"

# æ›´æ–°pip
python -m pip install --upgrade pip setuptools wheel
```

### 2. PyTorch CPUå®‰è£…ï¼ˆåŸºäºML.mdç‰ˆæœ¬çŸ©é˜µï¼‰

```bash
# PyTorch CPUç‰ˆæœ¬ï¼ˆML.mdç‰ˆæœ¬å…¼å®¹æ€§ç« èŠ‚CUDA12.6å¯¹åº”ç‰ˆæœ¬ï¼‰
pip install torch==2.6.0+cpu torchvision==0.15.0+cpu torchaudio==2.0.0+cpu \
  --index-url https://download.pytorch.org/whl/cpu

# éªŒè¯å®‰è£…ï¼ˆML.mdéªŒè¯æ ‡å‡†ç« èŠ‚ï¼‰
python -c "
import torch
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')  # å¿…é¡»ä¸ºFalse
print(f'CPUçº¿ç¨‹æ•°: {torch.get_num_threads()}')

# æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚ï¼‰
import time
x = torch.randn(1000, 1000)
start = time.time()
y = torch.matmul(x, x)
elapsed = time.time() - start
print(f'CPUè®¡ç®—é€Ÿåº¦: {elapsed:.3f}s (å‚è€ƒå€¼: Intel i7-12700 ~0.1s)')
"
```

### 3. PaddlePaddle CPUå®‰è£…ï¼ˆåŸºäºML.mdç‰ˆæœ¬çŸ©é˜µï¼‰

```bash
# PaddlePaddle CPUç‰ˆæœ¬ï¼ˆML.mdç‰ˆæœ¬å…¼å®¹æ€§ç« èŠ‚CUDA12.6å¯¹åº”ç‰ˆæœ¬ï¼‰
pip install paddlepaddle==2.6.0 \
  -f https://www.paddlepaddle.org.cn/whl/linux/cpu-mkl/avx/stable.html

# éªŒè¯å®‰è£…ï¼ˆML.mdéªŒè¯æ ‡å‡†ç« èŠ‚ï¼‰
python -c "
import paddle
print(f'PaddlePaddleç‰ˆæœ¬: {paddle.__version__}')
print(f'GPUç¼–è¯‘: {paddle.is_compiled_with_cuda()}')  # å¿…é¡»ä¸ºFalse
print(f'CPUçº¿ç¨‹æ•°: {paddle.get_num_threads()}')

# æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆML.mdç¬¬274-277è¡Œï¼‰
import time
x = paddle.randn([1000, 1000])
start = time.time()
y = paddle.matmul(x, x)
elapsed = time.time() - start
print(f'PaddlePaddle CPUè®¡ç®—é€Ÿåº¦: {elapsed:.3f}s')
"
```

### 4. é€šç”¨ä¾èµ–å®‰è£…

```bash
# å®‰è£…é€šç”¨ä¾èµ–
pip install pytorch-lightning==2.0.0 \
  omegaconf==2.3.0 \
  torchmetrics==0.11.0 \
  scikit-learn==1.3.0 \
  matplotlib==3.7.0 \
  seaborn==0.12.0 \
  tensorboard==2.13.0 \
  wandb==0.15.0 \
  ipdb==0.13.13 \
  rich==13.4.0
```

## ğŸ§ª ç¯å¢ƒéªŒè¯

### 1. åŸºç¡€éªŒè¯

```bash
# åˆ›å»ºéªŒè¯è„šæœ¬
cat > validate_env.py << 'EOF'
import torch
import paddle
import pytorch_lightning as pl
import omegaconf
import torchmetrics

print("=== ç¯å¢ƒéªŒè¯æŠ¥å‘Š ===")
print(f"Pythonç‰ˆæœ¬: {torch.__import__('sys').version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"PaddlePaddleç‰ˆæœ¬: {paddle.__version__}")
print(f"PyTorch Lightningç‰ˆæœ¬: {pl.__version__}")
print(f"OmegaConfç‰ˆæœ¬: {omegaconf.__version__}")
print(f"TorchMetricsç‰ˆæœ¬: {torchmetrics.__version__}")

# æµ‹è¯•CPUè®¡ç®—
x = torch.randn(1000, 1000)
y = torch.matmul(x, x)
print(f"âœ… CPUè®¡ç®—æµ‹è¯•: {y.shape}")

# æµ‹è¯•PaddlePaddle
x_paddle = paddle.randn([1000, 1000])
y_paddle = paddle.matmul(x_paddle, x_paddle)
print(f"âœ… PaddlePaddleè®¡ç®—æµ‹è¯•: {y_paddle.shape}")

print("=== ç¯å¢ƒéªŒè¯å®Œæˆ ===")
EOF

# è¿è¡ŒéªŒè¯
python validate_env.py
```

### 2. é¡¹ç›®é›†æˆéªŒè¯

```bash
# éªŒè¯é¡¹ç›®å¯¼å…¥
python -c "
import sys
sys.path.append('.')
from src.models.pytorch.yolov10 import YOLOv10
from src.datasets.coco_detection import COCODetection
print('âœ… é¡¹ç›®æ¨¡å—å¯¼å…¥æˆåŠŸ')
"

# éªŒè¯é…ç½®æ–‡ä»¶
python -c "
from omegaconf import OmegaConf
cfg = OmegaConf.load('configs/config.yaml')
print('âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ')
print(f'æ¨¡å‹: {cfg.model.get("name", "yolov10")}')
print(f'æ•°æ®é›†: {cfg.data.get("name", "coco2017")}')
"
```

### 3. å¿«é€Ÿè®­ç»ƒæµ‹è¯•

```bash
# 1-epochå¿«é€Ÿæµ‹è¯•
python scripts/train.py \
  model=yolov10n \
  data=coco128 \
  trainer.max_epochs=1 \
  trainer.limit_train_batches=5 \
  trainer.limit_val_batches=5 \
  trainer.fast_dev_run=true

# éªŒè¯è®­ç»ƒç»“æœ
ls -la logs/lightning_logs/version_0/
```

## ğŸ” æ€§èƒ½ä¼˜åŒ–ï¼ˆåŸºäºML.mdåŸºå‡†æ•°æ®ï¼‰

### CPUæ€§èƒ½è°ƒä¼˜ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚å‚è€ƒï¼‰

```bash
# åŸºäºç¡¬ä»¶è‡ªåŠ¨è®¾ç½®çº¿ç¨‹æ•°ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚å‚è€ƒï¼‰
CPU_CORES=$(python -c "import multiprocessing; print(multiprocessing.cpu_count())")
OPTIMAL_THREADS=$((CPU_CORES / 2))  # é¿å…è¶…çº¿ç¨‹å½±å“

export OMP_NUM_THREADS=$OPTIMAL_THREADS
export MKL_NUM_THREADS=$OPTIMAL_THREADS

# PyTorch CPUä¼˜åŒ–ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚éªŒè¯æ ‡å‡†ï¼‰
python -c "
import torch
import time
torch.set_num_threads($OPTIMAL_THREADS)
torch.set_num_interop_threads($OPTIMAL_THREADS)

# æ€§èƒ½åŸºå‡†éªŒè¯
x = torch.randn(2000, 2000)
start = time.time()
y = torch.matmul(x, x)
elapsed = time.time() - start
print(f'ä¼˜åŒ–åCPUè®¡ç®—: {elapsed:.3f}s')
print(f'æ€§èƒ½å‚è€ƒå€¼: Intel i7-12700 ~0.4s/2000x2000çŸ©é˜µä¹˜æ³•')
"

# PaddlePaddle CPUä¼˜åŒ–
python -c "
import paddle
import time
paddle.set_device('cpu')
paddle.set_num_threads($OPTIMAL_THREADS)

# æ€§èƒ½åŸºå‡†éªŒè¯
x = paddle.randn([2000, 2000])
start = time.time()
y = paddle.matmul(x, x)
elapsed = time.time() - start
print(f'PaddlePaddleä¼˜åŒ–å: {elapsed:.3f}s')
"
```

### å†…å­˜ç®¡ç†ï¼ˆåŸºäºML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚ï¼‰

```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨ï¼ˆåŸºäºML.mdæ€§èƒ½åŸºå‡†ï¼‰
python -c "
import psutil
mem = psutil.virtual_memory()
print(f'å†…å­˜ä½¿ç”¨: {mem.percent}%')
print(f'å¯ç”¨å†…å­˜: {mem.available // 1024**3} GB')

# åŸºäºML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚
print('=== å†…å­˜éœ€æ±‚è¯„ä¼° ===')
print('CIFAR-10 + ResNet18: ~1GB')
print('ImageNet + ResNet50: ~2GB')  
print('COCO128 + YOLOv10: ~3GB')
print(f'å½“å‰å¯ç”¨: {mem.available // 1024**3}GB (å»ºè®®â‰¥4GB)')

# å†…å­˜ä¼˜åŒ–å»ºè®®
if mem.available < 4 * 1024**3:
    print('âš ï¸ å†…å­˜ç´§å¼ ï¼Œå»ºè®®ä½¿ç”¨æ›´å°batch_size')
    print('å»ºè®®: CIFAR-10 batch_size=16, ImageNet batch_size=8')
"
```

## ğŸš¨ å¸¸è§é—®é¢˜è§£å†³

### é—®é¢˜1: å®‰è£…å¤±è´¥
```bash
# é”™è¯¯: No matching distribution found
# è§£å†³æ–¹æ¡ˆï¼šæ›´æ–°pipå’Œsetuptools
python -m pip install --upgrade pip setuptools wheel

# é‡æ–°å®‰è£…
pip install --no-cache-dir -r requirements-cpu.txt
```

### é—®é¢˜2: ç‰ˆæœ¬å†²çª
```bash
# é”™è¯¯: Package version conflicts
# è§£å†³æ–¹æ¡ˆï¼šåˆ›å»ºå¹²å‡€ç¯å¢ƒ
# è™šæ‹Ÿç¯å¢ƒæ–¹æ¡ˆ
rm -rf ml-debug
python -m venv ml-debug
source ml-debug/bin/activate
pip install -r requirements-cpu.txt

# æˆ–Condaæ–¹æ¡ˆ
conda remove -n ml-debug --all -y
conda create -n ml-debug python=3.10 -y
conda activate ml-debug
pip install -r requirements-cpu.txt
```

### é—®é¢˜3: å¯¼å…¥é”™è¯¯
```bash
# é”™è¯¯: ImportError
# è§£å†³æ–¹æ¡ˆï¼šæ£€æŸ¥PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
python -c "import sys; print(sys.path)"
```

## ğŸ“Š èµ„æºä½¿ç”¨åŸºå‡†ï¼ˆåŸºäºML.mdç¬¬266-277è¡Œï¼‰

### å†…å­˜ä½¿ç”¨å‚è€ƒï¼ˆML.mdç¬¬266-277è¡ŒéªŒè¯æ•°æ®ï¼‰
| ä»»åŠ¡ç±»å‹ | Batch Size | å†…å­˜ä½¿ç”¨ | è®­ç»ƒæ—¶é—´/epoch | æ•°æ®æ¥æº |
|----------|------------|----------|----------------|----------|
| CIFAR-10 | 32 | ~1GB | ~45ç§’ | ML.mdç¬¬266è¡Œ |
| ImageNet | 32 | ~2GB | ~45åˆ†é’Ÿ | ML.mdç¬¬267è¡Œ |
| COCO128 | 16 | ~3GB | ~5åˆ†é’Ÿ | ML.mdç¬¬267è¡Œ |

### CPUæ€§èƒ½å‚è€ƒï¼ˆML.mdç¬¬274-277è¡ŒåŸºå‡†æ•°æ®ï¼‰
| CPUç±»å‹ | çº¿ç¨‹æ•° | CIFAR-10è®­ç»ƒæ—¶é—´ | ImageNetè®­ç»ƒæ—¶é—´ | æ•°æ®æ¥æº |
|---------|--------|------------------|------------------|----------|
| Intel i7-12700 | 8 | ~30ç§’/epoch | ~30åˆ†é’Ÿ/epoch | ML.mdç¬¬274è¡Œ |
| Apple M1 | 8 | ~25ç§’/epoch | ~25åˆ†é’Ÿ/epoch | ML.mdç¬¬274è¡Œ |
| AMD Ryzen 5800X | 8 | ~35ç§’/epoch | ~35åˆ†é’Ÿ/epoch | ML.mdç¬¬275è¡Œ |

## ğŸ”„ ç¯å¢ƒåˆ‡æ¢

### ä»CPUåˆ‡æ¢åˆ°GPUç¯å¢ƒ

```bash
# å¤‡ä»½CPUç¯å¢ƒï¼ˆè™šæ‹Ÿç¯å¢ƒï¼‰
source ml-debug/bin/activate
pip freeze > cpu-requirements.txt

# æˆ–Condaç¯å¢ƒå¤‡ä»½
conda env export > cpu-environment.yml

# åˆ›å»ºGPUç¯å¢ƒï¼ˆæ¨èè™šæ‹Ÿç¯å¢ƒï¼‰
python -m venv ml-gpu
source ml-gpu/bin/activate
pip install -r requirements-gpu.txt

# æˆ–Conda GPUç¯å¢ƒ
conda create -n ml-gpu python=3.10 -y
conda activate ml-gpu
pip install -r requirements-gpu.txt

# éªŒè¯GPUç¯å¢ƒ
python -c "
import torch
print(f'GPUå¯ç”¨: {torch.cuda.is_available()}')
print(f'GPUæ•°é‡: {torch.cuda.device_count()}')
"
```

## ğŸ“‹ ç¯å¢ƒæ£€æŸ¥æ¸…å•

### å®‰è£…éªŒè¯
- [ ] Python 3.9-3.10
- [ ] PyTorch CPUç‰ˆæœ¬
- [ ] PaddlePaddle CPUç‰ˆæœ¬
- [ ] æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ

### åŠŸèƒ½éªŒè¯
- [ ] åŸºç¡€å¯¼å…¥æµ‹è¯•
- [ ] é…ç½®æ–‡ä»¶åŠ è½½
- [ ] æ¨¡å‹å®šä¹‰æµ‹è¯•
- [ ] æ•°æ®é›†åŠ è½½
- [ ] 1-epochè®­ç»ƒæµ‹è¯•

### æ€§èƒ½éªŒè¯
- [ ] CPUçº¿ç¨‹è®¾ç½®æ­£ç¡®
- [ ] å†…å­˜ä½¿ç”¨åˆç†
- [ ] è®­ç»ƒé€Ÿåº¦è¾¾æ ‡

## ğŸ¯ ä¸‹ä¸€æ­¥

å®ŒæˆCPUç¯å¢ƒé…ç½®åï¼š
1. è¿è¡Œ [DEBUG_CODE.md](./DEBUG_CODE.md) è¿›è¡Œä»£ç éªŒè¯
2. é…ç½® [DOCKER_CONFIG.md](./DOCKER_CONFIG.md) è¿›è¡ŒGPUéƒ¨ç½²
3. æ›´æ–° [PROJECT_BUILD_LOG.md](./PROJECT_BUILD_LOG.md)

**æ³¨æ„**ï¼šé¡¹ç›®åˆ›å»ºå‰è¯·å…ˆå®ŒæˆCREATE.mdçš„think hardè§„åˆ’ï¼Œå°†ç»“æœå†™å…¥INITIAL.md

---
**é…ç½®æ—¶é—´**: ~5åˆ†é’Ÿ | **éªŒè¯æ—¶é—´**: ~2åˆ†é’Ÿ | **æ€»è®¡**: ~7åˆ†é’Ÿ