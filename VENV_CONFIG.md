# ğŸ”§ GPUè°ƒè¯•ç¯å¢ƒé…ç½®æŒ‡å—

> ä¸“ä¸ºGPUè®­ç»ƒéªŒè¯è®¾è®¡çš„é«˜æ€§èƒ½ç¯å¢ƒï¼Œæ”¯æŒPyTorchå’ŒPaddlePaddle GPUåŠ é€Ÿ

## ğŸ¯ ç¯å¢ƒæ¦‚è¿°

| ç»„ä»¶ | ç‰ˆæœ¬ | ç”¨é€” | å¤‡æ³¨ |
|------|------|------|------|
| Python | 3.9-3.10 | è¿è¡Œç¯å¢ƒ | æ”¯æŒPyTorchå’ŒPaddlePaddle |
| PyTorch | 2.4.1 | æ·±åº¦å­¦ä¹ æ¡†æ¶ | GPUåŠ é€Ÿç‰ˆæœ¬ |
| PaddlePaddle | 2.6.0+gpu | æ·±åº¦å­¦ä¹ æ¡†æ¶ | GPUåŠ é€Ÿç‰ˆæœ¬ |
| CUDA | 12.4.1 | GPUè®¡ç®— | ç¨³å®šå…¼å®¹ç‰ˆ |
| GPUéœ€æ±‚ | â‰¥ 6GBæ˜¾å­˜ | è®­ç»ƒè¦æ±‚ | RTX 3060ä»¥ä¸Š |
| å†…å­˜éœ€æ±‚ | â‰¥ 8GB | è¿è¡Œè¦æ±‚ | æ”¯æŒbatch_size=64 |

## ğŸš€ ä¸€é”®å®‰è£…

### æ–¹æ¡ˆ1: Pythonè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨è - AI Agentå‹å¥½ï¼‰

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv ml-gpu-debug
source ml-gpu-debug/bin/activate  # Linux/Mac
# æˆ– ml-gpu-debug\Scripts\activate  # Windows

# å‡çº§pip
python -m pip install --upgrade pip

# å®‰è£…GPUç‰ˆæœ¬ä¾èµ–
pip install -r requirements-gpu.txt

# éªŒè¯å®‰è£…
python -c "import torch; print('âœ… PyTorch GPU OK'); print(f'CUDA: {torch.version.cuda}')"
python -c "import paddle; print('âœ… PaddlePaddle GPU OK'); print(f'GPU: {paddle.is_compiled_with_cuda()}')"
```

### æ–¹æ¡ˆ2: Condaç¯å¢ƒï¼ˆå¯é€‰ï¼‰

```bash
# åˆ›å»ºå¹¶æ¿€æ´»ç¯å¢ƒ
conda create -n ml-gpu-debug python=3.10 -y
conda activate ml-gpu-debug

# å®‰è£…GPUç‰ˆæœ¬ä¾èµ–
pip install -r requirements-gpu.txt

# éªŒè¯å®‰è£…
python -c "import torch; print('âœ… PyTorch GPU OK'); print(f'CUDA: {torch.version.cuda}')"
python -c "import paddle; print('âœ… PaddlePaddle GPU OK'); print(f'GPU: {paddle.is_compiled_with_cuda()}')"
```

## ğŸ“‹ è¯¦ç»†å®‰è£…æ­¥éª¤

### 1. Pythonç¯å¢ƒå‡†å¤‡ï¼ˆåŸºäºML.mdç‰ˆæœ¬çº¦æŸï¼‰

```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬ï¼ˆå‚è€ƒML.mdç‰ˆæœ¬å…¼å®¹æ€§ç« èŠ‚ï¼‰
python --version  # æœŸæœ›: Python 3.9-3.10ï¼ˆé¿å…3.11æµ‹è¯•ç‰ˆï¼‰

# éªŒè¯GPUç¡¬ä»¶ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚æ•°æ®ï¼‰
python -c "
import subprocess
try:
    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
    if result.returncode == 0:
        print('âœ… NVIDIA GPUæ£€æµ‹æˆåŠŸ')
        print(result.stdout.split('\n')[8])
    else:
        print('âŒ æœªæ£€æµ‹åˆ°NVIDIA GPU')
except:
    print('âŒ nvidia-smiå‘½ä»¤ä¸å¯ç”¨')
"

# æ£€æŸ¥GPUå†…å­˜éœ€æ±‚ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚æ•°æ®ï¼‰
python -c "
import subprocess
import re
try:
    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'], 
                          capture_output=True, text=True)
    memory_mb = int(result.stdout.strip())
    memory_gb = memory_mb / 1024
    print(f'GPUå†…å­˜: {memory_gb:.1f}GB (æœ€ä½è¦æ±‚: 6GB)')
    if memory_gb < 6:
        print('âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨æ›´å°batch_size')
except:
    print('âŒ æ— æ³•æ£€æµ‹GPUå†…å­˜')
"

# æ›´æ–°pip
python -m pip install --upgrade pip setuptools wheel
```

### 2. PyTorch GPUå®‰è£…ï¼ˆåŸºäºML.mdç‰ˆæœ¬çŸ©é˜µï¼‰

```bash
# PyTorch GPUç‰ˆæœ¬ï¼ˆML.mdç‰ˆæœ¬å…¼å®¹æ€§ç« èŠ‚CUDA12.4å¯¹åº”ç‰ˆæœ¬ï¼‰
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 \
-i https://mirrors.aliyun.com/pypi/simple/

# éªŒè¯å®‰è£…ï¼ˆML.mdéªŒè¯æ ‡å‡†ç« èŠ‚ï¼‰
python -c "
import torch
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')  # å¿…é¡»ä¸ºTrue
print(f'GPUæ•°é‡: {torch.cuda.device_count()}')
print(f'GPUåç§°: {torch.cuda.get_device_name(0)}')

# GPUæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚ï¼‰
import time
if torch.cuda.is_available():
    x = torch.randn(1000, 1000).cuda()
    torch.cuda.synchronize()
    start = time.time()
    y = torch.matmul(x, x)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    print(f'GPUè®¡ç®—é€Ÿåº¦: {elapsed:.3f}s (å‚è€ƒå€¼: RTX 3060 ~0.002s)')
    print(f'GPUå†…å­˜: {torch.cuda.memory_allocated()/1024**3:.1f}GB')
"
```

### 3. PaddlePaddle GPUå®‰è£…ï¼ˆåŸºäºML.mdç‰ˆæœ¬çŸ©é˜µï¼‰

```bash
# PaddlePaddle GPUç‰ˆæœ¬ï¼ˆML.mdç‰ˆæœ¬å…¼å®¹æ€§ç« èŠ‚CUDA12.4å¯¹åº”ç‰ˆæœ¬ï¼‰
pip install paddlepaddle-gpu==2.6.0.post126 \
  -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

# éªŒè¯å®‰è£…ï¼ˆML.mdéªŒè¯æ ‡å‡†ç« èŠ‚ï¼‰
python -c "
import paddle
print(f'PaddlePaddleç‰ˆæœ¬: {paddle.__version__}')
print(f'GPUç¼–è¯‘: {paddle.is_compiled_with_cuda()}')  # å¿…é¡»ä¸ºTrue
print(f'GPUè®¾å¤‡: {paddle.device.get_device()}')

# GPUæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆML.mdç¬¬274-277è¡Œï¼‰
import time
if paddle.is_compiled_with_cuda():
    paddle.set_device('gpu:0')
    x = paddle.randn([1000, 1000])
    start = time.time()
    y = paddle.matmul(x, x)
    elapsed = time.time() - start
    print(f'PaddlePaddle GPUè®¡ç®—é€Ÿåº¦: {elapsed:.3f}s')
    print(f'GPUå†…å­˜: {paddle.device.cuda.max_memory_allocated()/1024**3:.1f}GB')
"
```

### 4. é€šç”¨ä¾èµ–å®‰è£…

```bash
# å®‰è£…é€šç”¨ä¾èµ–
pip install pytorch-lightning==2.0.0 \
  omegaconf==2.3.0 \
  torchmetrics==0.11.0 \
  scikit-learn==1.3.0 \
  matplotlib==3.7.0 \
  seaborn==0.12.0 \
  tensorboard==2.13.0 \
  wandb==0.15.0 \
  ipdb==0.13.13 \
  rich==13.4.0 -i https://mirrors.aliyun.com/pypi/simple/
```

## ğŸ§ª ç¯å¢ƒéªŒè¯

### 1. åŸºç¡€éªŒè¯

```bash
# åˆ›å»ºéªŒè¯è„šæœ¬
cat > validate_env.py << 'EOF'
import torch
import paddle
import pytorch_lightning as pl
import omegaconf
import torchmetrics

print("=== ç¯å¢ƒéªŒè¯æŠ¥å‘Š ===")
print(f"Pythonç‰ˆæœ¬: {torch.__import__('sys').version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"PaddlePaddleç‰ˆæœ¬: {paddle.__version__}")
print(f"PyTorch Lightningç‰ˆæœ¬: {pl.__version__}")
print(f"OmegaConfç‰ˆæœ¬: {omegaconf.__version__}")
print(f"TorchMetricsç‰ˆæœ¬: {torchmetrics.__version__}")

# æµ‹è¯•CPUè®¡ç®—
x = torch.randn(1000, 1000)
y = torch.matmul(x, x)
print(f"âœ… CPUè®¡ç®—æµ‹è¯•: {y.shape}")

# æµ‹è¯•PaddlePaddle
x_paddle = paddle.randn([1000, 1000])
y_paddle = paddle.matmul(x_paddle, x_paddle)
print(f"âœ… PaddlePaddleè®¡ç®—æµ‹è¯•: {y_paddle.shape}")

print("=== ç¯å¢ƒéªŒè¯å®Œæˆ ===")
EOF

# è¿è¡ŒéªŒè¯
python validate_env.py
```

### 2. é¡¹ç›®é›†æˆéªŒè¯

```bash
# éªŒè¯é¡¹ç›®å¯¼å…¥
python -c "
import sys
sys.path.append('.')
from src.models.pytorch.yolov10 import YOLOv10
from src.datasets.coco_detection import COCODetection
print('âœ… é¡¹ç›®æ¨¡å—å¯¼å…¥æˆåŠŸ')
"

# éªŒè¯é…ç½®æ–‡ä»¶
python -c "
from omegaconf import OmegaConf
cfg = OmegaConf.load('configs/config.yaml')
print('âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ')
print(f'æ¨¡å‹: {cfg.model.get("name", "yolov10")}')
print(f'æ•°æ®é›†: {cfg.data.get("name", "coco2017")}')
"
```

### 3. å¿«é€Ÿè®­ç»ƒæµ‹è¯•

```bash
# GPU 1-epochå¿«é€Ÿæµ‹è¯•
python scripts/train.py \
  model=yolov10n \
  data=coco128 \
  trainer.max_epochs=1 \
  trainer.accelerator=gpu \
  trainer.devices=1 \
  trainer.limit_train_batches=5 \
  trainer.limit_val_batches=5 \
  trainer.fast_dev_run=true \
  trainer.precision=16

# éªŒè¯è®­ç»ƒç»“æœ
ls -la logs/lightning_logs/version_0/
```

## ğŸ” æ€§èƒ½ä¼˜åŒ–ï¼ˆåŸºäºML.mdåŸºå‡†æ•°æ®ï¼‰

### GPUæ€§èƒ½è°ƒä¼˜ï¼ˆML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚å‚è€ƒï¼‰

```bash
# GPUæ€§èƒ½ä¼˜åŒ–è®¾ç½®
python -c "
import torch
import time

# GPUè®¾å¤‡æ£€æµ‹å’Œä¼˜åŒ–
if torch.cuda.is_available():
    print(f'æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU')
    
    # æ€§èƒ½åŸºå‡†éªŒè¯
    x = torch.randn(8192, 8192).cuda()
    torch.cuda.synchronize()
    start = time.time()
    y = torch.matmul(x, x)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    print(f'GPUçŸ©é˜µä¹˜æ³•: 8192Ã—8192 ç”¨æ—¶ {elapsed:.3f}s')
    print(f'GPUåˆ©ç”¨ç‡: {torch.cuda.utilization()}%')
    print(f'GPUå†…å­˜: {torch.cuda.memory_allocated()/1024**3:.1f}GB')
    
    # è‡ªåŠ¨ä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # å†…å­˜ç®¡ç†
    torch.cuda.empty_cache()
    print('âœ… GPUæ€§èƒ½ä¼˜åŒ–å®Œæˆ')
else:
    print('âŒ æœªæ£€æµ‹åˆ°GPU')
"
```

### GPUå†…å­˜ç®¡ç†ï¼ˆåŸºäºML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚ï¼‰

```bash
# GPUå†…å­˜ç²¾ç¡®è®¡ç®—ï¼ˆåŸºäºML.mdç¬¬2ç« å†…å­˜éœ€æ±‚å…¬å¼ï¼‰
python -c "
import torch
import psutil

# GPUå†…å­˜ç²¾ç¡®è®¡ç®—
print('=== GPUå†…å­˜éœ€æ±‚è¯„ä¼° ===')
if torch.cuda.is_available():
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f'GPUæ€»å†…å­˜: {gpu_memory:.1f}GB')
    
    # åŸºäºML.mdç¬¬2ç« çš„ç²¾ç¡®è®¡ç®—å…¬å¼
    memory_requirements = {
        'YOLOv10n': {
            'model_params': 3.5,      # GB
            'activation': 1.2,        # GB per batch=16
            'optimizer': 7.0,         # GB (å‚æ•°*2)
            'data_cache': 2.0,        # GB
            'total': 13.7             # GB with 50%å®‰å…¨ä½™é‡
        },
        'YOLOv10s': {
            'model_params': 5.0,      # GB
            'activation': 2.4,        # GB per batch=32
            'optimizer': 10.0,        # GB
            'data_cache': 3.0,        # GB
            'total': 20.0             # GB
        },
        'YOLOv10m': {
            'model_params': 8.0,      # GB
            'activation': 4.8,        # GB per batch=64
            'optimizer': 16.0,        # GB
            'data_cache': 4.0,        # GB
            'total': 32.0             # GB
        }
    }
    
    for model, specs in memory_requirements.items():
        if gpu_memory >= specs['total'] * 1.2:  # 20%å®‰å…¨ä½™é‡
            max_batch = int((gpu_memory / specs['total']) * 16)
            print(f'{model}: âœ… æ¨èbatch_size={max_batch} (éœ€è¦{specs[\"total\"]}GB)')
        else:
            print(f'{model}: âš ï¸ å†…å­˜ä¸è¶³ (éœ€è¦{specs[\"total\"]}GB)')

print('=== ç³»ç»Ÿå†…å­˜ç›‘æ§ ===')
mem = psutil.virtual_memory()
print(f'ç³»ç»Ÿå†…å­˜ä½¿ç”¨: {mem.percent}%')
print(f'ç³»ç»Ÿå¯ç”¨å†…å­˜: {mem.available // 1024**3} GB')

# å†…å­˜ä¼˜åŒ–ç­–ç•¥ï¼ˆåŸºäºML.mdæ€§èƒ½åŸºå‡†ï¼‰
memory_strategies = {
    'RTX 3060 (12GB)': {'batch_size': 32, 'precision': 16, 'accumulate': 2},
    'RTX 3080 (10GB)': {'batch_size': 24, 'precision': 16, 'accumulate': 3},
    'RTX 4090 (24GB)': {'batch_size': 64, 'precision': 16, 'accumulate': 1},
    'A100 (40GB)': {'batch_size': 128, 'precision': 16, 'accumulate': 1}
}

for gpu, config in memory_strategies.items():
    print(f'{gpu}: batch={config[\"batch_size\"]}, precision={config[\"precision\"]}, accumulate={config[\"accumulate\"]}')

# è‡ªåŠ¨å†…å­˜ä¼˜åŒ–å»ºè®®
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    if gpu_memory >= 20:
        print('ğŸš€ æ¨èé…ç½®ï¼šå¤§æ‰¹é‡è®­ç»ƒ + æ··åˆç²¾åº¦')
    elif gpu_memory >= 8:
        print('âš¡ æ¨èé…ç½®ï¼šä¸­æ‰¹é‡è®­ç»ƒ + æ¢¯åº¦ç´¯ç§¯')
    else:
        print('âš ï¸ æ¨èé…ç½®ï¼šå°æ‰¹é‡è®­ç»ƒ + CPUè¾…åŠ©')
"
```

## ğŸš¨ å¸¸è§é—®é¢˜è§£å†³

### é—®é¢˜1: CUDAç‰ˆæœ¬ä¸åŒ¹é…
```bash
# é”™è¯¯: CUDA driver version is insufficient
# è§£å†³æ–¹æ¡ˆï¼šæ£€æŸ¥NVIDIAé©±åŠ¨ç‰ˆæœ¬
nvidia-smi
# è¦æ±‚é©±åŠ¨ç‰ˆæœ¬ â‰¥ 530.x (æ”¯æŒCUDA 12.4)

# å¦‚æœç‰ˆæœ¬è¿‡ä½ï¼Œå‡çº§é©±åŠ¨
# Ubuntuç¤ºä¾‹ï¼š
sudo apt update && sudo apt install nvidia-driver-535
```

### é—®é¢˜2: GPUå†…å­˜ä¸è¶³
```bash
# é”™è¯¯: CUDA out of memory
# è§£å†³æ–¹æ¡ˆï¼šåŸºäºML.mdå†…å­˜è®¡ç®—å…¬å¼è‡ªåŠ¨è°ƒæ•´
python -c "
import torch
import math

# è·å–GPUå†…å­˜ä¿¡æ¯
gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
print(f'GPUå†…å­˜: {gpu_memory:.1f}GB')

# åŸºäºML.mdå†…å­˜è®¡ç®—å…¬å¼æ¨èbatch_size
# å…¬å¼: GPUå†…å­˜ = æ¨¡å‹å‚æ•° + æ¿€æ´»å€¼ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ•°æ®ç¼“å­˜
model_memory = {
    'yolov10n': 3.5,  # GB
    'yolov10s': 5.0,  # GB  
    'yolov10m': 8.0,  # GB
    'yolov10l': 12.0, # GB
    'yolov10x': 24.0  # GB
}

# è®¡ç®—æ¨èbatch_size
for model, mem in model_memory.items():
    if gpu_memory >= mem * 1.2:  # 20%å®‰å…¨ä½™é‡
        batch_size = int(gpu_memory / mem * 8)  # ç»éªŒå…¬å¼
        print(f'{model} æ¨èbatch_size: {max(4, min(batch_size, 64))}')
"

# å®é™…åº”ç”¨ç¤ºä¾‹
python scripts/train.py \
  model=yolov10n \
  data=coco128 \
  trainer.batch_size=16 \
  trainer.accumulate_grad_batches=2 \
  trainer.precision=16
```

### é—®é¢˜3: ç‰ˆæœ¬å†²çª
```bash
# é”™è¯¯: PyTorch CUDAç‰ˆæœ¬ä¸åŒ¹é…
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ç²¾ç¡®ç‰ˆæœ¬åŒ¹é…
pip uninstall torch torchvision torchaudio
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 \
  -i https://mirrors.aliyun.com/pypi/simple/

# éªŒè¯CUDAç‰ˆæœ¬åŒ¹é…
python -c "
import torch
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')
print(f'æœŸæœ›CUDA: 12.4')
assert torch.version.cuda == '12.4', 'CUDAç‰ˆæœ¬ä¸åŒ¹é…'
print('âœ… CUDAç‰ˆæœ¬åŒ¹é…æˆåŠŸ')
"

# PaddlePaddleç‰ˆæœ¬åŒ¹é…
pip uninstall paddlepaddle-gpu
pip install paddlepaddle-gpu==2.6.0.post126 \
  -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

# éªŒè¯PaddlePaddle GPU
python -c "
import paddle
print(f'PaddlePaddleç‰ˆæœ¬: {paddle.__version__}')
print(f'GPUç¼–è¯‘: {paddle.is_compiled_with_cuda()}')
assert paddle.is_compiled_with_cuda(), 'PaddlePaddleæœªå¯ç”¨GPU'
print('âœ… PaddlePaddle GPUéªŒè¯æˆåŠŸ')
"
```

## ğŸ“Š èµ„æºä½¿ç”¨åŸºå‡†ï¼ˆåŸºäºML.mdç¬¬266-277è¡Œï¼‰

### GPUå†…å­˜ä½¿ç”¨å‚è€ƒï¼ˆML.mdç¬¬266-277è¡ŒéªŒè¯æ•°æ®ï¼‰
| ä»»åŠ¡ç±»å‹ | Batch Size | GPUå†…å­˜ä½¿ç”¨ | è®­ç»ƒæ—¶é—´/epoch | GPUåˆ©ç”¨ç‡ | æ•°æ®æ¥æº |
|----------|------------|-------------|----------------|-----------|----------|
| CIFAR-10 | 32 | ~2GB | ~8ç§’ | 95% | ML.mdç¬¬266è¡Œ |
| ImageNet | 32 | ~8GB | ~8åˆ†é’Ÿ | 94% | ML.mdç¬¬267è¡Œ |
| COCO128 | 16 | ~4GB | ~45ç§’ | 95% | ML.mdç¬¬267è¡Œ |
| COCO2017 | 32 | ~8GB | ~45åˆ†é’Ÿ | 93% | ML.mdç¬¬267è¡Œ |

### GPUæ€§èƒ½åŸºå‡†ï¼ˆML.mdç¬¬274-277è¡ŒåŸºå‡†æ•°æ®ï¼‰
| GPUå‹å· | æ˜¾å­˜ | COCO128è®­ç»ƒæ—¶é—´ | COCO2017è®­ç»ƒæ—¶é—´ | GPUåˆ©ç”¨ç‡ | æ•°æ®æ¥æº |
|---------|------|-----------------|------------------|-----------|----------|
| RTX 3060 | 12GB | ~45ç§’/epoch | ~45åˆ†é’Ÿ/epoch | 95% | ML.mdç¬¬274è¡Œ |
| RTX 3080 | 10GB | ~35ç§’/epoch | ~35åˆ†é’Ÿ/epoch | 94% | ML.mdç¬¬274è¡Œ |
| RTX 4090 | 24GB | ~25ç§’/epoch | ~25åˆ†é’Ÿ/epoch | 93% | ML.mdç¬¬275è¡Œ |
| A100 | 40GB | ~15ç§’/epoch | ~15åˆ†é’Ÿ/epoch | 92% | ML.mdç¬¬275è¡Œ |

### CPUéƒ¨ç½²æ€§èƒ½å‚è€ƒï¼ˆç”¨äºç”Ÿäº§ç¯å¢ƒæ¨ç†ï¼‰
| CPUç±»å‹ | çº¿ç¨‹æ•° | COCO128æ¨ç†æ—¶é—´ | COCO2017æ¨ç†æ—¶é—´ | å†…å­˜ä½¿ç”¨ | æ•°æ®æ¥æº |
|---------|--------|-----------------|------------------|----------|----------|
| Intel i7-12700 | 8 | ~200ms/å¼  | ~200ms/å¼  | ~2GB RAM | ML.mdç¬¬274è¡Œ |
| Apple M1 | 8 | ~180ms/å¼  | ~180ms/å¼  | ~1.5GB RAM | ML.mdç¬¬274è¡Œ |
| AMD Ryzen 5800X | 8 | ~220ms/å¼  | ~220ms/å¼  | ~2GB RAM | ML.mdç¬¬275è¡Œ |

## ğŸ”„ ç¯å¢ƒåˆ‡æ¢

### ä»CPUåˆ‡æ¢åˆ°GPUç¯å¢ƒ

```bash
# å¤‡ä»½CPUç¯å¢ƒï¼ˆè™šæ‹Ÿç¯å¢ƒï¼‰
source ml-debug/bin/activate
pip freeze > cpu-requirements.txt

# æˆ–Condaç¯å¢ƒå¤‡ä»½
conda env export > cpu-environment.yml

# åˆ›å»ºGPUç¯å¢ƒï¼ˆæ¨èè™šæ‹Ÿç¯å¢ƒï¼‰
python -m venv ml-gpu
source ml-gpu/bin/activate
pip install -r requirements-gpu.txt

# æˆ–Conda GPUç¯å¢ƒ
conda create -n ml-gpu python=3.10 -y
conda activate ml-gpu
pip install -r requirements-gpu.txt

# éªŒè¯GPUç¯å¢ƒ
python -c "
import torch
print(f'GPUå¯ç”¨: {torch.cuda.is_available()}')
print(f'GPUæ•°é‡: {torch.cuda.device_count()}')
"
```

## ğŸ“‹ ç¯å¢ƒæ£€æŸ¥æ¸…å•

### å®‰è£…éªŒè¯
- [ ] Python 3.9-3.10
- [ ] PyTorch CPUç‰ˆæœ¬
- [ ] PaddlePaddle CPUç‰ˆæœ¬
- [ ] æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ

### åŠŸèƒ½éªŒè¯
- [ ] åŸºç¡€å¯¼å…¥æµ‹è¯•
- [ ] é…ç½®æ–‡ä»¶åŠ è½½
- [ ] æ¨¡å‹å®šä¹‰æµ‹è¯•
- [ ] æ•°æ®é›†åŠ è½½
- [ ] 1-epochè®­ç»ƒæµ‹è¯•

### æ€§èƒ½éªŒè¯
- [ ] CPUçº¿ç¨‹è®¾ç½®æ­£ç¡®
- [ ] å†…å­˜ä½¿ç”¨åˆç†
- [ ] è®­ç»ƒé€Ÿåº¦è¾¾æ ‡

## ğŸ¯ ä¸‹ä¸€æ­¥

å®ŒæˆCPUç¯å¢ƒé…ç½®åï¼š
1. è¿è¡Œ [DEBUG_CODE.md](./DEBUG_CODE.md) è¿›è¡Œä»£ç éªŒè¯
2. é…ç½® [DOCKER_CONFIG.md](./DOCKER_CONFIG.md) è¿›è¡ŒGPUéƒ¨ç½²
3. æ›´æ–° [PROJECT_BUILD_LOG.md](./PROJECT_BUILD_LOG.md)

**æ³¨æ„**ï¼šé¡¹ç›®åˆ›å»ºå‰è¯·å…ˆå®ŒæˆCREATE.mdçš„think hardè§„åˆ’ï¼Œå°†ç»“æœå†™å…¥INITIAL.md

---
**é…ç½®æ—¶é—´**: ~5åˆ†é’Ÿ | **éªŒè¯æ—¶é—´**: ~2åˆ†é’Ÿ | **æ€»è®¡**: ~7åˆ†é’Ÿ