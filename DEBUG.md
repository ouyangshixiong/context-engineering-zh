# ğŸ› è°ƒè¯•æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ” ä¸€é”®è°ƒè¯•æ£€æŸ¥
```bash
# å…¨è‡ªåŠ¨åŒ–ç¯å¢ƒè¯Šæ–­ï¼ˆGPUä¼˜å…ˆéªŒè¯ï¼‰
python scripts/debug.py

# æ‰‹åŠ¨åˆ†æ­¥æ£€æŸ¥ï¼ˆGPUè°ƒè¯•å­¦ä¹ ç”¨ï¼‰
python scripts/debug.py --step-by-step --gpu-first
```

### ğŸ—ï¸ ç¯å¢ƒé…ç½®æŒ‡å—

#### ğŸš€ GPUè°ƒè¯•ç¯å¢ƒï¼ˆ**é¦–è¦éªŒè¯ç¯å¢ƒ** - CUDA 12.4.1ä¸“ç”¨ï¼‰
```bash
# åˆ›å»ºGPUè°ƒè¯•ç¯å¢ƒï¼ˆåŸºäºML.mdç‰ˆæœ¬çŸ©é˜µï¼‰
conda create -n ml-gpu-debug python=3.10
conda activate ml-gpu-debug

# å®‰è£…PyTorch GPUç‰ˆæœ¬ï¼ˆCUDA 12.4.1ä¸“ç”¨ï¼‰
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 \
  -i https://mirrors.aliyun.com/pypi/simple/

# å®‰è£…PaddlePaddle GPUç‰ˆæœ¬ï¼ˆCUDA 12.4.1ä¸“ç”¨ï¼‰
pip install paddlepaddle-gpu==2.6.0.post126 \
  -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

# éªŒè¯GPUå®‰è£…
python -c "import torch; print(f'âœ… PyTorch GPU: {torch.__version__}')"
python -c "import torch; print(f'âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}')"
python -c "import torch; print(f'âœ… GPUæ•°é‡: {torch.cuda.device_count()}')"
python -c "import paddle; print(f'âœ… PaddlePaddle GPU: {paddle.__version__}')"

# åŸºäºML.mdç¬¬274-277è¡Œçš„æ€§èƒ½åŸºå‡†éªŒè¯
python -c "
import torch, time
if torch.cuda.is_available():
    x = torch.randn(8192, 8192).cuda()
    torch.cuda.synchronize()
    start = time.time()
    y = torch.matmul(x, x)
    torch.cuda.synchronize()
    print(f'ğŸ”¥ GPUåŸºå‡†æµ‹è¯•: {time.time()-start:.3f}s (æœŸæœ›: RTX3060 ~0.002s)')
    print(f'ğŸ”¥ GPUåˆ©ç”¨ç‡: {torch.cuda.utilization()}% (æœŸæœ›: â‰¥90%)')
"
```

#### ğŸ’» CPUç”Ÿäº§ç¯å¢ƒï¼ˆ**éƒ¨ç½²éªŒè¯é˜¶æ®µ**ï¼‰
```bash
# åˆ›å»ºCPUç”Ÿäº§ç¯å¢ƒï¼ˆDockeréƒ¨ç½²ä¸“ç”¨ï¼‰
conda create -n ml-cpu-deploy python=3.10
conda activate ml-cpu-deploy

# å®‰è£…PyTorch CPUç‰ˆæœ¬
pip install torch==2.4.1+cpu torchvision==0.19.1+cpu torchaudio==2.4.1+cpu \
  --index-url https://download.pytorch.org/whl/cpu

# å®‰è£…PaddlePaddle CPUç‰ˆæœ¬
pip install paddlepaddle==2.6.0

# éªŒè¯CPUå®‰è£…
python -c "import torch; print('âœ… PyTorch CPUç‰ˆæœ¬å®‰è£…æˆåŠŸ')"
python -c "import paddle; print('âœ… PaddlePaddle CPUç‰ˆæœ¬å®‰è£…æˆåŠŸ')"

# åŸºäºML.mdç¬¬274-277è¡Œçš„CPUåŸºå‡†éªŒè¯
python -c "
import torch, time
x = torch.randn(1000, 1000)
start = time.time()
y = torch.matmul(x, x)
print(f'ğŸ–¥ï¸ CPUåŸºå‡†æµ‹è¯•: {time.time()-start:.3f}s (æœŸæœ›: i7-12700 ~0.8s)')
"
```

## ğŸ” åˆ†æ­¥è°ƒè¯•æŒ‡å—

### 1ï¸âƒ£ ç¯å¢ƒéªŒè¯ï¼ˆ2åˆ†é’Ÿï¼‰
```bash
# ğŸ” ç¯å¢ƒä¿¡æ¯æ€»è§ˆ
python -c "
import torch, platform
print('ğŸ“Š ç³»ç»Ÿä¿¡æ¯')
print(f'æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}')
print(f'Pythonç‰ˆæœ¬: {platform.python_version()}')
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')
    print(f'GPUæ•°é‡: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        print(f'GPU{i}: {torch.cuda.get_device_name(i)}')
"
```

### 2ï¸âƒ£ æ•°æ®é›†éªŒè¯ï¼ˆ3åˆ†é’Ÿï¼‰
```bash
# ğŸ“¥ GPUä¼˜å…ˆæµ‹è¯•æ•°æ®é›†ä¸‹è½½
python scripts/download.py --datasets coco128 --data_dir ./test_data

# âœ… GPUç¯å¢ƒæ•°æ®å®Œæ•´æ€§éªŒè¯
python -c "
from src.datasets.datamodules.coco_datamodule import COCODataModule
from pathlib import Path

data_dir = './test_data'
if Path(data_dir).exists():
    dm = COCODataModule(data_dir=data_dir, batch_size=16)  # GPUä¼˜åŒ–batch_size
    dm.prepare_data()
    dm.setup('fit')
    print(f'âœ… GPUè®­ç»ƒæ ·æœ¬: {len(dm.train_dataset):,}')
    print(f'âœ… GPUéªŒè¯æ ·æœ¬: {len(dm.val_dataset):,}')
    print(f'âœ… ç±»åˆ«æ•°é‡: {dm.num_classes}')
    print(f'âœ… å›¾åƒå°ºå¯¸: {dm.train_dataset[0][0].shape}')
    print(f'âœ… GPUå†…å­˜éœ€æ±‚: ~4GB (åŸºäºML.mdç¬¬266-277è¡Œ)')
else:
    print('âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½GPUæµ‹è¯•æ•°æ®é›†')
"

# ğŸ¯ CIFAR-10 CPUéªŒè¯ï¼ˆéƒ¨ç½²é˜¶æ®µï¼‰
python -c "
from src.datasets.datamodules.cifar10_datamodule import CIFAR10DataModule
from pathlib import Path

data_dir = './test_data'
if Path(data_dir).exists():
    dm = CIFAR10DataModule(data_dir=data_dir, batch_size=32)  # CPUä¼˜åŒ–batch_size
    dm.prepare_data()
    dm.setup('fit')
    print(f'âœ… CPUè®­ç»ƒæ ·æœ¬: {len(dm.train_dataset):,}')
    print(f'âœ… CPUéªŒè¯æ ·æœ¬: {len(dm.val_dataset):,}')
    print(f'âœ… CPUå†…å­˜éœ€æ±‚: ~2GB RAM')
"
```

### 3ï¸âƒ£ æ¨¡å‹éªŒè¯ï¼ˆ2åˆ†é’Ÿï¼‰
```bash
# ğŸ§  CPUæ¨¡å‹æµ‹è¯•
python -c "
from src.models.pytorch.resnet_classifier import ResNetClassifier
import torch

print('ğŸ§ª æµ‹è¯•CPUæ¨¡å‹åˆ›å»º...')
model = ResNetClassifier(num_classes=10)
dummy_input = torch.randn(1, 3, 32, 32)
output = model(dummy_input)
print(f'âœ… CPUæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè¾“å‡ºç»´åº¦: {output.shape}')
print(f'âœ… æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}')
"

# ğŸš€ GPUæ¨¡å‹æµ‹è¯•ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
python -c "
import torch
from src.models.pytorch.resnet_classifier import ResNetClassifier

if torch.cuda.is_available():
    print('ğŸ¯ æµ‹è¯•GPUæ¨¡å‹åˆ›å»º...')
    model = ResNetClassifier(num_classes=10).cuda()
    dummy_input = torch.randn(4, 3, 32, 32).cuda()
    output = model(dummy_input)
    print(f'âœ… GPUæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè¾“å‡ºç»´åº¦: {output.shape}')
    print(f'âœ… GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**2:.1f}MB')
else:
    print('â„¹ï¸  æ— GPUç¯å¢ƒï¼Œè·³è¿‡GPUæµ‹è¯•')
"
```

### 4ï¸âƒ£ è®­ç»ƒéªŒè¯ï¼ˆ3åˆ†é’Ÿï¼‰
```bash
# âš¡ è¶…å¿«é€Ÿè®­ç»ƒéªŒè¯ï¼ˆ30ç§’å®Œæˆï¼‰
echo "ğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒéªŒè¯..."
python scripts/train.py \
  model=resnet18 \
  data=cifar10 \
  trainer.fast_dev_run=true \
  trainer.accelerator=auto \
  data.batch_size=4 \
  trainer.limit_train_batches=2 \
  trainer.limit_val_batches=2

# ğŸ¯ éªŒè¯è®­ç»ƒç»“æœ
if [ $? -eq 0 ]; then
    echo "âœ… è®­ç»ƒéªŒè¯é€šè¿‡ï¼ä»£ç é€»è¾‘æ­£ç¡®"
else
    echo "âŒ è®­ç»ƒéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯"
fi
```

## ğŸš¨ å¸¸è§é—®é¢˜é€ŸæŸ¥

### ğŸ”§ å¯¼å…¥é”™è¯¯ä¿®å¤
```bash
# æ–¹æ³•1ï¼šæ°¸ä¹…æ·»åŠ åˆ°Pythonè·¯å¾„
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
echo 'export PYTHONPATH="${PYTHONPATH}:$(pwd)"' >> ~/.zshrc

# æ–¹æ³•2ï¼šä¸´æ—¶è¿è¡Œï¼ˆæ¨èï¼‰
PYTHONPATH=. python scripts/train.py model=resnet18 data=cifar10 trainer.fast_dev_run=true

# æ–¹æ³•3ï¼šä½¿ç”¨python -m
python -m scripts.train model=resnet18 data=cifar10 trainer.fast_dev_run=true
```

### ğŸ–¥ï¸ GPUé—®é¢˜è¯Šæ–­
```bash
# ğŸ” GPUçŠ¶æ€æ£€æŸ¥
nvidia-smi
python -c "
import torch
print(f'ğŸ” CUDAå¯ç”¨: {torch.cuda.is_available()}')
print(f'ğŸ” CUDAç‰ˆæœ¬: {torch.version.cuda}')
print(f'ğŸ” GPUæ•°é‡: {torch.cuda.device_count()}')
for i in range(torch.cuda.device_count()):
    print(f'ğŸ” GPU{i}: {torch.cuda.get_device_name(i)}')
"

# âš™ï¸ å¼ºåˆ¶ä½¿ç”¨CPUè°ƒè¯•
python scripts/train.py \
  model=resnet18 data=cifar10 \
  trainer.accelerator=cpu \
  trainer.fast_dev_run=true

# ğŸ¯ GPUå†…å­˜ä¼˜åŒ–
python scripts/train.py \
  model=resnet18 data=cifar10 \
  data.batch_size=8 \
  trainer.precision=16 \
  trainer.fast_dev_run=true
```

### ğŸ’¾ å†…å­˜ä¸è¶³è§£å†³
```bash
# ğŸ¯ é€æ­¥é™ä½batch size
for bs in 32 16 8 4 2; do
    echo "æµ‹è¯•batch_size=$bs"
    python scripts/train.py \
      model=resnet18 data=cifar10 \
      data.batch_size=$bs \
      trainer.fast_dev_run=true \
      && break
done

# ğŸš€ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
python scripts/train.py \
  model=resnet18 data=cifar10 \
  trainer.precision=16 \
  trainer.fast_dev_run=true

# âš¡ ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python scripts/train.py \
  model=resnet18 data=cifar10 \
  data.batch_size=4 \
  trainer.accumulate_grad_batches=4 \
  trainer.fast_dev_run=true
```

## ğŸ› ï¸ ä¸“ä¸šè°ƒè¯•å·¥å…·ç®±

### ğŸ“¦ å¿…å¤‡è°ƒè¯•å·¥å…·
```bash
# æ ¸å¿ƒè°ƒè¯•å·¥å…·åŒ…
pip install ipdb rich tensorboard wandb

# æ€§èƒ½åˆ†æå·¥å…·
pip install py-spy memory_profiler

# å¯è§†åŒ–è°ƒè¯•
pip install netron torch_tb_profiler
```

### ğŸ¯ è°ƒè¯•å®æˆ˜æŠ€å·§

#### **1. é—ªç”µè°ƒè¯•æ¨¡å¼**
```bash
# âš¡ 30ç§’å®Œæ•´éªŒè¯
python scripts/train.py \
  model=resnet18 \
  data=cifar10 \
  trainer.fast_dev_run=true \
  trainer.log_every_n_steps=1 \
  trainer.limit_train_batches=2 \
  trainer.limit_val_batches=2
```

#### **2. æ•°æ®ç®¡é“è°ƒè¯•**
```bash
# ğŸ“Š æ•°æ®åŠ è½½é€Ÿåº¦æµ‹è¯•
python -c "
import time, torch
from src.datasets.datamodules.cifar10_datamodule import CIFAR10DataModule
from torch.utils.data import DataLoader

dm = CIFAR10DataModule()
dm.setup('fit')
start = time.time()
for i, batch in enumerate(DataLoader(dm.train_dataset, batch_size=32, num_workers=4)):
    if i >= 10: break
print(f'âš¡ 10ä¸ªbatchåŠ è½½æ—¶é—´: {time.time()-start:.2f}s')
print(f'âš¡ å¹³å‡æ¯ä¸ªbatch: {(time.time()-start)/10:.2f}s')
"
```

#### **3. é…ç½®éªŒè¯å™¨**
```bash
# âœ… é…ç½®æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
python -c "
from omegaconf import OmegaConf
config = OmegaConf.load('configs/config.yaml')
print('ğŸ¯ é…ç½®ç»“æ„éªŒè¯ï¼š')
print(f'  - æ¨¡å‹é…ç½®: {\"âœ…\" if \"model\" in config else \"âŒ\"}')
print(f'  - æ•°æ®é…ç½®: {\"âœ…\" if \"data\" in config else \"âŒ\"}')
print(f'  - è®­ç»ƒé…ç½®: {\"âœ…\" if \"trainer\" in config else \"âŒ\"}')
print('ğŸ“‹ å®Œæ•´é…ç½®ï¼š')
OmegaConf.pretty(config)
"
```

## ğŸ“Š æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–

### ğŸ” å®æ—¶ç›‘æ§é¢æ¿

#### **GPUç›‘æ§**
```bash
# ğŸ–¥ï¸ å®æ—¶GPUçŠ¶æ€
watch -n 1 nvidia-smi

# ğŸ“Š é«˜çº§GPUç›‘æ§
pip install gpustat
gpustat -i 1

# ğŸ”¥ GPUæ¸©åº¦/åŠŸè€—ç›‘æ§
nvidia-smi --query-gpu=temperature.gpu,power.draw,memory.used --format=csv,noheader,nounits
```

#### **å†…å­˜ç›‘æ§**
```bash
# ğŸ’¾ å†…å­˜ä½¿ç”¨åˆ†æ
htop

# ğŸ¯ Pythonå†…å­˜ç›‘æ§
python -c "
import psutil, os
process = psutil.Process(os.getpid())
print(f'ğŸ’¾ å½“å‰å†…å­˜ä½¿ç”¨: {process.memory_info().rss / 1024**2:.1f} MB')
print(f'ğŸ¯ ç³»ç»Ÿå†…å­˜: {psutil.virtual_memory().percent}%')
"
```

#### **è®­ç»ƒæ€§èƒ½åˆ†æ**
```bash
# ğŸ“ˆ è®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•
python scripts/benchmark.py \
  --model resnet18 \
  --data cifar10 \
  --batch_sizes 16 32 64 \
  --duration 60

# ğŸ¯ ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
python -c "
import torch
from pytorch_lightning import Trainer
from src.models.pytorch.resnet_classifier import ResNetClassifier

model = ResNetClassifier(num_classes=10)
trainer = Trainer(fast_dev_run=True)
print(f'âœ… æ¨¡å‹éªŒè¯é€šè¿‡')
"
```
```