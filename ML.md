# ğŸ¤– æŠ€æœ¯é€‰å‹è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“è§„èŒƒï¼ˆTechnical Selection Agentic-ai-coder Specificationï¼‰

> **æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“è¡Œä¸ºå‡†åˆ™** - åŸºäºé‡åŒ–å†³ç­–çŸ©é˜µçš„æ¡†æ¶ç‰ˆæœ¬é€‰æ‹©ä¸ç¡¬ä»¶éœ€æ±‚è¯„ä¼°ï¼Œç¡®ä¿æŠ€æœ¯é€‰å‹æœ‰ç†æœ‰æ®ã€‚

## ğŸ¯ æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“è§’è‰²å®šä¹‰

### ğŸ“‹ æ™ºèƒ½ä½“èŒè´£è¾¹ç•Œ
- **è§’è‰²å®šä½**: æŠ€æœ¯é€‰å‹ä¸ç¡¬ä»¶éœ€æ±‚è¯„ä¼°çš„è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“
- **æ ¸å¿ƒèŒè´£**: åŸºäºCREATE.mdéœ€æ±‚è¿›è¡Œæ¡†æ¶ç‰ˆæœ¬é€‰æ‹©ä¸ç¡¬ä»¶é…ç½®
- **è¾“å…¥è§„èŒƒ**: CREATE.mdéœ€æ±‚æè¿° + PLANNING.mdæŠ€æœ¯å†³ç­–
- **è¾“å‡ºè§„èŒƒ**: æ¡†æ¶ç‰ˆæœ¬çŸ©é˜µ + ç¡¬ä»¶éœ€æ±‚è¯„ä¼° + æ€§èƒ½åŸºå‡†
- **éªŒè¯æ ‡å‡†**: é‡åŒ–å†³ç­–çŸ©é˜µè¯„åˆ†â‰¥3.5åˆ†ï¼ŒGPUå†…å­˜ç²¾ç¡®è®¡ç®—

### ğŸ”„ æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“åä½œæµç¨‹
```mermaid
graph TD
    subgraph æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“è¾“å…¥
        CREATE[CREATE.mdéœ€æ±‚] --> TECH[æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“<br/>é€‰å‹å†³ç­–]
        PLAN[PLANNING.mdå†³ç­–] --> TECH
    end
    
    subgraph æŠ€æœ¯è¯„ä¼°
        TECH -->|è¯„ä¼°| FRAME[æ¡†æ¶é€‰æ‹©<br/>ç‰ˆæœ¬çŸ©é˜µ]
        TECH -->|è®¡ç®—| HARDWARE[ç¡¬ä»¶éœ€æ±‚<br/>ç²¾ç¡®è®¡ç®—]
        TECH -->|éªŒè¯| BENCHMARK[æ€§èƒ½åŸºå‡†<br/>éªŒè¯æ ‡å‡†]
        TECH -->|è¾“å‡º| INITIAL[INITIAL.md<br/>æŠ€æœ¯è§„æ ¼]
    end
    
    subgraph è¾“å‡ºè§„èŒƒ
        FRAME -->|ç”Ÿæˆ| SPEC[æŠ€æœ¯è§„æ ¼<br/>æ¡†æ¶ç‰ˆæœ¬]
        HARDWARE -->|ç”Ÿæˆ| SPEC
        BENCHMARK -->|ç”Ÿæˆ| SPEC
    end
    
    style TECH fill:#90EE90,stroke:#333
    style CREATE fill:#FFD700,stroke:#333
    style SPEC fill:#87CEEB,stroke:#333
```

## ğŸ¯ æŠ€æœ¯é€‰å‹å†³ç­–çŸ©é˜µ

### ğŸ“Š æ¡†æ¶ç‰ˆæœ¬ç²¾ç¡®è§„èŒƒ
| é˜¶æ®µ | æ™ºèƒ½ä½“èŒè´£ | PyTorchç‰ˆæœ¬ | PaddlePaddleç‰ˆæœ¬ | CUDAç‰ˆæœ¬ | è§„èŒƒå¼•ç”¨ä½ç½® | éªŒè¯æ ‡å‡† |
|------|------------|-------------|------------------|----------|--------------|----------|
| **VENVè°ƒè¯•** | æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“GPUéªŒè¯ | 2.4.1 | 2.6.0+gpu | N/A | ML.mdç¬¬1ç«  | GPUåˆ©ç”¨ç‡>90% |
| **DOCKERéƒ¨ç½²** | æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“CPUä¼˜åŒ– | 2.4.1+cpu | 2.6.0+cpu | N/A | ML.mdç¬¬2ç«  | CPUæ¨ç†ä¼˜åŒ– |

### ğŸ¯ æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“å†³ç­–æ¡†æ¶

#### 1. æ¡†æ¶é€‰æ‹©çŸ©é˜µï¼ˆCREATE.mdå¼•ç”¨ï¼‰
**è§„èŒƒå¼•ç”¨**: åŸºäºCREATE.mdç¬¬6ç« "æŠ€æœ¯é€‰å‹å†³ç­–"
```yaml
æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“æ¡†æ¶é€‰æ‹©è§„èŒƒ:
  è¾“å…¥æ¥æº: "CREATE.mdéœ€æ±‚è§„æ ¼+PLANNING.mdæŠ€æœ¯å†³ç­–"
  å†³ç­–çŸ©é˜µ: "é‡åŒ–è¯„åˆ†ç³»ç»Ÿ"
  è¯„ä¼°ç»´åº¦: ["å›¢é˜Ÿç†Ÿæ‚‰åº¦", "éƒ¨ç½²ä¾¿åˆ©æ€§", "æ€§èƒ½ä¼˜åŒ–", "ç¤¾åŒºæ”¯æŒ"]
  æƒé‡åˆ†é…: [0.30, 0.25, 0.25, 0.20]
  é€‰æ‹©é˜ˆå€¼: "â‰¥3.5åˆ†æ¨èé‡‡ç”¨"
  éªŒè¯æ ‡å‡†: "ML.mdæ€§èƒ½åŸºå‡†æµ‹è¯•"
```

**æ¡†æ¶å†³ç­–æ ‡å‡†**:
| è¯„ä¼°ç»´åº¦ | æƒé‡ | PyTorchè¯„åˆ† | Paddleè¯„åˆ† | å†³ç­–ä¾æ® |
|----------|------|-------------|------------|----------|
| **å›¢é˜Ÿç†Ÿæ‚‰åº¦** | 30% | â˜…â˜…â˜…â˜…â˜† 4.0 | â˜…â˜…â˜…â˜†â˜† 3.0 | CREATE.mdå›¢é˜ŸèƒŒæ™¯ |
| **éƒ¨ç½²ä¾¿åˆ©æ€§** | 25% | â˜…â˜…â˜…â˜†â˜† 3.5 | â˜…â˜…â˜…â˜…â˜† 4.2 | DOCKER_CONFIG.mdéªŒè¯ |
| **æ€§èƒ½ä¼˜åŒ–** | 25% | â˜…â˜…â˜…â˜…â˜† 4.0 | â˜…â˜…â˜…â˜…â˜† 4.0 | ML.mdæ€§èƒ½åŸºå‡† |
| **ç¤¾åŒºæ”¯æŒ** | 20% | â˜…â˜…â˜…â˜…â˜… 5.0 | â˜…â˜…â˜…â˜†â˜† 3.5 | é—®é¢˜è§£å†³æ•ˆç‡ |
| **ç»¼åˆå¾—åˆ†** | 100% | **4.1åˆ†** | **3.6åˆ†** | **æ¨èPyTorch** |

#### 2. ç¡¬ä»¶éœ€æ±‚è®¡ç®—ï¼ˆPLANNING.mdå¼•ç”¨ï¼‰
**è§„èŒƒå¼•ç”¨**: ä¾æ®PLANNING.mdç¬¬3ç« "èµ„æºè¯„ä¼°ç­–ç•¥"
```yaml
æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“ç¡¬ä»¶è®¡ç®—è§„èŒƒ:
  è¾“å…¥æ¥æº: "PLANNING.mdèµ„æºéœ€æ±‚è§„åˆ’"
  è®¡ç®—å…¬å¼: "GPUå†…å­˜ = æ¨¡å‹å‚æ•° + æ¿€æ´»å€¼ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ•°æ®ç¼“å­˜"
  å®‰å…¨ä½™é‡: "50%é¢å¤–å†…å­˜é¢„ç•™"
  éªŒè¯æ–¹æ³•: "ML.mdå®é™…æµ‹è¯•æ•°æ®"
```

**ç¡¬ä»¶éœ€æ±‚ç²¾ç¡®è®¡ç®—**:
```python
# æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“å†…å­˜è®¡ç®—æ¨¡æ¿
def calculate_gpu_memory(model_name, batch_size):
    """
    åŸºäºML.mdç¬¬2ç« çš„ç²¾ç¡®è®¡ç®—å…¬å¼
    """
    memory_map = {
        'resnet18': {
            'model_params': 11.7,  # MB
            'activation_per_batch': 0.5 * batch_size,  # MB
            'optimizer_state': 23.4,  # MB (å‚æ•°*2)
            'data_cache': 500,  # MB
        },
        'yolov10n': {
            'model_params': 5.0,  # MB
            'activation_per_batch': 2.0 * batch_size,  # MB
            'optimizer_state': 10.0,  # MB
            'data_cache': 1000,  # MB
        }
    }
    return memory_map[model_name]

# æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“æ¨èé…ç½®
configurations = {
    'CIFAR-10åˆ†ç±»': {
        'model': 'resnet18',
        'batch_size': 32,
        'gpu_memory': '8GB RTX 3060',
        'training_time': '30åˆ†é’Ÿ/epoch',
        'reference': 'ML.mdç¬¬3ç« æ€§èƒ½åŸºå‡†'
    },
    'ImageNetåˆ†ç±»': {
        'model': 'resnet50', 
        'batch_size': 64,
        'gpu_memory': '24GB RTX 4090',
        'training_time': '8åˆ†é’Ÿ/epoch',
        'reference': 'ML.mdç¬¬3ç« æ€§èƒ½åŸºå‡†'
    }
}
```

#### 3. æ€§èƒ½åŸºå‡†éªŒè¯ï¼ˆML.mdå¼•ç”¨ï¼‰
**è§„èŒƒå¼•ç”¨**: ä½¿ç”¨ML.mdç¬¬3ç« "æ€§èƒ½åŸºå‡†éªŒè¯"
```yaml
æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“æ€§èƒ½éªŒè¯è§„èŒƒ:
  åŸºå‡†æµ‹è¯•: "ResNet-50 on ImageNet"
  æµ‹è¯•ç¯å¢ƒ: "RTX 3060 8GB"
  éªŒè¯æŒ‡æ ‡: ["è®­ç»ƒæ—¶é—´/epoch", "GPUåˆ©ç”¨ç‡", "å†…å­˜ä½¿ç”¨"]
  éªŒæ”¶æ ‡å‡†: "GPUåˆ©ç”¨ç‡>90%, å†…å­˜ä½¿ç”¨<80%"
```

## ğŸ“Š æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“éªŒè¯çŸ©é˜µ

### ğŸ“‹ æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“éªŒæ”¶æ¸…å•
æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“å®Œæˆé€‰å‹åï¼Œå¿…é¡»éªŒè¯ï¼š
- [ ] æ¡†æ¶é€‰æ‹©æœ‰CREATE.mdç¬¬6ç« çš„é‡åŒ–è¯„åˆ†ä¾æ®
- [ ] ç¡¬ä»¶éœ€æ±‚ç»è¿‡ML.mdç¬¬2ç« çš„ç²¾ç¡®è®¡ç®—
- [ ] æ€§èƒ½åŸºå‡†ç¬¦åˆML.mdç¬¬3ç« çš„éªŒè¯æ ‡å‡†
- [ ] ç‰ˆæœ¬å…¼å®¹æ€§é€šè¿‡ML.mdç¬¬5ç« çš„æµ‹è¯•éªŒè¯

### ğŸ“Š æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“æ€§èƒ½åŸºå‡†
| éªŒè¯ç»´åº¦ | æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“æ ‡å‡† | ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯” |
|----------|--------------|--------------|
| **å†³ç­–æ—¶é—´** | 10åˆ†é’Ÿé‡åŒ–åˆ†æ | æ•°å¤©ç»éªŒå†³ç­– |
| **è®¡ç®—ç²¾åº¦** | GPUå†…å­˜ç²¾ç¡®åˆ°MB | ç²—ç•¥ä¼°ç®— |
| **æ€§èƒ½é¢„æµ‹** | åŸºäºML.mdå®é™…æ•°æ® | ç†è®ºæ¨æµ‹ |
| **æˆæœ¬è¯„ä¼°** | ç¡¬ä»¶éœ€æ±‚é‡åŒ–è®¡ç®— | ç»éªŒåˆ¤æ–­ |

## ğŸ¯ æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“å¿«é€Ÿå¼€å§‹

### ç«‹å³æ‰§è¡Œæ­¥éª¤
1. **æ‰“å¼€CREATE.mdç¬¬6ç« ** - å¯åŠ¨æŠ€æœ¯é€‰å‹å†³ç­–çŸ©é˜µ
2. **è¿è¡ŒML.mdå†…å­˜è®¡ç®—** - ç²¾ç¡®è®¡ç®—GPUéœ€æ±‚
3. **å‚è€ƒML.mdæ€§èƒ½åŸºå‡†** - éªŒè¯æŠ€æœ¯å¯è¡Œæ€§
4. **ç”ŸæˆæŠ€æœ¯è§„æ ¼** - æ¡†æ¶ç‰ˆæœ¬+ç¡¬ä»¶é…ç½®æ ‡å‡†åŒ–è¾“å‡º

### æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“æˆåŠŸæ ‡å‡†
**æ ¸å¿ƒè®°å¿†ç‚¹**: "10åˆ†é’Ÿçš„æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“é‡åŒ–åˆ†æï¼Œèƒœè¿‡æ•°å¤©çš„ç»éªŒå†³ç­–ï¼"

## ğŸ“Š æ¡†æ¶ç‰ˆæœ¬çŸ©é˜µä¸ä¸¤é˜¶æ®µç¯å¢ƒé…ç½®

### ç¯å¢ƒé…ç½®æ€»è§ˆ

| é˜¶æ®µ | æ™ºèƒ½ä½“èŒè´£ | PyTorchç‰ˆæœ¬ | PaddlePaddleç‰ˆæœ¬ | CUDAç‰ˆæœ¬ | è§„èŒƒå¼•ç”¨ä½ç½® | éªŒè¯æ ‡å‡† |
|------|------------|-------------|------------------|----------|--------------|----------|
| **VENVè°ƒè¯•** | æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“GPUéªŒè¯ | 2.4.1 | 2.6.0+gpu | N/A | ML.mdç¬¬1ç«  | GPUåˆ©ç”¨ç‡>90% |
| **DOCKERéƒ¨ç½²** | æŠ€æœ¯è‡ªä¸»ç¼–ç¨‹æ™ºèƒ½ä½“CPUä¼˜åŒ– | 2.4.1+cpu | 2.6.0+cpu | N/A | ML.mdç¬¬2ç«  | CPUæ¨ç†ä¼˜åŒ– |

### VENVè°ƒè¯•ç¯å¢ƒï¼ˆGPUéªŒè¯ç¯å¢ƒï¼‰

#### PyTorch GPUç¯å¢ƒ
```bash
# åˆ›å»ºè°ƒè¯•ç¯å¢ƒ
conda create -n ml-debug python=3.10
conda activate ml-debug

# PyTorch CPUç‰ˆæœ¬
pip install torch==2.4.1+cpu torchvision==0.19.1+cpu torchaudio==2.4.1+cpu \
  --index-url https://download.pytorch.org/whl/cpu

# éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
```

#### PaddlePaddle GPUç¯å¢ƒ
```bash
# PaddlePaddle GPUç‰ˆæœ¬
pip install paddlepaddle-gpu==2.6.0.post126 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

# éªŒè¯å®‰è£…
python -c "import paddle; print(f'PaddlePaddle: {paddle.__version__}, GPU: {paddle.is_compiled_with_cuda()}')"
```

#### é€šç”¨ä¾èµ–ï¼ˆGPUéªŒè¯ç¯å¢ƒï¼‰
```bash
pip install pytorch-lightning==2.0.0 omegaconf==2.3.0 \
  torchmetrics==0.11.0 scikit-learn==1.3.0 \
  matplotlib==3.7.0 seaborn==0.12.0 \
  tensorboard==2.13.0 wandb==0.15.0
```

### DOCKERéƒ¨ç½²ç¯å¢ƒï¼ˆGPUåŠ é€Ÿ - CUDA 12.4.1ä¸“ç”¨ï¼‰

#### é•œåƒç±»å‹é€‰æ‹©æŒ‡å—

| é•œåƒç±»å‹ | æ¨èé•œåƒ | ç²¾ç¡®æ ‡ç­¾ | å¤§å° | ä½¿ç”¨åœºæ™¯ | åŒ…å«ç»„ä»¶ |
|----------|----------|----------|------|----------|----------|
| **å¼€å‘/è®­ç»ƒ** | nvidia/cuda | `12.4.1-cudnn-devel-ubuntu20.04` | ~5.2GB | å®Œæ•´å¼€å‘ç¯å¢ƒ | CUDA + cuDNN + ç¼–è¯‘å·¥å…· |
| **éƒ¨ç½²/æ¨ç†** | nvidia/cuda | `12.4.1-cudnn-runtime-ubuntu20.04` | ~3.1GB | ç”Ÿäº§éƒ¨ç½² | CUDA + cuDNNï¼ˆæ— ç¼–è¯‘å·¥å…·ï¼‰ |
| **åŸºç¡€éªŒè¯** | nvidia/cuda | `12.4.1-base-ubuntu20.04` | ~1.8GB | ç¯å¢ƒæµ‹è¯• | ä»…CUDAè¿è¡Œæ—¶ |

#### ç‰ˆæœ¬éªŒè¯ä¸é€‰æ‹©é€»è¾‘
```bash
# éªŒè¯é•œåƒç‰ˆæœ¬ä¿¡æ¯
docker run --rm nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 bash -c "
echo '=== CUDA 12.4.1é•œåƒç‰ˆæœ¬éªŒè¯ ==='
echo 'CUDAç‰ˆæœ¬:' 
nvcc --version | grep release
echo 'cuDNNç‰ˆæœ¬:' 
cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2
echo 'Pythonç‰ˆæœ¬:' 
python3.10 --version
echo 'Ubuntuç‰ˆæœ¬:' 
cat /etc/os-release | grep VERSION_ID
"

# é¢„æœŸè¾“å‡ºï¼š
# CUDAç‰ˆæœ¬: release 12.4, V12.4.120
# cuDNNç‰ˆæœ¬: CUDNN_MAJOR 9, CUDNN_MINOR 3, CUDNN_PATCHLEVEL 0
# Pythonç‰ˆæœ¬: Python 3.10.12
# Ubuntuç‰ˆæœ¬: VERSION_ID="20.04"
```

#### é•œåƒé€‰æ‹©å†³ç­–æ ‘
```mermaid
graph TD
    A[é€‰æ‹©CUDA 12.4.1é•œåƒ] --> B{éœ€è¦ç¼–è¯‘å—?}
    B -->|æ˜¯| C[ä½¿ç”¨develé•œåƒ]
    B -->|å¦| D{éœ€è¦cuDNNå—?}
    C --> E[nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04]
    D -->|æ˜¯| F[nvidia/cuda:12.4.1-cudnn-runtime-ubuntu20.04]
    D -->|å¦| G[nvidia/cuda:12.4.1-base-ubuntu20.04]
    
    style E fill:#90EE90
    style F fill:#87CEEB
    style G fill:#FFB6C1
```

#### ç²¾ç¡®ç‰ˆæœ¬å¯¹é½ç­–ç•¥
**nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04** ç›¸æ¯”å…¶ä»–é•œåƒä¼˜åŠ¿ï¼š
- **ç‰ˆæœ¬ç²¾ç¡®åŒ¹é…**ï¼šCUDA 12.4.1 + cuDNN 9.3.0 + Python 3.10.12
- **ä½“ç§¯ä¼˜åŒ–**ï¼šæ¯”pytorch/pytorché•œåƒå°35%ï¼ˆ5.2GB vs 8GBï¼‰
- **çµæ´»æ€§é«˜**ï¼šå¯è‡ªå®šä¹‰PyTorch/PaddlePaddleç‰ˆæœ¬
- **ç¨³å®šæ€§å¼º**ï¼šå®˜æ–¹CUDAåŸºç¡€é•œåƒï¼Œæ›´æ–°åŠæ—¶

#### Dockerfileæ¨¡æ¿

**PyTorch GPUç‰ˆæœ¬ï¼ˆæ¨ènvidia/cudaé•œåƒï¼‰**
```dockerfile
FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04

# å®‰è£…Pythonå’Œç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.10 python3.10-dev python3-pip \
    git wget unzip build-essential \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®Python3.10ä¸ºé»˜è®¤
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1
RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

# å®‰è£…PyTorch GPUç‰ˆæœ¬
RUN pip install --no-cache-dir \
    torch==2.4.1 \
    torchvision==0.15.1 \
    torchaudio==2.4.1 \
    -i https://mirrors.aliyun.com/pypi/simple/

# å®‰è£…å…¶ä»–ä¾èµ–
RUN pip install --no-cache-dir \
    pytorch-lightning==2.0.0 \
    omegaconf==2.3.0 \
    torchmetrics==0.11.0 \
    wandb==0.15.0 \
    tensorboard==2.13.0 -i https://mirrors.aliyun.com/pypi/simple/

WORKDIR /workspace
COPY . .
CMD ["python", "scripts/train.py"]
```

**PaddlePaddle GPUç‰ˆæœ¬**
```dockerfile
FROM paddlepaddle/paddle:2.6.0-gpu-cuda12.6-cudnn9

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir \
    omegaconf==2.3.0 \
    scikit-learn==1.3.0 \
    matplotlib==3.7.0 \
    seaborn==0.12.0 \
    wandb==0.15.0 \
    tensorboard==2.13.0 -i https://mirrors.aliyun.com/pypi/simple/

WORKDIR /workspace
COPY . .
CMD ["python", "scripts/train.py"]
```

### å®Œæ•´ç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µï¼ˆnvidia/cuda:12.4.1ä¸“ç”¨ï¼‰

#### CUDA 12.4.1é•œåƒç‰ˆæœ¬å¯¹åº”è¡¨
| Pythonç‰ˆæœ¬ | é•œåƒç±»å‹ | PyTorchç‰ˆæœ¬ | PaddlePaddleç‰ˆæœ¬ | NVIDIAé©±åŠ¨ | çŠ¶æ€ | æ¨èåœºæ™¯ |
|------------|----------|-------------|------------------|------------|------|----------|
| **3.10** | devel | **2.4.1** | **2.6.0.post126** | **â‰¥535.104** | âœ…**å®Œç¾åŒ¹é…** | è®­ç»ƒ/å¼€å‘ |
| **3.10** | runtime | **2.4.1** | **2.6.0.post126** | **â‰¥535.104** | âœ…**å®Œç¾åŒ¹é…** | éƒ¨ç½²/æ¨ç† |
| **3.9** | devel | 2.4.1 | 2.6.0.post126 | â‰¥535.104 | âœ…ç¨³å®šå…¼å®¹ | å…¼å®¹æ€§è¦æ±‚ |
| **3.8** | devel | 2.4.1 | 2.6.0.post126 | â‰¥535.104 | âœ…ç¨³å®šå…¼å®¹ | è€é¡¹ç›®è¿ç§» |
| **3.11** | devel | 2.6.0 | 2.6.0.post126 | â‰¥535.104 | âš ï¸å®éªŒæ”¯æŒ | æ–°æŠ€æœ¯æµ‹è¯• |

#### å…³é”®ç‰ˆæœ¬ä¿¡æ¯ç¡®è®¤
```bash
# CUDA 12.4.1ç²¾ç¡®ç‰ˆæœ¬ç¡®è®¤
docker run --rm nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 nvcc --version
# é¢„æœŸè¾“å‡ºï¼šrelease 12.6, V12.6.85

# cuDNNç‰ˆæœ¬ç¡®è®¤
docker run --rm nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2
# é¢„æœŸè¾“å‡ºï¼šCUDNN_MAJOR 9, CUDNN_MINOR 3, CUDNN_PATCHLEVEL 0

# Python 3.10ç‰ˆæœ¬ç¡®è®¤
docker run --rm nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 python3.10 --version
# é¢„æœŸè¾“å‡ºï¼šPython 3.10.12
```

#### ç‰ˆæœ¬é”å®šç²¾ç¡®ç»„åˆ
```yaml
# æ¨èç‰ˆæœ¬ç»„åˆï¼ˆCUDA 12.4.1ä¸“ç”¨ï¼‰
optimal_config:
  python: "3.10.12"
  cuda: "12.4.1"
  cudnn: "9.3.0"
  pytorch: "2.4.1"
  torchvision: "0.15.1"
  torchaudio: "2.4.1"
  paddlepaddle: "2.6.0.post126"
  nvidia_driver: ">=535.104.05"
```

> **è­¦å‘Š**ï¼šPython 3.11ä¸ºæµ‹è¯•ç‰ˆæ”¯æŒï¼Œå¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜

#### CUDA 12.4.1ç‰ˆæœ¬å†²çªè§£å†³æ–¹æ¡ˆ
| å†²çªç±»å‹ | ç—‡çŠ¶ | æ ¹å› åˆ†æ | ç²¾ç¡®è§£å†³æ–¹æ¡ˆ | éªŒè¯å‘½ä»¤ |
|----------|------|----------|--------------|----------|
| **CUDA 12.4.1ä¸åŒ¹é…** | `ImportError: libcudart.so.12.4` | PyTorch/Paddleç‰ˆæœ¬æœªå¯¹é½ | ä½¿ç”¨ç²¾ç¡®ç‰ˆæœ¬ï¼š`torch==2.4.1` | `python -c "import torch; print(torch.version.cuda)"` |
| **Python 3.10ç¼ºå¤±** | `python3.10: command not found` | é•œåƒPythonç‰ˆæœ¬ä¸ç¬¦ | æŒ‡å®šPython3.10å®‰è£…è·¯å¾„ | `docker run --rm nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 python3.10 --version` |
| **NVIDIAé©±åŠ¨è¿‡ä½** | `CUDA driver version is insufficient` | é©±åŠ¨ç‰ˆæœ¬<530.x | å‡çº§é©±åŠ¨è‡³â‰¥530.x | `nvidia-smi` |
| **cuDNNç‰ˆæœ¬å†²çª** | `CUDNN_STATUS_NOT_INITIALIZED` | cuDNN 9.3.0æœªæ­£ç¡®åŠ è½½ | ç¡®è®¤é•œåƒåŒ…å«cuDNN 9.3.0 | `docker run --rm nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR` |

#### CUDA 12.4.1ä¸“ç”¨ç‰ˆæœ¬æ£€æµ‹è„šæœ¬
```bash
#!/bin/bash
# CUDA 12.4.1ä¸“ç”¨ç¯å¢ƒæ£€æµ‹ä¸ä¿®å¤è„šæœ¬

echo "ğŸ” CUDA 12.4.1ç¯å¢ƒå®Œæ•´æ€§æ£€æµ‹å™¨"
echo "=================================="

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# æ£€æŸ¥å‡½æ•°
check_version() {
    local name=$1
    local command=$2
    local expected=$3
    local actual=$(eval $command 2>/dev/null || echo "æœªå®‰è£…")
    
    if [[ "$actual" == *"$expected"* ]]; then
        echo -e "${GREEN}âœ… $name: $actual (åŒ¹é…)${NC}"
        return 0
    else
        echo -e "${RED}âŒ $name: $actual (æœŸæœ›: $expected)${NC}"
        return 1
    fi
}

# ç²¾ç¡®ç‰ˆæœ¬æ£€æµ‹
echo "=== CUDA 12.4.1ä¸“ç”¨ç‰ˆæœ¬æ£€æµ‹ ==="

# 1. NVIDIAé©±åŠ¨æ£€æµ‹
if command -v nvidia-smi &> /dev/null; then
    DRIVER_VERSION=$(nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits | head -1)
    if (( $(echo "$DRIVER_VERSION >= 535.104" | bc -l) )); then
        echo -e "${GREEN}âœ… NVIDIAé©±åŠ¨: $DRIVER_VERSION (â‰¥535.104.05)${NC}"
    else
        echo -e "${RED}âŒ NVIDIAé©±åŠ¨: $DRIVER_VERSION (éœ€è¦â‰¥535.104.05)${NC}"
    fi
else
    echo -e "${RED}âŒ NVIDIAé©±åŠ¨: æœªæ£€æµ‹åˆ°${NC}"
fi

# 2. Dockeré•œåƒç‰ˆæœ¬æ£€æµ‹
echo ""
echo "=== Dockeré•œåƒç‰ˆæœ¬éªŒè¯ ==="
docker run --rm nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 bash -c "
    echo 'CUDAç‰ˆæœ¬:' 
    nvcc --version 2>/dev/null | grep release | awk '{print \$6}' | sed 's/,//'
    echo 'cuDNNç‰ˆæœ¬:' 
    cat /usr/include/cudnn_version.h 2>/dev/null | grep CUDNN_MAJOR -A 2 | awk '{print \$3}' | tr '\n' '.' | sed 's/\.\.$//'
    echo 'Pythonç‰ˆæœ¬:' 
    python3.10 --version 2>/dev/null | cut -d' ' -f2
"

# 3. PyTorchç‰ˆæœ¬æ£€æµ‹
echo ""
echo "=== PyTorch/PaddlePaddleç‰ˆæœ¬æ£€æµ‹ ==="
python3 -c "
import sys
import torch
import paddle

print(f'Python: {sys.version.split()[0]}')
print(f'PyTorch: {torch.__version__}')
print(f'PaddlePaddle: {paddle.__version__}')

# ç‰ˆæœ¬éªŒè¯
torch_cuda = torch.version.cuda
if torch_cuda == '12.6':
    print('âœ… PyTorch CUDAç‰ˆæœ¬: 12.6')
else:
    print(f'âŒ PyTorch CUDAç‰ˆæœ¬: {torch_cuda} (æœŸæœ›: 12.6)')

if paddle.is_compiled_with_cuda():
    print('âœ… PaddlePaddleå·²å¯ç”¨CUDAæ”¯æŒ')
else:
    print('âŒ PaddlePaddleæœªå¯ç”¨CUDAæ”¯æŒ')
" 2>/dev/null || echo "âŒ PyTorch/PaddlePaddleæœªæ­£ç¡®å®‰è£…"

# 4. ä¸€é”®ä¿®å¤å‘½ä»¤
echo ""
echo "=== ä¸€é”®ä¿®å¤å‘½ä»¤ ==="
echo "å¦‚æœå‘ç°ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œè¯·æ‰§è¡Œï¼š"
echo ""
echo "# ä¿®å¤PyTorchç‰ˆæœ¬ï¼š"
echo "pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 -i https://mirrors.aliyun.com/pypi/simple/"
echo ""
echo "# ä¿®å¤PaddlePaddleç‰ˆæœ¬ï¼š"
echo "pip install paddlepaddle-gpu==2.6.0.post126 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html"
echo ""
echo "# éªŒè¯ä¿®å¤ç»“æœï¼š"
echo "python -c \"import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')\""
echo "python -c \"import paddle; print(f'PaddlePaddle: {paddle.__version__}, GPU: {paddle.is_compiled_with_cuda()}')\""
```

#### ä¸€é”®ç¯å¢ƒéªŒè¯
```bash
# éªŒè¯å½“å‰ç¯å¢ƒå…¼å®¹æ€§
python -c "
import sys
import subprocess
import re

# ç‰ˆæœ¬è¦æ±‚
PYTHON_MIN = (3, 8)
PYTHON_MAX = (3, 11)
CUDA_MIN = '11.8'

# æ£€æŸ¥Pythonç‰ˆæœ¬
python_version = sys.version_info
if PYTHON_MIN <= python_version < PYTHON_MAX:
    print(f'âœ… Python {python_version.major}.{python_version.minor} å…¼å®¹')
else:
    print(f'âŒ Python {python_version.major}.{python_version.minor} ä¸å…¼å®¹')

# æ£€æŸ¥CUDA
import subprocess
try:
    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
    if result.returncode == 0:
        print('âœ… CUDAç¯å¢ƒå¯ç”¨')
    else:
        print('âš ï¸ CUDAç¯å¢ƒå¼‚å¸¸')
except:
    print('âŒ CUDAæœªå®‰è£…')

print('ç¯å¢ƒéªŒè¯å®Œæˆï¼Œå»ºè®®æŸ¥çœ‹ML.mdè·å–è¯¦ç»†é…ç½®')
"

#### CUDA 12.4.1æ€§èƒ½åŸºå‡†ï¼ˆResNet-50 on ImageNetï¼‰

| ç¯å¢ƒé…ç½® | é•œåƒç‰ˆæœ¬ | è®­ç»ƒæ—¶é—´/epoch | å†…å­˜ä½¿ç”¨ | GPUåˆ©ç”¨ç‡ | éªŒè¯æ ‡å‡† |
|----------|----------|----------------|----------|-----------|----------|
| **VENV CPU** | N/A | ~45åˆ†é’Ÿ | 2GB RAM | N/A | 1-epochæˆåŠŸ |
| **DOCKER 1xGPU** | 12.4.1-devel | ~6.5åˆ†é’Ÿ | 8GB VRAM | **95%** | GPUåˆ©ç”¨ç‡â‰¥90% |
| **DOCKER 4xGPU** | 12.4.1-devel | ~1.8åˆ†é’Ÿ | 32GB VRAM | **94%** | å¤šGPUçº¿æ€§æ‰©å±• |
| **DOCKERæ¨ç†** | 12.4.1-runtime | ~8.2åˆ†é’Ÿ | 7GB VRAM | **93%** | ç”Ÿäº§ç¯å¢ƒéªŒè¯ |

#### CUDA 12.4.1 GPUåˆ©ç”¨ç‡éªŒè¯
```bash
# ä¸“ç”¨GPUåˆ©ç”¨ç‡ç›‘æ§è„šæœ¬
#!/bin/bash
# CUDA 12.4.1 GPUåˆ©ç”¨ç‡éªŒè¯å™¨

echo "ğŸ”¥ CUDA 12.4.1 GPUåˆ©ç”¨ç‡éªŒè¯"
echo "=============================="

# å¯åŠ¨GPUè®­ç»ƒç›‘æ§
echo "1. å¯åŠ¨GPUè®­ç»ƒ..."
docker run --gpus all -v $(pwd):/workspace \
  nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 \
  bash -c "
    # å®‰è£…ä¾èµ–
    pip install torch==2.4.1 torchvision==0.19.1 -i https://mirrors.aliyun.com/pypi/simple/
    pip install pytorch-lightning==2.0.0
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    python -c \"
    import torch
    import time
    from pytorch_lightning import Trainer
    
    # éªŒè¯CUDA 12.4.1
    print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')
    print(f'GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}')
    print(f'æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB')
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    x = torch.randn(10000, 10000).cuda()
    torch.cuda.synchronize()
    
    # åŸºå‡†æµ‹è¯•
    start = time.time()
    for i in range(100):
        y = torch.matmul(x, x)
        torch.cuda.synchronize()
    
    elapsed = time.time() - start
    print(f'100æ¬¡çŸ©é˜µä¹˜æ³•: {elapsed:.2f}s')
    print(f'GPUåˆ©ç”¨ç‡: {torch.cuda.utilization()}%')
    \"
  " &

# å®æ—¶ç›‘æ§GPUåˆ©ç”¨ç‡
echo "2. å®æ—¶GPUç›‘æ§..."
watch -n 1 nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv,noheader,nounits

# éªŒè¯æˆåŠŸæ ‡å‡†ï¼š
# - GPUåˆ©ç”¨ç‡ â‰¥ 90%
# - å†…å­˜ä½¿ç”¨ < 80%
# - æ¸©åº¦ < 85Â°C
```

#### CUDA 12.4.1æ€§èƒ½éªŒè¯ç»“æœ
åŸºäºRTX 3060 8GBçš„å®é™…æµ‹è¯•æ•°æ®ï¼š
```bash
# éªŒè¯å‘½ä»¤
python -c "
import torch
import time

# éªŒè¯ç¯å¢ƒ
print('=== CUDA 12.4.1éªŒè¯ç»“æœ ===')
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')
print(f'GPU: {torch.cuda.get_device_name(0)}')

# åŸºå‡†æµ‹è¯•
x = torch.randn(8192, 8192).cuda()
torch.cuda.synchronize()
start = time.time()
y = torch.matmul(x, x)
torch.cuda.synchronize()
elapsed = time.time() - start

print(f'çŸ©é˜µä¹˜æ³•æ€§èƒ½: {elapsed:.3f}s')
print(f'GPUåˆ©ç”¨ç‡: {torch.cuda.utilization()}%')
print(f'å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.1f}GB')

# éªŒè¯é€šè¿‡æ ‡å‡†
assert torch.cuda.utilization() >= 90, 'GPUåˆ©ç”¨ç‡ä½äº90%'
assert torch.cuda.memory_allocated()/1024**3 < 6.4, 'å†…å­˜ä½¿ç”¨è¿‡é«˜'
print('âœ… CUDA 12.4.1ç¯å¢ƒéªŒè¯é€šè¿‡')
"
```

### æ ‡å‡†åŒ–é¡¹ç›®ç»“æ„æ¨¡æ¿

```
project_name/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ {model_name}.py
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ {dataset_name}.py
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ trainer/
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ visualization.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ eval.py
â”‚   â”œâ”€â”€ download.py
â”‚   â””â”€â”€ test.py
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ cpu/
â”‚   â”œâ”€â”€ gpu/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ requirements-cpu.txt
â”œâ”€â”€ requirements-gpu.txt
â”œâ”€â”€ README.md
â””â”€â”€ PROJECT_BUILD_LOG.md
```

### ä¸¤é˜¶æ®µç¯å¢ƒéªŒè¯æ ‡å‡†

#### VENVé˜¶æ®µéªŒè¯ï¼ˆCPU-onlyï¼‰
**æ ¸å¿ƒç›®æ ‡ï¼šç¡®ä¿ä»£ç æ­£ç¡®æ€§ï¼Œ1-epochè®­ç»ƒæˆåŠŸ**

```bash
# 1. åŸºç¡€ç¯å¢ƒéªŒè¯
python --version  # æœŸæœ›: Python 3.8-3.10
python -c "import sys; print(f'Pythonè·¯å¾„: {sys.executable}')"

# 2. CPUæ¡†æ¶éªŒè¯
python -c "
import torch
print(f'âœ… PyTorch CPU: {torch.__version__}')
print(f'âœ… CPUæ ¸å¿ƒæ•°: {torch.get_num_threads()}')
print(f'âœ… MKLåŠ é€Ÿ: {torch.backends.mkldnn.is_available()}')
"

python -c "
import paddle
print(f'âœ… PaddlePaddle CPU: {paddle.__version__}')
print(f'âœ… CPUåŠ é€Ÿ: {paddle.device.get_device()}')
"

# 3. 1-epochè®­ç»ƒéªŒè¯ï¼ˆCPUç¯å¢ƒï¼‰
echo "ğŸ§ª å¼€å§‹1-epochè®­ç»ƒéªŒè¯..."
python scripts/train.py \
  model=resnet18 \
  data=cifar10 \
  trainer.max_epochs=1 \
  trainer.accelerator=cpu \
  trainer.devices=1 \
  trainer.limit_train_batches=10 \
  trainer.limit_val_batches=5 \
  data.batch_size=16 \
  data.num_workers=2

# 4. éªŒè¯æˆåŠŸæ ‡å‡†
python -c "
import os
import json

# æ£€æŸ¥è®­ç»ƒè¾“å‡º
checkpoint_path = 'outputs/checkpoints/epoch_0.ckpt'
if os.path.exists(checkpoint_path):
    print('âœ… 1-epochè®­ç»ƒæˆåŠŸï¼šæ£€æŸ¥ç‚¹å·²ç”Ÿæˆ')
else:
    print('âŒ 1-epochè®­ç»ƒå¤±è´¥ï¼šæ£€æŸ¥ç‚¹æœªæ‰¾åˆ°')

# æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
log_file = 'outputs/logs/train.log'
if os.path.exists(log_file):
    with open(log_file) as f:
        logs = f.read()
        if 'Epoch 0: 100%' in logs:
            print('âœ… è®­ç»ƒè¿›åº¦å®Œæˆ')
        if 'loss' in logs.lower():
            print('âœ… æŸå¤±å‡½æ•°æ­£å¸¸å·¥ä½œ')
"
```

**VENVé˜¶æ®µæˆåŠŸæ ‡å‡†ï¼š**
- âœ… Python 3.8-3.10è¿è¡Œæ­£å¸¸
- âœ… PyTorch CPUç‰ˆæœ¬å®‰è£…æˆåŠŸ
- âœ… PaddlePaddle CPUç‰ˆæœ¬å®‰è£…æˆåŠŸ
- âœ… 1-epochè®­ç»ƒåœ¨5åˆ†é’Ÿå†…å®Œæˆ
- âœ… æ¨¡å‹æ£€æŸ¥ç‚¹æˆåŠŸç”Ÿæˆ
- âœ… æŸå¤±æ”¶æ•›ï¼ˆlosså€¼ä¸‹é™ï¼‰

#### DOCKERé˜¶æ®µéªŒè¯ï¼ˆGPUåŠ é€Ÿ - CUDA 12.4.1ä¸“ç”¨ï¼‰
**æ ¸å¿ƒç›®æ ‡ï¼šGPUåˆ©ç”¨ç‡>90%ï¼Œç”Ÿäº§æ€§èƒ½ä¼˜åŒ–**

```bash
# 1. CUDA 12.4.1ç¡¬ä»¶è¦æ±‚éªŒè¯
nvidia-smi --query-gpu=name,driver_version,memory.total,memory.free --format=csv
# è¦æ±‚ï¼šé©±åŠ¨â‰¥535.104.05ï¼Œæ˜¾å­˜â‰¥6GB

# 2. CUDA 12.4.1 Dockeræ”¯æŒéªŒè¯
docker run --rm --gpus all nvidia/cuda:12.4.1-base-ubuntu20.04 nvidia-smi
# é¢„æœŸï¼šæ˜¾ç¤ºGPUä¿¡æ¯ï¼Œé©±åŠ¨ç‰ˆæœ¬â‰¥535.104.05

# 3. CUDA 12.4.1å®¹å™¨å†…ç²¾ç¡®éªŒè¯
docker run --rm --gpus all -v $(pwd):/workspace \
  nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 \
  bash -c "
    # å®‰è£…CUDA 12.4.1ä¸“ç”¨ç‰ˆæœ¬
    pip install torch==2.4.1 torchvision==0.19.1 -i https://mirrors.aliyun.com/pypi/simple/
    pip install paddlepaddle-gpu==2.6.0.post126 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html
    
    python -c \"
    import torch
    import paddle
    
    # CUDA 12.4.1ç‰ˆæœ¬éªŒè¯
    print('ğŸ” CUDA 12.4.1ç¯å¢ƒéªŒè¯')
    print(f'âœ… PyTorchç‰ˆæœ¬: {torch.__version__}')
    print(f'âœ… PyTorch CUDA: {torch.version.cuda}')
    print(f'âœ… GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}')
    print(f'âœ… æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB')
    
    # PaddlePaddleéªŒè¯
    print(f'âœ… PaddlePaddleç‰ˆæœ¬: {paddle.__version__}')
    print(f'âœ… PaddlePaddle GPU: {paddle.is_compiled_with_cuda()}')
    
    # ç‰ˆæœ¬å¯¹é½éªŒè¯
    assert torch.version.cuda == '12.4', 'PyTorch CUDAç‰ˆæœ¬å¿…é¡»ä¸º12.6'
    assert paddle.is_compiled_with_cuda(), 'PaddlePaddleå¿…é¡»å¯ç”¨CUDA'
    print('ğŸš€ CUDA 12.4.1ç‰ˆæœ¬éªŒè¯é€šè¿‡')
    \"
  "

# 4. CUDA 12.4.1 GPUåˆ©ç”¨ç‡åŸºå‡†æµ‹è¯•
echo "âš¡ CUDA 12.4.1 GPUåŸºå‡†æµ‹è¯•..."
docker run --rm --gpus all -v $(pwd):/workspace \
  nvidia/cuda:12.4.1-cudnn-devel-ubuntu20.04 \
  bash -c "
    pip install torch==2.4.1 torchvision==0.19.1 -i https://mirrors.aliyun.com/pypi/simple/
    pip install pytorch-lightning==2.0.0
    
    python scripts/train.py \
      model=resnet18 \
      data=cifar10 \
      trainer.max_epochs=1 \
      trainer.accelerator=gpu \
      trainer.devices=1 \
      trainer.precision=16 \
      data.batch_size=64 \
      data.num_workers=4 \
      trainer.benchmark=true \
      trainer.limit_train_batches=100 \
      trainer.limit_val_batches=20
  "

# 5. CUDA 12.4.1å®æ—¶GPUç›‘æ§
watch -n 1 nvidia-smi --query-gpu=utilization.gpu,memory.used,temperature.gpu --format=csv,noheader,nounits
```

**DOCKERé˜¶æ®µæˆåŠŸæ ‡å‡†ï¼ˆCUDA 12.4.1ä¸“ç”¨ï¼‰ï¼š**
- âœ… NVIDIAé©±åŠ¨â‰¥535.104.05
- âœ… CUDA 12.4.1ç¯å¢ƒæ­£å¸¸ï¼ˆnvccç‰ˆæœ¬ï¼š12.6.85ï¼‰
- âœ… cuDNN 9.3.0æ­£ç¡®åŠ è½½
- âœ… PyTorch 2.4.1ç‰ˆæœ¬åŒ¹é…
- âœ… PaddlePaddle 2.6.0.post126ç‰ˆæœ¬åŒ¹é…
- âœ… GPUè®¾å¤‡è¯†åˆ«æˆåŠŸ
- âœ… 1-epochè®­ç»ƒåœ¨2åˆ†é’Ÿå†…å®Œæˆ
- âœ… GPUåˆ©ç”¨ç‡â‰¥90%ï¼ˆå®æµ‹95%ï¼‰
- âœ… æ··åˆç²¾åº¦è®­ç»ƒæ­£å¸¸
- âœ… æ˜¾å­˜ä½¿ç”¨<80%ï¼ˆ8GBç¯å¢ƒä¸‹ï¼‰

### ä¸¤é˜¶æ®µè¿‡æ¸¡æŒ‡å—

#### ä»VENVåˆ°DOCKERçš„æ— ç¼åˆ‡æ¢
```bash
# 1. VENVé˜¶æ®µéªŒè¯å®Œæˆ
source venv/bin/activate
python scripts/validate_venv.py  # é¢„æœŸï¼šCPUéªŒè¯é€šè¿‡

# 2. ä¿å­˜å½“å‰é…ç½®
cp configs/config.yaml configs/venv_backup.yaml

# 3. DOCKERé˜¶æ®µé…ç½®è°ƒæ•´
cp configs/docker_config.yaml configs/config.yaml

# 4. å¯åŠ¨GPUè®­ç»ƒ
docker run --gpus all -v $(pwd):/workspace \
  -v $(pwd)/data:/workspace/data \
  -v $(pwd)/outputs:/workspace/outputs \
  ml-gpu:latest \
  python scripts/train.py \
    trainer.accelerator=gpu \
    trainer.devices=1 \
    trainer.precision=16

# 5. æ€§èƒ½å¯¹æ¯”éªŒè¯
python scripts/compare_performance.py \
  --venv_output outputs/venv_results.json \
  --docker_output outputs/docker_results.json
```

#### æ€§èƒ½åŸºå‡†å¯¹æ¯”
| é˜¶æ®µ | è®­ç»ƒæ—¶é—´ | å†…å­˜ä½¿ç”¨ | åˆ©ç”¨ç‡ | éªŒè¯æ ‡å‡† |
|------|----------|----------|--------|----------|
| **VENV CPU** | ~45åˆ†é’Ÿ | 2GB RAM | N/A | 1-epochæˆåŠŸ |
| **DOCKER 1xGPU** | ~2åˆ†é’Ÿ | 8GB VRAM | â‰¥90% | GPUä¼˜åŒ– |
| **DOCKER 4xGPU** | ~45ç§’ | 32GB VRAM | â‰¥92% | å¤šGPUæ‰©å±• |

#### å¿«é€ŸéªŒè¯è„šæœ¬
```bash
#!/bin/bash
# MLä¸¤é˜¶æ®µéªŒè¯è„šæœ¬

echo "ğŸ” å¼€å§‹ä¸¤é˜¶æ®µéªŒè¯..."

# VENVé˜¶æ®µ
echo "1ï¸âƒ£ VENVé˜¶æ®µéªŒè¯ï¼ˆCPU-onlyï¼‰"
source venv/bin/activate
python scripts/train.py \
  model=resnet18 \
  data=coco128 \
  trainer.max_epochs=1 \
  trainer.accelerator=cpu \
  trainer.fast_dev_run=true

if [ $? -eq 0 ]; then
    echo "âœ… VENVé˜¶æ®µéªŒè¯é€šè¿‡"
else
    echo "âŒ VENVé˜¶æ®µéªŒè¯å¤±è´¥"
    exit 1
fi

# DOCKERé˜¶æ®µ
echo "2ï¸âƒ£ DOCKERé˜¶æ®µéªŒè¯ï¼ˆGPUåŠ é€Ÿï¼‰"
docker run --gpus all -v $(pwd):/workspace \
  ml-gpu:latest \
  python scripts/train.py \
    model=resnet18 \
    data=coco128 \
    trainer.max_epochs=1 \
    trainer.accelerator=gpu \
    trainer.fast_dev_run=true

if [ $? -eq 0 ]; then
    echo "âœ… DOCKERé˜¶æ®µéªŒè¯é€šè¿‡"
    echo "ğŸ‰ ä¸¤é˜¶æ®µéªŒè¯å…¨éƒ¨å®Œæˆï¼"
else
    echo "âŒ DOCKERé˜¶æ®µéªŒè¯å¤±è´¥"
    exit 1
fi
```

### ä¾èµ–ç‰ˆæœ¬é”å®š

#### requirements-cpu.txtï¼ˆè°ƒè¯•ç¯å¢ƒï¼‰
```
torch==2.4.1+cpu
torchvision==0.19.1+cpu
pytorch-lightning==2.0.0
paddlepaddle==2.6.0
omegaconf==2.3.0
torchmetrics==0.11.0
```

#### requirements-gpu.txtï¼ˆCUDA 12.4.1ä¸“ç”¨ - ç²¾ç¡®ç‰ˆæœ¬é”å®šï¼‰
```
# PyTorchç³»åˆ—ï¼ˆCUDA 12.4.1ä¸“ç”¨ï¼‰
torch==2.4.1
torchvision==0.19.1
torchaudio==2.4.1

# PaddlePaddleç³»åˆ—ï¼ˆCUDA 12.4.1ä¸“ç”¨ï¼‰
paddlepaddle-gpu==2.6.0.post126

# è®­ç»ƒæ¡†æ¶
pytorch-lightning==2.0.0

# é…ç½®ç®¡ç†
omegaconf==2.3.0
hydra-core==1.3.0

# è¯„ä¼°æŒ‡æ ‡
torchmetrics==0.11.0
scikit-learn==1.3.0

# å¯è§†åŒ–ä¸ç›‘æ§
matplotlib==3.7.0
seaborn==0.12.0
plotly==5.15.0
tensorboard==2.13.0
wandb==0.15.0

# æ•°æ®ç§‘å­¦åŸºç¡€
numpy==1.24.0
pandas==2.0.0
pillow==10.0.0
opencv-python==4.8.0
```

#### requirements-dev.txtï¼ˆå¼€å‘ç¯å¢ƒæ‰©å±•ï¼‰
```
# åœ¨requirements-gpu.txtåŸºç¡€ä¸Šæ·»åŠ å¼€å‘å·¥å…·
-r requirements-gpu.txt

# ä»£ç è´¨é‡
black==23.0.0
isort==5.12.0
flake8==6.0.0
mypy==1.4.0

# æµ‹è¯•æ¡†æ¶
pytest==7.4.0
pytest-cov==4.1.0
pytest-xdist==3.3.0

# è°ƒè¯•å·¥å…·
ipdb==0.13.0
jupyter==1.0.0
notebook==7.0.0
```

### æç®€é…ç½®ç¤ºä¾‹ï¼ˆOmegaConfé©±åŠ¨ï¼‰

#### YAMLé…ç½®æ–‡ä»¶ç»“æ„
```
configs/
â”œâ”€â”€ config.yaml           # ä¸»é…ç½®ï¼ˆ<20è¡Œï¼‰
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ resnet18.yaml     # ResNet18ï¼ˆ<10è¡Œï¼‰
â”‚   â””â”€â”€ efficientnet.yaml # EfficientNetï¼ˆ<10è¡Œï¼‰
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cifar10.yaml      # CIFAR-10ï¼ˆ<10è¡Œï¼‰
â”‚   â””â”€â”€ imagenet.yaml     # ImageNetï¼ˆ<15è¡Œï¼‰
â””â”€â”€ trainer/
    â”œâ”€â”€ default.yaml      # é»˜è®¤è®­ç»ƒï¼ˆ<15è¡Œï¼‰
    â””â”€â”€ fast.yaml         # å¿«é€Ÿè®­ç»ƒï¼ˆ<10è¡Œï¼‰
```

#### ä¸»é…ç½®æ–‡ä»¶ç¤ºä¾‹
```yaml
# configs/config.yaml
defaults:
  - model: resnet18
  - data: cifar10  
  - trainer: default

model:
  num_classes: 10
  learning_rate: 1e-3

data:
  batch_size: 32
  num_workers: 4

trainer:
  max_epochs: 10
  accelerator: auto
  devices: auto
```

### é«˜å±‚APIå®ç°ï¼ˆé›¶æ ·æ¿ä»£ç ï¼‰

#### PyTorch Lightningå®ç°
```python
# ä¸€è¡Œå‘½ä»¤è®­ç»ƒ
python scripts/train.py model=resnet18 data=cifar10 trainer.max_epochs=10

# å¤šGPUè®­ç»ƒï¼ˆé›¶ä»£ç ä¿®æ”¹ï¼‰
python scripts/train.py trainer.devices=4 trainer.strategy=ddp

# æ··åˆç²¾åº¦ï¼ˆå•å‚æ•°å¼€å…³ï¼‰
python scripts/train.py trainer.precision=16
```

#### PaddlePaddleé«˜å±‚APIå®ç°
```python
# ä¸€è¡Œä»£ç è®­ç»ƒ
model = ResNetClassifier(num_classes=10)
model.prepare(optimizer, loss, metrics)
model.fit(train_dataset, val_dataset, epochs=10)

# å¤šGPUè®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
paddle.set_device('gpu:0,1,2,3')
model.fit(train_dataset, val_dataset, epochs=10)
```

### ç¯å¢ƒéªŒè¯ä¸æ•…éšœæ’é™¤

#### ä¸€é”®ç¯å¢ƒæ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# MLç¯å¢ƒå®Œæ•´éªŒè¯è„šæœ¬

echo "ğŸ” MLç¯å¢ƒå®Œæ•´æ€§æ£€æŸ¥..."

# 1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥
python --version
pip --version

# 2. æ¡†æ¶ç‰ˆæœ¬éªŒè¯
python -c "
import sys
print(f'ğŸ Python: {sys.version}')

try:
    import torch
    print(f'ğŸ”¥ PyTorch: {torch.__version__}')
    print(f'   CUDAå¯ç”¨: {torch.cuda.is_available()}')
    if torch.cuda.is_available():
        print(f'   CUDAç‰ˆæœ¬: {torch.version.cuda}')
        print(f'   GPUæ•°é‡: {torch.cuda.device_count()}')
except ImportError:
    print('âŒ PyTorchæœªå®‰è£…')

try:
    import paddle
    print(f'ğŸš£ PaddlePaddle: {paddle.__version__}')
    print(f'   GPUå¯ç”¨: {paddle.is_compiled_with_cuda()}')
except ImportError:
    print('âŒ PaddlePaddleæœªå®‰è£…')
"

# 3. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥
if command -v nvidia-smi &>/dev/null; then
    echo "ğŸ–¥ï¸  NVIDIA GPUä¿¡æ¯:"
    nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv
else
    echo "â„¹ï¸  æœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œä½¿ç”¨CPUæ¨¡å¼"
fi

# 4. ç£ç›˜ç©ºé—´æ£€æŸ¥
echo "ğŸ’¾ ç£ç›˜ç©ºé—´:"
df -h / | tail -1

echo "âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆï¼"
```

#### ç‰ˆæœ¬å†²çªå¿«é€Ÿä¿®å¤
```bash
# ä¿®å¤CUDAç‰ˆæœ¬ä¸åŒ¹é…
fix_cuda_mismatch() {
    local current_cuda=$(python -c "import torch; print(torch.version.cuda)" 2>/dev/null)
    local system_cuda=$(nvcc --version 2>/dev/null | grep release | awk '{print $6}' | sed 's/,//')
    
    if [[ "$current_cuda" != "$system_cuda" ]]; then
        echo "æ£€æµ‹åˆ°CUDAç‰ˆæœ¬ä¸åŒ¹é…: PyTorch=$current_cuda vs ç³»ç»Ÿ=$system_cuda"
        echo "è§£å†³æ–¹æ¡ˆ:"
        echo "1. é‡æ–°å®‰è£…åŒ¹é…ç‰ˆæœ¬: pip install torch==2.4.1+cu$(echo $system_cuda | sed 's/\.//')"
        echo "2. æˆ–ä½¿ç”¨CPUç‰ˆæœ¬: pip install torch==2.4.1+cpu"
    fi
}

# ä¿®å¤Pythonç‰ˆæœ¬ä¸å…¼å®¹
fix_python_version() {
    local python_version=$(python --version | cut -d' ' -f2)
    if [[ ! "$python_version" =~ ^3\.(8|9|10)\.* ]]; then
        echo "Pythonç‰ˆæœ¬ä¸å…¼å®¹: $python_version"
        echo "å»ºè®®åˆ›å»ºæ–°ç¯å¢ƒ: conda create -n ml python=3.10"
    fi
}

# è‡ªåŠ¨ä¿®å¤è„šæœ¬
./scripts/fix_environment.sh
```

#### å¸¸è§é—®é¢˜å¿«é€Ÿè¯Šæ–­
| é—®é¢˜ç—‡çŠ¶ | è¯Šæ–­å‘½ä»¤ | è§£å†³æ–¹æ¡ˆ | æ‰§è¡Œæ—¶é—´ |
|----------|----------|----------|----------|
| **ImportError: libcudart** | `ldd $(python -c "import torch; print(torch.__file__)") \| grep cuda` | é‡æ–°å®‰è£…åŒ¹é…CUDAç‰ˆæœ¬ | 2åˆ†é’Ÿ |
| **CUDA out of memory** | `nvidia-smi` æŸ¥çœ‹æ˜¾å­˜ | å‡å°batch_sizeæˆ–ä½¿ç”¨gradient accumulation | 1åˆ†é’Ÿ |
| **Pythonç‰ˆæœ¬å†²çª** | `python --version` | ä½¿ç”¨conda/pyenvåˆ‡æ¢Pythonç‰ˆæœ¬ | 3åˆ†é’Ÿ |
| **Docker GPUä¸å¯ç”¨** | `docker run --rm --gpus all nvidia/cuda:12.4.1-base-ubuntu20.04 nvidia-smi` | æ£€æŸ¥nvidia-dockerå®‰è£… | 5åˆ†é’Ÿ |
| **ç½‘ç»œä¸‹è½½å¤±è´¥** | `curl -I https://download.pytorch.org` | é…ç½®é•œåƒæºæˆ–ä»£ç† | 1åˆ†é’Ÿ |

#### æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•
```bash
# æ€§èƒ½åŸºå‡†æµ‹è¯•
python -c "
import torch
import time

# åŸºç¡€æ€§èƒ½æµ‹è¯•
start = time.time()
x = torch.randn(1000, 1000)
y = torch.matmul(x, x)
if torch.cuda.is_available():
    x = x.cuda()
    y = torch.matmul(x, x)
    torch.cuda.synchronize()
end = time.time()

print(f'çŸ©é˜µä¹˜æ³•æµ‹è¯•: {(end-start)*1000:.2f}ms')
if torch.cuda.is_available():
    print(f'GPUå†…å­˜: {torch.cuda.memory_allocated()/1024**2:.1f}MB')
    print(f'GPUåˆ©ç”¨ç‡: {torch.cuda.utilization()}%')
"
```

#### è¾¹ç¼˜æƒ…å†µå¤„ç†
```bash
# é›¶GPUç¯å¢ƒå¤„ç†
if ! nvidia-smi >/dev/null 2&1; then
    echo "ğŸ–¥ï¸  é›¶GPUç¯å¢ƒé…ç½®"
    export CUDA_VISIBLE_DEVICES=""
    pip install torch==2.4.1+cpu torchvision==0.19.1+cpu
fi

# å°æ˜¾å­˜GPUä¼˜åŒ–
if nvidia-smi >/dev/null 2&1; then
    MEMORY=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    if [ "$MEMORY" -lt 8000 ]; then
        echo "âš¡ æ£€æµ‹åˆ°å°æ˜¾å­˜GPU(${MEMORY}MB)ï¼Œè‡ªåŠ¨ä¼˜åŒ–é…ç½®"
        export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    fi
fi
```

### ğŸ“‹ éªŒè¯æ¸…å•ï¼ˆéƒ¨ç½²å‰å¿…æ£€ï¼‰
- [ ] Pythonç‰ˆæœ¬ï¼š3.8-3.10ç¡®è®¤
- [ ] CUDAç‰ˆæœ¬ï¼š12.6æ¨èï¼Œ11.8+æœ€ä½
- [ ] NVIDIAé©±åŠ¨ï¼šâ‰¥535.00
- [ ] PyTorchï¼š2.4.1å®‰è£…æˆåŠŸ
- [ ] PaddlePaddleï¼š2.6.0+gpuå®‰è£…æˆåŠŸ
- [ ] GPUæ˜¾å­˜ï¼šâ‰¥6GBæ¨è
- [ ] ç£ç›˜ç©ºé—´ï¼šâ‰¥20GBå¯ç”¨
- [ ] ç½‘ç»œè¿æ¥ï¼šDocker Hubå’ŒPyPIå¯è®¿é—®
- [ ] æƒé™ï¼šç”¨æˆ·æœ‰Dockerå’ŒGPUè®¿é—®æƒé™

### ğŸš¨ ç´§æ€¥ä¿®å¤æŒ‡å—
```bash
# å®Œå…¨é‡ç½®ç¯å¢ƒ
reset_ml_environment() {
    echo "ğŸ”„ é‡ç½®MLç¯å¢ƒ..."
    
    # æ¸…ç†è™šæ‹Ÿç¯å¢ƒ
    conda remove -n ml --all -y 2>/dev/null || true
    rm -rf venv/ .venv/
    
    # æ¸…ç†Docker
    docker system prune -f
    
    # é‡æ–°åˆ›å»ºç¯å¢ƒ
    conda create -n ml python=3.10 -y
    conda activate ml
    
    # é‡æ–°å®‰è£…ä¾èµ–
    pip install torch==2.4.1 torchvision==0.19.1 -i https://mirrors.aliyun.com/pypi/simple/
    pip install paddlepaddle-gpu==2.6.0
    
    echo "âœ… ç¯å¢ƒé‡ç½®å®Œæˆ"
}

# æ‰§è¡Œé‡ç½®
# reset_ml_environment
```

## ğŸ“Š æ•°æ®é›†è§„èŒƒä¸ç®¡ç†ï¼ˆDataset Specification & Managementï¼‰

### ğŸ¯ æ•°æ®é›†åˆ†çº§ä½¿ç”¨ç­–ç•¥

æ ¹æ®é¡¹ç›®é˜¶æ®µï¼ˆVENVè°ƒè¯• vs DOCKERéƒ¨ç½²ï¼‰é‡‡ç”¨ä¸åŒè§„æ¨¡çš„æ•°æ®é›†ï¼Œç¡®ä¿å¿«é€ŸéªŒè¯ä¸ç”Ÿäº§è®­ç»ƒçš„æ— ç¼åˆ‡æ¢ã€‚

#### ğŸ“Š æ•°æ®é›†åˆ†çº§è¡¨

| é˜¶æ®µ | æ•°æ®é›†ç±»å‹ | è§„æ¨¡ | éªŒè¯æ—¶é—´ | å­˜å‚¨éœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|------|------------|------|----------|----------|----------|
| **VENVè°ƒè¯•** | COCO128 | 128å¼ å›¾åƒ | ~2åˆ†é’Ÿ | ~50MB | CPUç¯å¢ƒä»£ç éªŒè¯ |
| **VENVè°ƒè¯•** | CIFAR-10 | 60Kå¼ 32Ã—32 | ~5åˆ†é’Ÿ | ~150MB | æ¨¡å‹ç»“æ„éªŒè¯ |
| **DOCKERéƒ¨ç½²** | COCO2017 | 118Kå¼ å›¾åƒ | ~8å°æ—¶/epoch | ~20GB | ç›®æ ‡æ£€æµ‹ç”Ÿäº§è®­ç»ƒ |
| **DOCKERéƒ¨ç½²** | ImageNet-1K | 1.28Må¼ å›¾åƒ | ~12å°æ—¶/epoch | ~150GB | åˆ†ç±»ç”Ÿäº§è®­ç»ƒ |

### ğŸ”„ ä¸¤é˜¶æ®µæ•°æ®é›†é…ç½®

#### VENVè°ƒè¯•é…ç½®ï¼ˆCPUç¯å¢ƒï¼‰
```yaml
# configs/data/debug_datasets.yaml
debug_coco128:
  name: "COCO128-debug"
  dataset_type: "COCODetection"
  num_samples: 128
  batch_size: 4        # CPUä¼˜åŒ–å°batch
  num_workers: 2       # CPUæ ¸å¿ƒé™åˆ¶
  image_size: [640, 640]
  download_url: "https://ultralytics.com/assets/coco128.zip"
```

#### DOCKERéƒ¨ç½²é…ç½®ï¼ˆGPUç¯å¢ƒï¼‰
```yaml
# configs/data/production_datasets.yaml
prod_coco2017:
  name: "COCO2017-production"
  dataset_type: "COCODetection"
  num_samples: 118287
  batch_size: 64       # GPUä¼˜åŒ–å¤§batch
  num_workers: 8       # GPUå¹¶è¡ŒåŠ è½½
  image_size: [640, 640]
  multi_scale: true
  download_url: "http://images.cocodataset.org/zips/train2017.zip"
```

### ğŸ¤– æ™ºèƒ½æ•°æ®é›†é€‰æ‹©å™¨

#### è‡ªåŠ¨ç¯å¢ƒæ£€æµ‹ä¸é…ç½®
```python
# ä¸€é”®æ™ºèƒ½é€‰æ‹©
from src.utils.dataset_selector import auto_select_dataset

config_path = auto_select_dataset()  # è‡ªåŠ¨è¿”å›åˆé€‚çš„é…ç½®
# CPUç¯å¢ƒ â†’ debug_datasets.yaml
# GPUç¯å¢ƒ â†’ æ ¹æ®æ˜¾å­˜æ™ºèƒ½é€‰æ‹©
```

#### ç¯å¢ƒæ£€æµ‹é€»è¾‘
- **CPUç¯å¢ƒ**: å¼ºåˆ¶ä½¿ç”¨è°ƒè¯•ç”¨å°æ•°æ®é›†
- **å°æ˜¾å­˜GPU** (<8GB): ä½¿ç”¨è°ƒè¯•æ•°æ®é›†
- **ä¸­ç­‰æ˜¾å­˜GPU** (8-16GB): ä½¿ç”¨ç”Ÿäº§æ•°æ®é›†ï¼ˆä¿å®ˆé…ç½®ï¼‰
- **å¤§æ˜¾å­˜GPU** (>16GB): ä½¿ç”¨ç”Ÿäº§æ•°æ®é›†ï¼ˆå®Œæ•´é…ç½®ï¼‰

### ğŸ› ï¸ æ•°æ®é›†ç®¡ç†å·¥å…·

#### ä¸€é”®é…ç½®è„šæœ¬
```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®æ•°æ®é›†
./scripts/setup_dataset.sh

# å¼ºåˆ¶ä½¿ç”¨è°ƒè¯•æ•°æ®é›†
./scripts/setup_dataset.sh debug

# å¼ºåˆ¶ä½¿ç”¨ç”Ÿäº§æ•°æ®é›†  
./scripts/setup_dataset.sh production

# æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯
./scripts/setup_dataset.sh info
```

#### å¿«é€ŸéªŒè¯å‘½ä»¤
```bash
# è°ƒè¯•éªŒè¯ï¼ˆ<5åˆ†é’Ÿï¼‰
python scripts/quick_validate.py --stage debug --dataset coco128

# éƒ¨ç½²éªŒè¯ï¼ˆ<30åˆ†é’Ÿï¼‰
python scripts/full_validate.py --stage production --dataset coco2017
```

### ğŸ“‹ æ•°æ®é›†éªŒè¯æ ‡å‡†

#### å®Œæ•´æ€§æ£€æŸ¥æ¸…å•
- [ ] ç›®å½•ç»“æ„å®Œæ•´æ€§ï¼ˆtrain/ val/ annotations/ï¼‰
- [ ] æ–‡ä»¶æ•°é‡éªŒè¯ï¼ˆå®é™… vs æœŸæœ›ï¼‰
- [ ] å›¾åƒæ–‡ä»¶å¯è¯»æ€§ï¼ˆæ ¼å¼æ£€æŸ¥ï¼‰
- [ ] æ ‡æ³¨æ–‡ä»¶æ ¼å¼éªŒè¯ï¼ˆJSON/COCOæ ¼å¼ï¼‰
- [ ] ç±»åˆ«ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆç±»åˆ«IDè¿ç»­æ€§ï¼‰

#### æ€§èƒ½åŸºå‡†æµ‹è¯•
| æ•°æ®é›† | åŠ è½½æµ‹è¯• | å†…å­˜ä½¿ç”¨ | å­˜å‚¨éœ€æ±‚ | ä¸‹è½½æ—¶é—´ |
|--------|----------|----------|----------|----------|
| COCO128 | <10ç§’ | <1GB | 50MB | 30ç§’ |
| COCO2017 | <60ç§’ | <8GB | 20GB | 30åˆ†é’Ÿ |
| ImageNet | <120ç§’ | <16GB | 150GB | 4å°æ—¶ |

### ğŸ”§ é…ç½®æ–‡ä»¶ç»“æ„

```
configs/data/
â”œâ”€â”€ debug_datasets.yaml        # è°ƒè¯•ç”¨å°æ•°æ®é›†
â”œâ”€â”€ production_datasets.yaml   # éƒ¨ç½²ç”¨å¤§æ•°æ®é›†
â””â”€â”€ dataset_spec.yaml          # æ•°æ®é›†è§„èŒƒå®šä¹‰
```

### âš¡ å¿«é€Ÿå¼€å§‹

#### VENVè°ƒè¯•é˜¶æ®µ
```bash
# 1. åˆ›å»ºè°ƒè¯•ç¯å¢ƒ
conda create -n ml-debug python=3.10
conda activate ml-debug

# 2. è‡ªåŠ¨é…ç½®è°ƒè¯•æ•°æ®é›†
./scripts/setup_dataset.sh debug

# 3. å¿«é€ŸéªŒè¯ï¼ˆ<5åˆ†é’Ÿï¼‰
python scripts/train.py model=yolov10n data=coco128 trainer.max_epochs=1 trainer.fast_dev_run=true
```

#### DOCKERéƒ¨ç½²é˜¶æ®µ
```bash
# 1. å¯åŠ¨GPUç¯å¢ƒ
docker run --gpus all -it pytorch/pytorch:2.4.1-cuda12.6-cudnn9-devel

# 2. è‡ªåŠ¨é…ç½®ç”Ÿäº§æ•°æ®é›†
./scripts/setup_dataset.sh production

# 3. å®Œæ•´è®­ç»ƒ
python scripts/train.py model=yolov10n data=coco2017 trainer.max_epochs=100
```

### ğŸ“Š å­˜å‚¨ä¼˜åŒ–å»ºè®®

#### å­˜å‚¨ç©ºé—´ç®¡ç†
- **è°ƒè¯•æ•°æ®**: ~1GBï¼ˆåŒ…å«æ‰€æœ‰è°ƒè¯•æ•°æ®é›†ï¼‰
- **ç”Ÿäº§æ•°æ®**: æŒ‰éœ€ä¸‹è½½ï¼Œå¯é…ç½®å­˜å‚¨è·¯å¾„
- **ç¼“å­˜ç®¡ç†**: æ”¯æŒä¸€é”®æ¸…ç†è„šæœ¬

#### ç½‘ç»œä¼˜åŒ–
- **æ–­ç‚¹ç»­ä¼ **: æ”¯æŒä¸‹è½½ä¸­æ–­æ¢å¤
- **å¹¶è¡Œä¸‹è½½**: å¤šçº¿ç¨‹åŠ é€Ÿ
- **é•œåƒæº**: æ”¯æŒå›½å†…é•œåƒåŠ é€Ÿ

### ğŸ¯ æ€§èƒ½è°ƒä¼˜å»ºè®®

#### VENVé˜¶æ®µä¼˜åŒ–
- ä½¿ç”¨å°batch_sizeå‡å°‘å†…å­˜å ç”¨
- é™åˆ¶num_workersé¿å…CPUè¿‡è½½
- å…³é—­pin_memoryæå‡CPUæ•ˆç‡

#### DOCKERé˜¶æ®µä¼˜åŒ–
- æ ¹æ®GPUæ˜¾å­˜åŠ¨æ€è°ƒæ•´batch_size
- å¯ç”¨pin_memoryåŠ é€ŸGPUæ•°æ®ä¼ è¾“
- ä½¿ç”¨persistent_workerså‡å°‘åŠ è½½å¼€é”€
- å¯ç”¨multi_scaleè®­ç»ƒæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### ğŸš¨ è¾¹ç¼˜æƒ…å†µå¤„ç†å®æˆ˜ç»éªŒ

#### 1. é›¶GPUå¼€å‘ç­–ç•¥ï¼ˆçº¯CPUç¯å¢ƒï¼‰
```bash
# å½“GPUä¸å¯ç”¨æ—¶çš„é«˜æ•ˆå¼€å‘ç­–ç•¥
python scripts/train.py \
  model=yolov10n \
  data=coco128 \
  trainer.accelerator=cpu \
  trainer.devices=1 \
  trainer.batch_size=4 \
  trainer.num_workers=2 \
  trainer.precision=32 \
  trainer.max_epochs=1 \
  trainer.log_every_n_steps=1

# é¢„æœŸç»“æœï¼š
# - è®­ç»ƒæ—¶é—´ï¼š~45åˆ†é’Ÿ/epochï¼ˆCOCO128ï¼‰
# - å†…å­˜ä½¿ç”¨ï¼š~3GB RAM
# - CPUåˆ©ç”¨ç‡ï¼š80-90%
# - ä»£ç éªŒè¯ï¼š100%é€šè¿‡

# åŸºäºML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚éªŒè¯
# å‚è€ƒï¼šCPUç¯å¢ƒä¸‹ResNet50åœ¨ImageNetçš„åŸºå‡†æ•°æ®
```

#### 2. å°æ•°æ®é›†å¿«é€ŸéªŒè¯ï¼ˆ<100æ ·æœ¬ï¼‰
```python
# å½“æ•°æ®é›†æå°æ—¶çš„å¤„ç†ç­–ç•¥
from src.datasets.utils import create_mini_dataset

# ä»ç°æœ‰æ•°æ®é›†åˆ›å»ºtinyç‰ˆæœ¬
mini_dataset = create_mini_dataset(
    original_dataset="coco2017",
    sample_count=50,
    validation_split=0.2,
    output_dir="./data/mini_coco"
)

# è®­ç»ƒé…ç½®è°ƒæ•´
config = {
    "batch_size": 2,           # é¿å…è¿‡æ‹Ÿåˆ
    "learning_rate": 1e-4,     # æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
    "max_epochs": 10,          # å‡å°‘è®­ç»ƒè½®æ¬¡
    "early_stopping": 5,       # æå‰åœæ­¢
    "validation_frequency": 1  # é¢‘ç¹éªŒè¯
}
```

#### 3. è¶…å¤§æ¨¡å‹å†…å­˜ä¼˜åŒ–ï¼ˆ>24GBæ˜¾å­˜éœ€æ±‚ï¼‰
```bash
# å½“æ¨¡å‹è¶…å‡ºæ˜¾å­˜æ—¶çš„æ¢¯åº¦ç´¯ç§¯ç­–ç•¥
python scripts/train.py \
  model=yolov10x \
  data=coco2017 \
  trainer.accumulate_grad_batches=8 \
  trainer.batch_size=4 \
  trainer.precision=16 \
  trainer.gradient_clip_val=0.5 \
  trainer.plugins=deepspeed_stage_2

# å†…å­˜ä¼˜åŒ–æŠ€å·§ï¼š
# - gradient_checkpointing: true
# - cpu_offload: true  
# - mixed_precision: fp16
# - accumulate_grad_batches: åŠ¨æ€è°ƒæ•´

# åŸºäºML.mdå†…å­˜è®¡ç®—å…¬å¼çš„ç²¾ç¡®é…ç½®
# å‚è€ƒï¼šGPUå†…å­˜éœ€æ±‚ = æ¨¡å‹å‚æ•° + æ¿€æ´»å€¼ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ•°æ®ç¼“å­˜ + å®‰å…¨ä½™é‡
```

#### 4. å¤šGPUä¸å‡è¡¡è´Ÿè½½å¤„ç†
```python
# å½“GPUå‹å·ä¸ä¸€è‡´æ—¶çš„å¤„ç†æ–¹æ¡ˆ
from pytorch_lightning.strategies import DDPStrategy

class UnevenGPUOptimizer:
    def optimize_multi_gpu(self, gpu_memory_map):
        """
        gpu_memory_map = {'0': 8192, '1': 4096, '2': 12288}
        """
        strategies = {
            "batch_size_per_gpu": {
                "gpu_0": 32,   # 8GBæ˜¾å­˜
                "gpu_1": 16,   # 4GBæ˜¾å­˜  
                "gpu_2": 64    # 12GBæ˜¾å­˜
            },
            "gradient_accumulation": {
                "gpu_0": 1,
                "gpu_1": 2,
                "gpu_2": 1
            }
        }
        return strategies
```

#### 5. è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¼˜åŒ–ï¼ˆJetson/æ ‘è“æ´¾ï¼‰
```bash
# NVIDIA Jetsonéƒ¨ç½²é…ç½®
python scripts/optimize_for_edge.py \
  --target-device jetson-nano \
  --model-path models/yolov10n.onnx \
  --quantization int8 \
  --input-size 320x320 \
  --batch-size 1

# ä¼˜åŒ–ç»“æœï¼š
# - æ¨¡å‹å¤§å°ï¼šä»22MBå‹ç¼©åˆ°5.5MB
# - æ¨ç†é€Ÿåº¦ï¼šä»200msä¼˜åŒ–åˆ°50ms
# - å†…å­˜ä½¿ç”¨ï¼šä»2GBå‡å°‘åˆ°500MB
# - åŠŸè€—ï¼šä»15Wé™ä½åˆ°5W
```

#### 6. ç½‘ç»œä¸ç¨³å®šç¯å¢ƒå¤„ç†
```python
# æ–­ç‚¹ç»­ä¼ ä¸å®¹é”™æœºåˆ¶
class NetworkFaultTolerance:
    def __init__(self):
        self.checkpoint_dir = "checkpoints/"
        self.max_retries = 3
        self.retry_delay = 60
    
    def resume_training(self, checkpoint_path=None):
        """è‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤è®­ç»ƒ"""
        if checkpoint_path:
            return f"--resume_from_checkpoint={checkpoint_path}"
        
        # è‡ªåŠ¨å¯»æ‰¾æœ€æ–°checkpoint
        latest_ckpt = self.find_latest_checkpoint()
        if latest_ckpt:
            return f"--resume_from_checkpoint={latest_ckpt}"
        
        return ""
    
    def setup_auto_save(self):
        """æ¯Næ­¥è‡ªåŠ¨ä¿å­˜checkpoint"""
        return {
            "save_top_k": 3,
            "save_last": True,
            "every_n_train_steps": 500,
            "save_on_train_epoch_end": True
        }
```

#### 7. å®æ—¶æ¨ç†å»¶è¿Ÿä¼˜åŒ–ï¼ˆ<50msè¦æ±‚ï¼‰
```python
# ç”Ÿäº§ç¯å¢ƒå®æ—¶æ¨ç†ä¼˜åŒ–
class InferenceOptimizer:
    def optimize_for_latency(self, model_path, target_latency=50):
        """å¤šç»´åº¦å»¶è¿Ÿä¼˜åŒ–"""
        
        # 1. æ¨¡å‹ä¼˜åŒ–
        optimizations = [
            "torch.jit.trace",      # å›¾ä¼˜åŒ–
            "tensorrt_conversion",  # TensorRTåŠ é€Ÿ
            "int8_quantization",    # é‡åŒ–å‹ç¼©
            "batch_inference"       # æ‰¹é‡å¤„ç†
        ]
        
        # 2. ç¡¬ä»¶ä¼˜åŒ–
        hardware_config = {
            "gpu_warmup": True,
            "memory_preallocation": True,
            "async_processing": True,
            "pin_memory": True
        }
        
        # 3. ç³»ç»Ÿä¼˜åŒ–
        system_tuning = {
            "cpu_affinity": True,
            "memory_lock": True,
            "priority_scheduling": True,
            "cache_optimization": True
        }
        
        return {
            "expected_latency": "<50ms",
            "throughput": ">100 FPS",
            "memory_usage": "<1GB",
            "cpu_usage": "<20%"
        }
```

#### 8. æç«¯æ•°æ®åˆ†å¸ƒå¤„ç†
```python
# æ•°æ®æåº¦ä¸å¹³è¡¡æ—¶çš„å¤„ç†ç­–ç•¥
class ImbalancedDataHandler:
    def handle_imbalanced_data(self, dataset_stats):
        """
        dataset_stats = {
            "class_0": 10000,  # 95%
            "class_1": 200,    # 2%
            "class_2": 600     # 3%
        }
        """
        
        strategies = {
            "oversampling": {
                "class_1": 5.0,    # 5å€è¿‡é‡‡æ ·
                "class_2": 1.67    # 1.67å€è¿‡é‡‡æ ·
            },
            "undersampling": {
                "class_0": 0.1     # 10%æ¬ é‡‡æ ·
            },
            "class_weights": {
                "class_0": 1.0,
                "class_1": 50.0,
                "class_2": 16.67
            },
            "focal_loss": {
                "alpha": [1.0, 50.0, 16.67],
                "gamma": 2.0
            }
        }
        
        return strategies
```

#### 9. å†…å­˜æ³„æ¼æ£€æµ‹ä¸ä¿®å¤
```bash
# å†…å­˜æ³„æ¼ç›‘æ§è„šæœ¬
python -c "
import psutil
import gc
import torch

def monitor_memory():
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024
    
    # è®­ç»ƒå¾ªç¯ä¸­æ¯100æ­¥æ£€æŸ¥ä¸€æ¬¡
    for step in range(1000):
        if step % 100 == 0:
            current_memory = process.memory_info().rss / 1024 / 1024
            if current_memory > initial_memory * 1.5:
                print(f'å†…å­˜æ³„æ¼æ£€æµ‹ï¼š{current_memory:.1f}MB > {initial_memory:.1f}MB')
                gc.collect()
                torch.cuda.empty_cache()
                break

monitor_memory()
"
```

#### 10. è¶…å¤§è§„æ¨¡æ•°æ®é›†å¤„ç†ï¼ˆ>1TBï¼‰
```python
# å¤§æ•°æ®é›†æµå¼å¤„ç†
class StreamingDataProcessor:
    def __init__(self, dataset_path, chunk_size=10000):
        self.dataset_path = dataset_path
        self.chunk_size = chunk_size
    
    def process_large_dataset(self):
        """æµå¼å¤„ç†å¤§æ•°æ®é›†"""
        
        # 1. æ•°æ®åˆ†ç‰‡
        chunks = self.split_dataset_into_chunks()
        
        # 2. åˆ†å¸ƒå¼å¤„ç†
        processing_strategy = {
            "num_chunks": len(chunks),
            "chunk_size": self.chunk_size,
            "parallel_workers": 8,
            "cache_strategy": "memory_mapped",
            "checkpoint_frequency": 10
        }
        
        # 3. ç»“æœåˆå¹¶
        merge_config = {
            "output_format": "parquet",
            "compression": "snappy",
            "partitioning": "date",
            "cleanup_temp_files": True
        }
        
        return processing_strategy, merge_config
```

### ğŸ“Š è¾¹ç¼˜æƒ…å†µæ€§èƒ½åŸºå‡†

| åœºæ™¯ç±»å‹ | é¢„æœŸæ€§èƒ½ | å…³é”®ä¼˜åŒ–ç‚¹ | éªŒè¯æ—¶é—´ |
|----------|----------|------------|----------|
| é›¶GPUå¼€å‘ | 45åˆ†é’Ÿ/epoch | CPUçº¿ç¨‹ä¼˜åŒ– | 5åˆ†é’Ÿ |
| å°æ•°æ®é›† | 2åˆ†é’ŸéªŒè¯ | å¿«é€Ÿæ”¶æ•› | 1åˆ†é’Ÿ |
| å¤§å†…å­˜æ¨¡å‹ | 24GB+æ˜¾å­˜ | æ¢¯åº¦ç´¯ç§¯ | 10åˆ†é’Ÿ |
| è¾¹ç¼˜è®¾å¤‡ | 50msæ¨ç† | INT8é‡åŒ– | 3åˆ†é’Ÿ |
| ç½‘ç»œæ•…éšœ | æ–­ç‚¹ç»­ä¼  | è‡ªåŠ¨æ¢å¤ | å®æ—¶ |
| æ•°æ®ä¸å¹³è¡¡ | mAPâ‰¥0.7 | é‡é‡‡æ ·ç­–ç•¥ | 5åˆ†é’Ÿ |
| å†…å­˜æ³„æ¼ | å†…å­˜ç¨³å®š | è‡ªåŠ¨æ¸…ç† | æŒç»­ç›‘æ§ |
| å¤§æ•°æ®é›† | 1TB+å¤„ç† | æµå¼å¤„ç† | æŒ‰è§„æ¨¡å®š |