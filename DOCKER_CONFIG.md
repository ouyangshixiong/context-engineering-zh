# ðŸ³ GPUç”Ÿäº§çŽ¯å¢ƒé…ç½®æŒ‡å—

> ä¸“ä¸ºé«˜æ€§èƒ½è®­ç»ƒè®¾è®¡çš„Docker GPUçŽ¯å¢ƒï¼Œæ”¯æŒPyTorchå’ŒPaddlePaddle

## ðŸŽ¯ çŽ¯å¢ƒæ¦‚è¿°

| ç»„ä»¶ | ç‰ˆæœ¬ | ç”¨é€” | å¤‡æ³¨ |
|------|------|------|------|
| CUDA | 12.6 | GPUè®¡ç®— | æœ€æ–°ç¨³å®šç‰ˆ |
| PyTorch | 2.6.0+cu126 | æ·±åº¦å­¦ä¹ æ¡†æž¶ | GPUåŠ é€Ÿ |
| PaddlePaddle | 2.6.0+gpu | æ·±åº¦å­¦ä¹ æ¡†æž¶ | GPUåŠ é€Ÿ |
| å†…å­˜éœ€æ±‚ | â‰¥ 8GB | è¿è¡Œè¦æ±‚ | æ”¯æŒbatch_size=64 |
| GPUéœ€æ±‚ | â‰¥ 8GBæ˜¾å­˜ | è®­ç»ƒè¦æ±‚ | RTX 3060ä»¥ä¸Š |

## ðŸš€ ä¸€é”®éƒ¨ç½²

### æ–¹æ¡ˆ1: Docker Composeï¼ˆæŽ¨èï¼‰

```bash
# å¯åŠ¨GPUçŽ¯å¢ƒ
docker-compose -f deploy/gpu/docker-compose.yml up -d

# éªŒè¯çŽ¯å¢ƒ
docker exec yolov10 python -c "import torch; print(f'GPUæ•°é‡: {torch.cuda.device_count()}')"
```

### æ–¹æ¡ˆ2: Dockerå‘½ä»¤è¡Œ

```bash
# æž„å»ºé•œåƒ
docker build -t yolov10-gpu -f deploy/gpu/Dockerfile .

# è¿è¡Œå®¹å™¨
docker run --gpus all -it --name yolov10 \
  -v $(pwd):/workspace \
  -p 8888:8888 -p 6006:6006 \
  yolov10-gpu bash
```

## ðŸ“‹ è¯¦ç»†é…ç½®æ­¥éª¤

### 1. ç³»ç»Ÿè¦æ±‚æ£€æŸ¥

```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi
# æœŸæœ›è¾“å‡º: CUDA Version: 12.6

# æ£€æŸ¥Docker
docker --version
# æœŸæœ›: Docker version 20.10+

# æ£€æŸ¥NVIDIA Docker
docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu20.04 nvidia-smi
```

### 2. Dockerfileé…ç½®

#### åŸºç¡€Dockerfile
```dockerfile
FROM pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    git wget unzip htop vim \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /workspace

# å®‰è£…Pythonä¾èµ–
COPY requirements-gpu.txt .
RUN pip install --no-cache-dir -r requirements-gpu.txt

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# è®¾ç½®çŽ¯å¢ƒå˜é‡
ENV PYTHONPATH=/workspace
ENV CUDA_VISIBLE_DEVICES=0,1,2,3

# æš´éœ²ç«¯å£
EXPOSE 8888 6006

CMD ["python", "scripts/train.py"]
```

#### PaddlePaddle Dockerfile
```dockerfile
FROM paddlepaddle/paddle:2.6.0-gpu-cuda12.6-cudnn9

# å®‰è£…é¢å¤–ä¾èµ–
RUN pip install --no-cache-dir \
    pytorch-lightning==2.0.0 \
    omegaconf==2.3.0 \
    torchmetrics==0.11.0 \
    wandb==0.15.0 \
    tensorboard==2.13.0

WORKDIR /workspace
COPY . .
ENV PYTHONPATH=/workspace

CMD ["python", "scripts/train.py"]
```

### 3. Docker Composeé…ç½®

#### docker-compose.yml
```yaml
version: '3.8'
services:
  yolov10:
    build:
      context: .
      dockerfile: deploy/gpu/Dockerfile
    container_name: yolov10
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0,1,2,3
    volumes:
      - ./:/workspace
      - ./data:/workspace/data
      - ./logs:/workspace/logs
    ports:
      - "8888:8888"  # Jupyter
      - "6006:6006"  # TensorBoard
    command: >
      bash -c "
        echo '=== GPUä¿¡æ¯ ===' &&
        nvidia-smi &&
        echo '=== çŽ¯å¢ƒéªŒè¯ ===' &&
        python -c 'import torch; print(f\"PyTorch: {torch.__version__}\"); print(f\"GPU: {torch.cuda.device_count()}\")' &&
        echo '=== å¯åŠ¨è®­ç»ƒ ===' &&
        python scripts/train.py model=yolov10n data=coco128 trainer.max_epochs=10
      "
```

### 4. çŽ¯å¢ƒéªŒè¯è„šæœ¬

#### åˆ›å»ºéªŒè¯è„šæœ¬
```bash
cat > validate_gpu.py << 'EOF'
import torch
import paddle
import subprocess
import os

print("=== GPUçŽ¯å¢ƒéªŒè¯ ===")

# æ£€æŸ¥CUDAå¯ç”¨æ€§
print(f"PyTorch CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"PaddlePaddle GPUå¯ç”¨: {paddle.is_compiled_with_cuda()}")

# æ£€æŸ¥GPUä¿¡æ¯
if torch.cuda.is_available():
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"GPU {i}: {props.name}, {props.total_memory/1024**3:.1f}GB")

# æ£€æŸ¥nvidia-smi
try:
    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
    print("âœ… nvidia-smi å¯ç”¨")
except FileNotFoundError:
    print("âŒ nvidia-smi ä¸å¯ç”¨")

print("=== çŽ¯å¢ƒéªŒè¯å®Œæˆ ===")
EOF
```

## ðŸ§ª GPUæ€§èƒ½æµ‹è¯•

### 1. åŸºç¡€æ€§èƒ½æµ‹è¯•

```bash
# è¿è¡ŒGPUæ€§èƒ½æµ‹è¯•
docker exec yolov10 python -c "
import torch
import time

# æµ‹è¯•GPUè®¡ç®—æ€§èƒ½
size = 8192
a = torch.randn(size, size, device='cuda')
b = torch.randn(size, size, device='cuda')

start = time.time()
c = torch.matmul(a, b)
torch.cuda.synchronize()
end = time.time()

print(f'GPUçŸ©é˜µä¹˜æ³•: {size}x{size} ç”¨æ—¶ {end-start:.2f}s')
print(f'GPUå†…å­˜: {torch.cuda.memory_allocated()/1024**3:.1f}GB')
"
```

### 2. è®­ç»ƒæ€§èƒ½åŸºå‡†

```bash
# å•GPUè®­ç»ƒæµ‹è¯•
docker exec yolov10 python scripts/train.py \
  model=yolov10n \
  data=coco128 \
  trainer.max_epochs=3 \
  trainer.accelerator=gpu \
  trainer.devices=1 \
  trainer.precision=16

# å¤šGPUè®­ç»ƒæµ‹è¯•  
docker exec yolov10 python scripts/train.py \
  model=yolov10n \
  data=coco128 \
  trainer.max_epochs=3 \
  trainer.accelerator=gpu \
  trainer.devices=4 \
  trainer.strategy=ddp \
  trainer.precision=16
```

### 3. æ€§èƒ½ç›‘æŽ§

```bash
# å®žæ—¶ç›‘æŽ§GPUä½¿ç”¨çŽ‡
watch -n 1 nvidia-smi

# ç›‘æŽ§å®¹å™¨èµ„æº
docker stats yolov10

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
docker exec yolov10 tail -f logs/lightning_logs/version_0/metrics.csv
```

## ðŸ”§ é«˜çº§é…ç½®

### 1. å¤šGPUé…ç½®

#### çŽ¯å¢ƒå˜é‡è®¾ç½®
```bash
# Dockerè¿è¡Œæ—¶è®¾ç½®
docker run --gpus '"device=0,1,2,3"' -it yolov10-gpu bash

# å®¹å™¨å†…è®¾ç½®
export CUDA_VISIBLE_DEVICES=0,1,2,3
export NCCL_DEBUG=INFO
```

#### è®­ç»ƒå‚æ•°ä¼˜åŒ–
```bash
# å¤šGPUè®­ç»ƒé…ç½®
python scripts/train.py \
  model=yolov10n \
  data=coco2017 \
  trainer.max_epochs=300 \
  trainer.accelerator=gpu \
  trainer.devices=4 \
  trainer.strategy=ddp \
  trainer.precision=16 \
  trainer.gradient_clip_val=0.1 \
  trainer.accumulate_grad_batches=4
```

### 2. å†…å­˜ä¼˜åŒ–

#### CUDAå†…å­˜ç®¡ç†
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ 
import torch

# å†…å­˜ä¼˜åŒ–è®¾ç½®
torch.cuda.empty_cache()
torch.backends.cudnn.benchmark = True

# ç›‘æŽ§å†…å­˜ä½¿ç”¨
if torch.cuda.is_available():
    print(f'GPUå†…å­˜: {torch.cuda.memory_allocated()/1024**3:.1f}GB')
    print(f'GPUç¼“å­˜: {torch.cuda.memory_reserved()/1024**3:.1f}GB')
```

### 3. æ€§èƒ½è°ƒä¼˜

#### è®­ç»ƒåŠ é€Ÿé…ç½®
```yaml
# configs/trainer/gpu_speed.yaml
trainer:
  accelerator: gpu
  devices: auto
  precision: 16
  strategy: ddp
  gradient_clip_val: 0.1
  accumulate_grad_batches: 4
  sync_batchnorm: true
  benchmark: true
```

## ðŸ“Š æ€§èƒ½åŸºå‡†

### ç¡¬ä»¶æ€§èƒ½å¯¹æ¯”
| GPUåž‹å· | æ˜¾å­˜ | COCOè®­ç»ƒæ—¶é—´ | å†…å­˜ä½¿ç”¨ | æŽ¨èbatch_size |
|---------|------|--------------|----------|----------------|
| RTX 3060 | 12GB | ~45åˆ†é’Ÿ/epoch | 8GB | 32 |
| RTX 3080 | 10GB | ~35åˆ†é’Ÿ/epoch | 8GB | 32 |
| RTX 4090 | 24GB | ~25åˆ†é’Ÿ/epoch | 12GB | 64 |
| A100 | 40GB | ~15åˆ†é’Ÿ/epoch | 20GB | 128 |

### å¤šGPUæ€§èƒ½
| GPUæ•°é‡ | é€Ÿåº¦æå‡ | å†…å­˜ä½¿ç”¨ | åŒæ­¥å¼€é”€ |
|---------|----------|----------|----------|
| 1x | 1x | 8GB | 0% |
| 2x | 1.8x | 16GB | 10% |
| 4x | 3.2x | 32GB | 20% |
| 8x | 6.0x | 64GB | 25% |

## ðŸš¨ å¸¸è§é—®é¢˜è§£å†³

### é—®é¢˜1: CUDAç‰ˆæœ¬ä¸åŒ¹é…
```bash
# é”™è¯¯ä¿¡æ¯: CUDA version mismatch
# è§£å†³æ–¹æ¡ˆï¼š
docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu20.04 nvidia-smi
# ç¡®è®¤CUDAç‰ˆæœ¬ä¸€è‡´
```

### é—®é¢˜2: GPUå†…å­˜ä¸è¶³
```bash
# é”™è¯¯ä¿¡æ¯: CUDA out of memory
# è§£å†³æ–¹æ¡ˆï¼š
# 1. å‡å°batch_size
# 2. å¯ç”¨æ¢¯åº¦ç´¯ç§¯
# 3. ä½¿ç”¨æ··åˆç²¾åº¦
python scripts/train.py \
  model=yolov10n \
  data=coco2017 \
  trainer.precision=16 \
  data.batch_size=16
```

### é—®é¢˜3: NCCLé€šä¿¡é”™è¯¯
```bash
# é”™è¯¯ä¿¡æ¯: NCCL error
# è§£å†³æ–¹æ¡ˆï¼š
export NCCL_DEBUG=INFO
export NCCL_P2P_DISABLE=1
export NCCL_IB_DISABLE=1
```

### é—®é¢˜4: Dockeræƒé™é—®é¢˜
```bash
# é”™è¯¯ä¿¡æ¯: permission denied
# è§£å†³æ–¹æ¡ˆï¼š
sudo usermod -aG docker $USER
sudo systemctl restart docker
docker run hello-world
```

## ðŸ”„ çŽ¯å¢ƒç®¡ç†

### å®¹å™¨ç®¡ç†å‘½ä»¤
```bash
# å¯åŠ¨å®¹å™¨
docker-compose -f deploy/gpu/docker-compose.yml up -d

# è¿›å…¥å®¹å™¨
docker exec -it yolov10 bash

# åœæ­¢å®¹å™¨
docker-compose -f deploy/gpu/docker-compose.yml down

# æŸ¥çœ‹æ—¥å¿—
docker logs -f yolov10

# æ¸…ç†å®¹å™¨
docker system prune -f
```

### é•œåƒç®¡ç†
```bash
# æž„å»ºé•œåƒ
docker build -t yolov10-gpu -f deploy/gpu/Dockerfile .

# æ ‡è®°é•œåƒ
docker tag yolov10-gpu your-registry/yolov10:latest

# æŽ¨é€é•œåƒ
docker push your-registry/yolov10:latest

# æ‹‰å–é•œåƒ
docker pull your-registry/yolov10:latest
```

## ðŸ“‹ é…ç½®æ£€æŸ¥æ¸…å•

### ç³»ç»Ÿè¦æ±‚
- [ ] NVIDIAé©±åŠ¨ â‰¥ 535.00
- [ ] Docker â‰¥ 20.10
- [ ] nvidia-docker2 å·²å®‰è£…
- [ ] CUDA 12.6 å…¼å®¹

### é…ç½®éªŒè¯
- [ ] GPUæ£€æµ‹æˆåŠŸ
- [ ] é•œåƒæž„å»ºæˆåŠŸ
- [ ] å®¹å™¨å¯åŠ¨æ­£å¸¸
- [ ] å¤šGPUè¯†åˆ«æ­£ç¡®
- [ ] å†…å­˜åˆ†é…åˆç†

### æ€§èƒ½éªŒè¯
- [ ] å•GPUè®­ç»ƒæ­£å¸¸
- [ ] å¤šGPUè®­ç»ƒæ­£å¸¸
- [ ] æ··åˆç²¾åº¦å·¥ä½œ
- [ ] æ€§èƒ½ç¬¦åˆé¢„æœŸ

## ðŸŽ¯ ä¸‹ä¸€æ­¥

å®ŒæˆGPUçŽ¯å¢ƒé…ç½®åŽï¼š
1. æŸ¥çœ‹ [DEPLOY.md](./DEPLOY.md) è¿›è¡Œç”Ÿäº§éƒ¨ç½²
2. æ›´æ–° [PROJECT_BUILD_LOG.md](./PROJECT_BUILD_LOG.md)
3. é…ç½®CI/CDæµæ°´çº¿

**å®Œæ•´æµç¨‹**ï¼š
- **è§„åˆ’é˜¶æ®µ**ï¼šCREATE.md â†’ INITIAL.mdï¼ˆéœ€æ±‚è§„æ ¼ï¼‰
- **éªŒè¯é˜¶æ®µ**ï¼šVENV_CONFIG.md â†’ DEBUG_CODE.md â†’ DOCKER_CONFIG.md
- **éƒ¨ç½²é˜¶æ®µ**ï¼šDEPLOY.md â†’ PROJECT_BUILD_LOG.md

---
**é…ç½®æ—¶é—´**: ~15åˆ†é’Ÿ | **éªŒè¯æ—¶é—´**: ~10åˆ†é’Ÿ | **æ€»è®¡**: ~25åˆ†é’Ÿ