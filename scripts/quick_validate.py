#!/usr/bin/env python3
"""
å¿«é€Ÿæ•°æ®é›†éªŒè¯è„šæœ¬
ä¸“ä¸ºVENVè°ƒè¯•é˜¶æ®µè®¾è®¡ï¼Œç¡®ä¿åœ¨5åˆ†é’Ÿå†…å®Œæˆæ•°æ®é›†éªŒè¯
"""

import os
import sys
import time
import yaml
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Any
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QuickValidator:
    """å¿«é€Ÿæ•°æ®é›†éªŒè¯å™¨"""
    
    def __init__(self, config_path: str, dataset_name: str):
        self.config_path = Path(config_path)
        self.dataset_name = dataset_name
        self.config = self.load_config()
        self.results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "dataset": dataset_name,
            "config_path": str(self.config_path),
            "checks": {},
            "passed": True,
            "duration": 0
        }
    
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é›†é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # æ ¹æ®æ•°æ®é›†åç§°è·å–é…ç½®
            if 'debug_datasets' in config and self.dataset_name in config['debug_datasets']:
                return config['debug_datasets'][self.dataset_name]
            elif 'production_datasets' in config and self.dataset_name in config['production_datasets']:
                return config['production_datasets'][self.dataset_name]
            else:
                raise ValueError(f"æ•°æ®é›† {self.dataset_name} æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)
    
    def check_directory_structure(self) -> bool:
        """æ£€æŸ¥ç›®å½•ç»“æ„"""
        logger.info("æ£€æŸ¥ç›®å½•ç»“æ„...")
        
        data_dir = Path(self.config['data_dir'])
        required_dirs = ['train', 'val']
        
        missing_dirs = []
        for dir_name in required_dirs:
            dir_path = data_dir / dir_name
            if not dir_path.exists():
                missing_dirs.append(str(dir_path))
        
        # COCOæ ¼å¼é¢å¤–æ£€æŸ¥
        if self.config['dataset_type'] == 'COCODetection':
            annotations_dir = data_dir / 'annotations'
            if not annotations_dir.exists():
                missing_dirs.append(str(annotations_dir))
        
        if missing_dirs:
            logger.warning(f"ç¼ºå¤±ç›®å½•: {missing_dirs}")
            self.results["checks"]["directory_structure"] = {
                "status": "FAILED",
                "message": f"ç¼ºå¤±ç›®å½•: {missing_dirs}",
                "missing_dirs": missing_dirs
            }
            return False
        else:
            logger.success("âœ… ç›®å½•ç»“æ„å®Œæ•´")
            self.results["checks"]["directory_structure"] = {
                "status": "PASSED",
                "message": "æ‰€æœ‰å¿…éœ€ç›®å½•å­˜åœ¨"
            }
            return True
    
    def check_file_count(self) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ•°é‡"""
        logger.info("æ£€æŸ¥æ–‡ä»¶æ•°é‡...")
        
        data_dir = Path(self.config['data_dir'])
        
        # æ£€æŸ¥è®­ç»ƒé›†
        train_dir = data_dir / 'train'
        if train_dir.exists():
            train_images = list(train_dir.glob('*.jpg')) + list(train_dir.glob('*.png'))
            actual_count = len(train_images)
            expected_count = self.config.get('max_train_samples', self.config.get('num_samples', 100))
            
            if actual_count >= min(expected_count * 0.8, expected_count - 5):
                logger.success(f"âœ… è®­ç»ƒé›†æ–‡ä»¶æ•°é‡æ­£å¸¸: {actual_count}")
                self.results["checks"]["train_file_count"] = {
                    "status": "PASSED",
                    "actual": actual_count,
                    "expected": expected_count
                }
                return True
            else:
                logger.warning(f"è®­ç»ƒé›†æ–‡ä»¶æ•°é‡ä¸è¶³: {actual_count} < {expected_count}")
                self.results["checks"]["train_file_count"] = {
                    "status": "FAILED",
                    "actual": actual_count,
                    "expected": expected_count,
                    "message": "è®­ç»ƒé›†æ–‡ä»¶æ•°é‡ä¸è¶³"
                }
                return False
        else:
            logger.error("è®­ç»ƒé›†ç›®å½•ä¸å­˜åœ¨")
            return False
    
    def check_image_files(self) -> bool:
        """æ£€æŸ¥å›¾åƒæ–‡ä»¶å¯è¯»æ€§"""
        logger.info("æ£€æŸ¥å›¾åƒæ–‡ä»¶å¯è¯»æ€§...")
        
        data_dir = Path(self.config['data_dir'])
        train_dir = data_dir / 'train'
        
        if not train_dir.exists():
            return False
        
        # éšæœºæ£€æŸ¥å‰5ä¸ªå›¾åƒæ–‡ä»¶
        image_files = list(train_dir.glob('*.jpg'))[:5] + list(train_dir.glob('*.png'))[:5]
        failed_files = []
        
        try:
            from PIL import Image
            for img_file in image_files:
                try:
                    with Image.open(img_file) as img:
                        img.verify()
                except Exception as e:
                    failed_files.append(str(img_file))
        except ImportError:
            logger.warning("PILæœªå®‰è£…ï¼Œè·³è¿‡å›¾åƒéªŒè¯")
            self.results["checks"]["image_files"] = {
                "status": "SKIPPED",
                "message": "PILæœªå®‰è£…ï¼Œè·³è¿‡å›¾åƒéªŒè¯"
            }
            return True
        
        if failed_files:
            logger.warning(f"æŸåçš„å›¾åƒæ–‡ä»¶: {failed_files}")
            self.results["checks"]["image_files"] = {
                "status": "FAILED",
                "message": f"{len(failed_files)}ä¸ªå›¾åƒæ–‡ä»¶æŸå",
                "failed_files": failed_files
            }
            return False
        else:
            logger.success("âœ… æ‰€æœ‰å›¾åƒæ–‡ä»¶å¯è¯»")
            self.results["checks"]["image_files"] = {
                "status": "PASSED",
                "message": "æ‰€æœ‰å›¾åƒæ–‡ä»¶å¯è¯»"
            }
            return True
    
    def check_annotations(self) -> bool:
        """æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶"""
        logger.info("æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶...")
        
        data_dir = Path(self.config['data_dir'])
        
        if self.config['dataset_type'] == 'COCODetection':
            annot_file = data_dir / 'annotations' / 'instances_train2017.json'
            if annot_file.exists():
                try:
                    with open(annot_file, 'r') as f:
                        annot_data = json.load(f)
                    
                    # æ£€æŸ¥åŸºæœ¬ç»“æ„
                    required_keys = ['images', 'annotations', 'categories']
                    missing_keys = [key for key in required_keys if key not in annot_data]
                    
                    if missing_keys:
                        logger.warning(f"æ ‡æ³¨æ–‡ä»¶ç¼ºå¤±é”®: {missing_keys}")
                        self.results["checks"]["annotations"] = {
                            "status": "FAILED",
                            "message": f"æ ‡æ³¨æ–‡ä»¶ç¼ºå¤±é”®: {missing_keys}"
                        }
                        return False
                    else:
                        logger.success("âœ… æ ‡æ³¨æ–‡ä»¶æ ¼å¼æ­£ç¡®")
                        self.results["checks"]["annotations"] = {
                            "status": "PASSED",
                            "message": "æ ‡æ³¨æ–‡ä»¶æ ¼å¼æ­£ç¡®",
                            "num_images": len(annot_data['images']),
                            "num_annotations": len(annot_data['annotations']),
                            "num_categories": len(annot_data['categories'])
                        }
                        return True
                        
                except Exception as e:
                    logger.error(f"æ ‡æ³¨æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
                    return False
            else:
                logger.warning("æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨")
                return True  # å¯¹äºéCOCOæ ¼å¼ï¼Œè·³è¿‡æ­¤æ£€æŸ¥
        
        return True
    
    def check_memory_usage(self) -> bool:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨"""
        logger.info("æ£€æŸ¥å†…å­˜ä½¿ç”¨...")
        
        try:
            import psutil
            
            # æ£€æŸ¥æ•°æ®é›†ç›®å½•å¤§å°
            data_dir = Path(self.config['data_dir'])
            if data_dir.exists():
                total_size = sum(f.stat().st_size for f in data_dir.rglob('*') if f.is_file())
                total_size_mb = total_size / (1024 * 1024)
                
                # è·å–å¯ç”¨å†…å­˜
                memory = psutil.virtual_memory()
                available_memory = memory.available / (1024 * 1024 * 1024)
                
                logger.success(f"æ•°æ®é›†å¤§å°: {total_size_mb:.1f}MB, å¯ç”¨å†…å­˜: {available_memory:.1f}GB")
                
                self.results["checks"]["memory_usage"] = {
                    "status": "PASSED",
                    "dataset_size_mb": round(total_size_mb, 1),
                    "available_memory_gb": round(available_memory, 1)
                }
                return True
            else:
                logger.warning("æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨")
                return False
                
        except ImportError:
            logger.warning("psutilæœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜æ£€æŸ¥")
            self.results["checks"]["memory_usage"] = {
                "status": "SKIPPED",
                "message": "psutilæœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜æ£€æŸ¥"
            }
            return True
    
    def run_all_checks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ£€æŸ¥"""
        start_time = time.time()
        
        logger.info(f"å¼€å§‹éªŒè¯æ•°æ®é›†: {self.dataset_name}")
        logger.info(f"æ•°æ®ç›®å½•: {self.config['data_dir']}")
        
        # è¿è¡Œå„é¡¹æ£€æŸ¥
        checks = [
            ("directory_structure", self.check_directory_structure),
            ("file_count", self.check_file_count),
            ("image_files", self.check_image_files),
            ("annotations", self.check_annotations),
            ("memory_usage", self.check_memory_usage)
        ]
        
        passed_checks = 0
        total_checks = len(checks)
        
        for name, check_func in checks:
            try:
                if check_func():
                    passed_checks += 1
                else:
                    self.results["passed"] = False
            except Exception as e:
                logger.error(f"æ£€æŸ¥ {name} å¤±è´¥: {e}")
                self.results["checks"][name] = {
                    "status": "ERROR",
                    "message": str(e)
                }
                self.results["passed"] = False
        
        # è®¡ç®—è€—æ—¶
        end_time = time.time()
        self.results["duration"] = round(end_time - start_time, 2)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        return self.results
    
    def generate_report(self):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report_path = Path("outputs") / f"dataset_validation_{self.dataset_name}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # æ§åˆ¶å°è¾“å‡ºæ‘˜è¦
        logger.info("=" * 50)
        logger.info("ğŸ“Š éªŒè¯ç»“æœæ‘˜è¦")
        logger.info("=" * 50)
        
        if self.results["passed"]:
            logger.success("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
        else:
            logger.error("âŒ éƒ¨åˆ†æ£€æŸ¥å¤±è´¥")
        
        logger.info(f"æ£€æŸ¥è€—æ—¶: {self.results['duration']}ç§’")
        logger.info(f"è¯¦ç»†æŠ¥å‘Š: {report_path}")
        
        # æ˜¾ç¤ºå¤±è´¥é¡¹
        failed_checks = [
            name for name, check in self.results["checks"].items()
            if check["status"] in ["FAILED", "ERROR"]
        ]
        
        if failed_checks:
            logger.warning(f"å¤±è´¥é¡¹: {failed_checks}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¿«é€Ÿæ•°æ®é›†éªŒè¯å·¥å…·")
    parser.add_argument("--config", type=str, default="configs/data/debug_datasets.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset", type=str, default="coco128",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--stage", type=str, choices=["debug", "production"], 
                       default="debug", help="éªŒè¯é˜¶æ®µ")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # æ ¹æ®é˜¶æ®µé€‰æ‹©é»˜è®¤é…ç½®
    if args.stage == "debug":
        config_path = "configs/data/debug_datasets.yaml"
    else:
        config_path = "configs/data/production_datasets.yaml"
    
    if args.config != "configs/data/debug_datasets.yaml":
        config_path = args.config
    
    validator = QuickValidator(config_path, args.dataset)
    results = validator.run_all_checks()
    
    # é€€å‡ºç 
    sys.exit(0 if results["passed"] else 1)

if __name__ == "__main__":
    main()