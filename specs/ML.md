# ğŸš€ æœºå™¨å­¦ä¹ æŠ€æœ¯æ ˆè§„èŒƒï¼ˆMachine Learning Technology Stack Specificationï¼‰

> **æŠ€æœ¯æ ˆè§„èŒƒ** - åŸºäºæœ€ä½³å®è·µçš„æ¡†æ¶ç‰ˆæœ¬é€‰æ‹©ä¸ç¡¬ä»¶éœ€æ±‚è¯„ä¼°ï¼Œç¡®ä¿æŠ€æœ¯é€‰å‹ç¨³å®šå¯é ã€‚

## ğŸ¯ æŠ€æœ¯æ ˆæ€»è§ˆ

### ğŸ“‹ è§„èŒƒç›®æ ‡
- **æ ¸å¿ƒç›®æ ‡**: æä¾›æ ‡å‡†åŒ–çš„æœºå™¨å­¦ä¹ æŠ€æœ¯æ ˆé…ç½®
- **é€‚ç”¨èŒƒå›´**: æ·±åº¦å­¦ä¹ é¡¹ç›®å¼€å‘ä¸éƒ¨ç½²
- **æŠ€æœ¯è¦†ç›–**: PyTorchã€PaddlePaddleåŒæ ˆæ”¯æŒ
- **éªŒè¯æ ‡å‡†**: GPUåˆ©ç”¨ç‡>90%ï¼ŒCPUç¯å¢ƒ1-epochéªŒè¯é€šè¿‡

## ğŸ¯ æŠ€æœ¯é€‰å‹å†³ç­–çŸ©é˜µ

### ğŸ“Š æ¡†æ¶ç‰ˆæœ¬ç²¾ç¡®è§„èŒƒ
| é˜¶æ®µ | ç¯å¢ƒç›®æ ‡ | PyTorchç‰ˆæœ¬ | PaddlePaddleç‰ˆæœ¬ | CUDAç‰ˆæœ¬ | éªŒè¯æ ‡å‡† |
|------|----------|-------------|------------------|----------|----------|
| **VENVè°ƒè¯•** | GPUéªŒè¯ä»£ç æ­£ç¡®æ€§ | 2.4.1ï¼ˆè‡ªåŠ¨é€‚é…CUDA 12.xï¼‰ | 2.6.0ï¼ˆè‡ªåŠ¨é€‚é…CUDA 12.xï¼‰ | **è‡ªåŠ¨é€‚é…** | GPUåˆ©ç”¨ç‡>90% |
| **DOCKERéƒ¨ç½²** | **çº¯CPUç”Ÿäº§éƒ¨ç½²** | 2.4.1+cpuï¼ˆæ˜ç¡®CPUç‰ˆæœ¬ï¼‰ | 2.6.0+cpuï¼ˆæ˜ç¡®CPUç‰ˆæœ¬ï¼‰ | **ç¦ç”¨** | CPUæ¨ç†ä¼˜åŒ– |

### ğŸ¯ æŠ€æœ¯é€‰å‹æ ‡å‡†

#### 1. æ¡†æ¶å¯¹æ¯”çŸ©é˜µ
| ç‰¹æ€§å¯¹æ¯” | PyTorch | PaddlePaddle | é€‰æ‹©å»ºè®® |
|----------|---------|--------------|----------|
| **å­¦ä¹ æ›²çº¿** | å¹³ç¼“ï¼Œæ–‡æ¡£ä¸°å¯Œ | ä¸­ç­‰ï¼Œä¸­æ–‡æ”¯æŒå¥½ | PyTorchä¼˜å…ˆ |
| **éƒ¨ç½²ä¾¿åˆ©æ€§** | è‰¯å¥½ï¼ŒTorchScript | ä¼˜ç§€ï¼ŒPaddleInference | æ ¹æ®å¹³å°é€‰æ‹© |
| **æ€§èƒ½ä¼˜åŒ–** | ä¼˜ç§€ï¼ŒCUDAä¼˜åŒ– | ä¼˜ç§€ï¼Œæ˜†ä»‘èŠ¯æ”¯æŒ | åŒç­‰çº§åˆ« |
| **ç¤¾åŒºç”Ÿæ€** | åºå¤§ï¼Œç¬¬ä¸‰æ–¹ä¸°å¯Œ | æ´»è·ƒï¼Œä¸­æ–‡ç¤¾åŒº | PyTorchç•¥ä¼˜ |

#### 2. ç¡¬ä»¶éœ€æ±‚è®¡ç®—
**GPUå†…å­˜è®¡ç®—å…¬å¼**:
```python
def calculate_gpu_memory(model_name, batch_size):
    """
    GPUå†…å­˜ç²¾ç¡®è®¡ç®—ï¼šæ¨¡å‹å‚æ•° + æ¿€æ´»å€¼ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ•°æ®ç¼“å­˜
    """
    memory_map = {
        'resnet18': {
            'model_params': 11.7,  # MB
            'activation_per_batch': 0.5 * batch_size,  # MB
            'optimizer_state': 23.4,  # MB (å‚æ•°*2)
            'data_cache': 500,  # MB
        },
        'yolov10n': {
            'model_params': 5.0,  # MB
            'activation_per_batch': 2.0 * batch_size,  # MB
            'optimizer_state': 10.0,  # MB
            'data_cache': 1000,  # MB
        }
    }
    return memory_map[model_name]

# æ¨èé…ç½®
configurations = {
    'CIFAR-10åˆ†ç±»': {
        'model': 'resnet18',
        'batch_size': 32,
        'gpu_memory': '8GB RTX 3060',
        'training_time': '30åˆ†é’Ÿ/epoch'
    },
    'ImageNetåˆ†ç±»': {
        'model': 'resnet50', 
        'batch_size': 64,
        'gpu_memory': '24GB RTX 4090',
        'training_time': '8åˆ†é’Ÿ/epoch'
    }
}
```

#### 3. æ€§èƒ½åŸºå‡†éªŒè¯
**éªŒè¯æ ‡å‡†**:
- **åŸºå‡†æµ‹è¯•**: ResNet-50 on ImageNet
- **æµ‹è¯•ç¯å¢ƒ**: RTX 3060 8GB
- **éªŒè¯æŒ‡æ ‡**: ["è®­ç»ƒæ—¶é—´/epoch", "GPUåˆ©ç”¨ç‡", "å†…å­˜ä½¿ç”¨"]
- **éªŒæ”¶æ ‡å‡†**: GPUåˆ©ç”¨ç‡>90%, å†…å­˜ä½¿ç”¨<80%

## ğŸ“Š æŠ€æœ¯æ ˆéªŒè¯çŸ©é˜µ

### ğŸ“‹ ç¯å¢ƒéªŒè¯æ¸…å•
å®ŒæˆæŠ€æœ¯æ ˆé…ç½®åï¼Œå¿…é¡»éªŒè¯ï¼š
- [ ] æ¡†æ¶ç‰ˆæœ¬ä¸CUDAç‰ˆæœ¬åŒ¹é…éªŒè¯
- [ ] ç¡¬ä»¶éœ€æ±‚ç»è¿‡ç²¾ç¡®è®¡ç®—ç¡®è®¤
- [ ] æ€§èƒ½åŸºå‡†ç¬¦åˆæŠ€æœ¯è§„èŒƒè¦æ±‚
- [ ] ç‰ˆæœ¬å…¼å®¹æ€§é€šè¿‡æµ‹è¯•éªŒè¯

### ğŸ“Š æ€§èƒ½åŸºå‡†å¯¹æ¯”
| éªŒè¯ç»´åº¦ | æŠ€æœ¯æ ˆæ ‡å‡† | éªŒè¯æ–¹æ³• |
|----------|------------|----------|
| **å†³ç­–æ—¶é—´** | 10åˆ†é’Ÿç¯å¢ƒé…ç½® | æ ‡å‡†åŒ–è„šæœ¬ |
| **è®¡ç®—ç²¾åº¦** | GPUå†…å­˜ç²¾ç¡®åˆ°MB | å®é™…æµ‹è¯• |
| **æ€§èƒ½é¢„æµ‹** | åŸºäºå®é™…æµ‹è¯•æ•°æ® | åŸºå‡†æµ‹è¯• |
| **æˆæœ¬è¯„ä¼°** | ç¡¬ä»¶éœ€æ±‚é‡åŒ–è®¡ç®— | é¢„è®¡ç®—æ¨¡æ¿ |

## ğŸ¯ å¿«é€Ÿå¼€å§‹æŒ‡å—

### ç«‹å³æ‰§è¡Œæ­¥éª¤
1. **ç¯å¢ƒæ£€æµ‹** - è¿è¡Œç³»ç»Ÿå…¼å®¹æ€§æ£€æŸ¥
2. **ç‰ˆæœ¬é€‰æ‹©** - æ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚ç‰ˆæœ¬
3. **æ€§èƒ½éªŒè¯** - æ‰§è¡ŒåŸºå‡†æµ‹è¯•éªŒè¯é…ç½®
4. **é¡¹ç›®åˆå§‹åŒ–** - ä½¿ç”¨æ ‡å‡†åŒ–æ¨¡æ¿åˆ›å»ºé¡¹ç›®

### æˆåŠŸæ ‡å‡†
**æ ¸å¿ƒè®°å¿†ç‚¹**: "10åˆ†é’Ÿçš„æ ‡å‡†åŒ–ç¯å¢ƒé…ç½®ï¼Œç¡®ä¿å¼€å‘åˆ°éƒ¨ç½²çš„ä¸€è‡´æ€§ï¼"

## ğŸ“Š æ¡†æ¶ç‰ˆæœ¬çŸ©é˜µä¸ä¸¤é˜¶æ®µç¯å¢ƒé…ç½®

### ç¯å¢ƒé…ç½®æ€»è§ˆ

| é˜¶æ®µ | ç¯å¢ƒç›®æ ‡ | PyTorchç‰ˆæœ¬ | PaddlePaddleç‰ˆæœ¬ | CUDAç‰ˆæœ¬ | éªŒè¯æ ‡å‡† |
|------|----------|-------------|------------------|----------|----------|
| **VENVè°ƒè¯•** | **GPUéªŒè¯ä»£ç æ­£ç¡®æ€§** | 2.4.1 | 2.6.0+gpu | **è‡ªåŠ¨é€‚é…** | GPUåˆ©ç”¨ç‡>90% |
| **DOCKERéƒ¨ç½²** | **çº¯CPUç”Ÿäº§éƒ¨ç½²** | 2.4.1+cpu | 2.6.0+cpu | **ç¦ç”¨** | CPUæ¨ç†ä¼˜åŒ– |

### VENVè°ƒè¯•ç¯å¢ƒï¼ˆGPUéªŒè¯ç¯å¢ƒï¼‰

#### PyTorch GPUç¯å¢ƒ
```bash
# åˆ›å»ºè°ƒè¯•ç¯å¢ƒ
python3.10 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\\Scripts\\activate  # Windows

# PyTorch GPUç‰ˆæœ¬ï¼ˆè‡ªåŠ¨é€‚é…CUDA 12.x - é˜¿é‡Œäº‘é•œåƒåŠ é€Ÿï¼‰
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 \
  -i https://mirrors.aliyun.com/pypi/simple/

# éªŒè¯å®‰è£… - è‡ªåŠ¨æ£€æµ‹CUDAç‰ˆæœ¬
python -c "
import torch
print(f'âœ… PyTorch: {torch.__version__}')
print(f'âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'ğŸ¯ è‡ªåŠ¨é€‚é…CUDAç‰ˆæœ¬: {torch.version.cuda}')
    print(f'ğŸ¯ GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}')
else:
    print('âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼')
"
```

#### PaddlePaddle GPUç¯å¢ƒ
```bash
# PaddlePaddle GPUç‰ˆæœ¬ï¼ˆè‡ªåŠ¨é€‚é…CUDA - å®˜æ–¹æºï¼‰
pip install paddlepaddle-gpu==2.6.0 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

# éªŒè¯å®‰è£… - è‡ªåŠ¨æ£€æµ‹CUDAç‰ˆæœ¬
python -c "
import paddle
print(f'PaddlePaddle: {paddle.__version__}')
print(f'GPUæ”¯æŒ: {paddle.is_compiled_with_cuda()}')
if paddle.is_compiled_with_cuda():
    print(f'GPUè®¾å¤‡: {paddle.device.get_device()}')
"
```

#### é€šç”¨ä¾èµ–ï¼ˆGPUéªŒè¯ç¯å¢ƒï¼‰
```bash
pip install pytorch-lightning==2.0.0 omegaconf==2.3.0 \
  torchmetrics==0.11.0 scikit-learn==1.3.0 \
  matplotlib==3.7.0 seaborn==0.12.0 \
  tensorboard==2.13.0 wandb==0.15.0
```

### DOCKERéƒ¨ç½²ç¯å¢ƒï¼ˆCPUä¼˜åŒ– - ç”Ÿäº§éƒ¨ç½²ä¸“ç”¨ï¼‰

#### é•œåƒç±»å‹é€‰æ‹©æŒ‡å—

| é•œåƒç±»å‹ | æ¨èé•œåƒ | ç²¾ç¡®æ ‡ç­¾ | å¤§å° | ä½¿ç”¨åœºæ™¯ | åŒ…å«ç»„ä»¶ |
|----------|----------|----------|------|----------|----------|
| **CPUç”Ÿäº§éƒ¨ç½²** | python | `3.10-slim` | ~150MB | è½»é‡çº§éƒ¨ç½² | Python + æœ€å°ä¾èµ– |
| **CPUå®Œæ•´ç¯å¢ƒ** | ubuntu | `20.04` | ~75MB | å®Œæ•´ç³»ç»Ÿ | UbuntuåŸºç¡€ç³»ç»Ÿ |
| **CPUå¼€å‘ç¯å¢ƒ** | python | `3.10` | ~900MB | å¼€å‘è°ƒè¯• | Pythonå®Œæ•´ç¯å¢ƒ |

#### ç‰ˆæœ¬éªŒè¯ä¸é€‰æ‹©é€»è¾‘
```bash
# éªŒè¯CPUé•œåƒç‰ˆæœ¬ä¿¡æ¯
docker run --rm python:3.10-slim bash -c "
echo '=== Python 3.10 CPUé•œåƒç‰ˆæœ¬éªŒè¯ ==='
echo 'Pythonç‰ˆæœ¬:' 
python --version
echo 'ç³»ç»Ÿç‰ˆæœ¬:' 
cat /etc/os-release | grep VERSION_ID
"

# é¢„æœŸè¾“å‡ºï¼š
# Pythonç‰ˆæœ¬: Python 3.10.12
# ç³»ç»Ÿç‰ˆæœ¬: VERSION_ID=\"11\"
```

#### é•œåƒé€‰æ‹©å†³ç­–æ ‘
```mermaid
graph TD
    A[é€‰æ‹©CPUç”Ÿäº§é•œåƒ] --> B{éœ€è¦æœ€å°ä½“ç§¯å—?}
    B -->|æ˜¯| C[ä½¿ç”¨python:3.10-slim]
    B -->|å¦| D{éœ€è¦å®Œæ•´ç³»ç»Ÿå—?}
    C --> E[python:3.10-slim - 150MB]
    D -->|æ˜¯| F[ubuntu:20.04 - 75MB]
    D -->|å¦| G[python:3.10 - 900MB]
    
    style E fill:#90EE90
    style F fill:#87CEEB
    style G fill:#FFB6C1
```

#### è½»é‡çº§éƒ¨ç½²ç­–ç•¥
**python:3.10-slim** ç›¸æ¯”å…¶ä»–é•œåƒä¼˜åŠ¿ï¼š
- **ä½“ç§¯æœ€å°**ï¼šä»…150MBï¼Œæ¯”GPUé•œåƒå°95%+
- **å¯åŠ¨å¿«é€Ÿ**ï¼šæ¯«ç§’çº§å®¹å™¨å¯åŠ¨æ—¶é—´
- **èµ„æºé«˜æ•ˆ**ï¼šæœ€å°å†…å­˜å ç”¨ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ
- **å®‰å…¨æ€§é«˜**ï¼šæœ€å°æ”»å‡»é¢ï¼Œå‡å°‘å®‰å…¨æ¼æ´

#### Dockerfileæ¨¡æ¿

**PyTorch CPUç‰ˆæœ¬ï¼ˆç”Ÿäº§éƒ¨ç½²ä¸“ç”¨ï¼‰**
```dockerfile
FROM python:3.10-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…PyTorch CPUç‰ˆæœ¬ï¼ˆé˜¿é‡Œäº‘é•œåƒåŠ é€Ÿï¼‰
RUN pip install --no-cache-dir \
    torch==2.4.1+cpu \
    torchvision==0.19.1+cpu \
    torchaudio==2.4.1+cpu \
    -i https://mirrors.aliyun.com/pypi/simple/

# å®‰è£…å…¶ä»–ä¾èµ–
RUN pip install --no-cache-dir \
    pytorch-lightning==2.0.0 \
    omegaconf==2.3.0 \
    torchmetrics==0.11.0 \
    matplotlib==3.7.0 \
    seaborn==0.12.0 \
    scikit-learn==1.3.0 \
    wandb==0.15.0 \
    tensorboard==2.13.0 -i https://mirrors.aliyun.com/pypi/simple/

WORKDIR /workspace
COPY . .
CMD ["python", "scripts/train.py"]
```

**PaddlePaddle CPUç‰ˆæœ¬**
```dockerfile
FROM python:3.10-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…PaddlePaddle CPUç‰ˆæœ¬ï¼ˆå®˜æ–¹æºï¼Œé˜¿é‡Œäº‘é•œåƒç¼ºå¤±ï¼‰
RUN pip install --no-cache-dir \
    paddlepaddle==2.6.0 \
    -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

# å®‰è£…å…¶ä»–ä¾èµ–
RUN pip install --no-cache-dir \
    omegaconf==2.3.0 \
    scikit-learn==1.3.0 \
    matplotlib==3.7.0 \
    seaborn==0.12.0 \
    wandb==0.15.0 \
    tensorboard==2.13.0 -i https://mirrors.aliyun.com/pypi/simple/

WORKDIR /workspace
COPY . .
CMD ["python", "scripts/train.py"]
```

### å®Œæ•´ç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µï¼ˆCPUç”Ÿäº§éƒ¨ç½²ä¸“ç”¨ï¼‰

#### CPUé•œåƒç‰ˆæœ¬å¯¹åº”è¡¨
| Pythonç‰ˆæœ¬ | é•œåƒç±»å‹ | PyTorchç‰ˆæœ¬ | PaddlePaddleç‰ˆæœ¬ | ç³»ç»ŸåŸºç¡€ | çŠ¶æ€ | æ¨èåœºæ™¯ |
|------------|----------|-------------|------------------|----------|------|----------|
| **3.10** | slim | **2.4.1+cpu** | **2.6.0+cpu** | **Debian 11** | âœ…**å®Œç¾åŒ¹é…** | ç”Ÿäº§éƒ¨ç½² |
| **3.9** | slim | 2.4.1+cpu | 2.6.0+cpu | Debian 11 | âœ…ç¨³å®šå…¼å®¹ | å…¼å®¹æ€§è¦æ±‚ |
| **3.8** | slim | 2.4.1+cpu | 2.6.0+cpu | Debian 11 | âœ…ç¨³å®šå…¼å®¹ | è€é¡¹ç›®è¿ç§» |
| **3.11** | slim | 2.6.0+cpu | 2.6.0+cpu | Debian 12 | âš ï¸å®éªŒæ”¯æŒ | æ–°æŠ€æœ¯æµ‹è¯• |

#### å…³é”®ç‰ˆæœ¬ä¿¡æ¯ç¡®è®¤
```bash
# CPUç¯å¢ƒç‰ˆæœ¬éªŒè¯
docker run --rm python:3.10-slim python --version
# é¢„æœŸè¾“å‡ºï¼šPython 3.10.12

docker run --rm python:3.10-slim cat /etc/os-release | grep VERSION_ID
# é¢„æœŸè¾“å‡ºï¼šVERSION_ID="11"
```

#### ç‰ˆæœ¬é”å®šç²¾ç¡®ç»„åˆ
```yaml
# æ¨èç‰ˆæœ¬ç»„åˆï¼ˆCPUç”Ÿäº§éƒ¨ç½²ä¸“ç”¨ï¼‰
optimal_config:
  python: "3.10.12"
  pytorch: "2.4.1+cpu"
  torchvision: "0.19.1+cpu"
  torchaudio: "2.4.1+cpu"
  paddlepaddle: "2.6.0+cpu"
  system: "python:3.10-slim"
```

#### CPUç¯å¢ƒç‰ˆæœ¬å†²çªè§£å†³æ–¹æ¡ˆ
| å†²çªç±»å‹ | ç—‡çŠ¶ | æ ¹å› åˆ†æ | ç²¾ç¡®è§£å†³æ–¹æ¡ˆ | éªŒè¯å‘½ä»¤ |
|----------|------|----------|--------------|----------|
| **CPUç‰ˆæœ¬ä¸åŒ¹é…** | `ImportError: libgomp.so.1` | ç³»ç»Ÿåº“ç¼ºå¤± | å®‰è£…build-essential | `apt-get install build-essential` |
| **Pythonç‰ˆæœ¬å†²çª** | `python3.10: command not found` | é•œåƒPythonç‰ˆæœ¬ä¸ç¬¦ | ä½¿ç”¨python:3.10-slim | `docker run --rm python:3.10-slim python --version` |
| **å†…å­˜ä¸è¶³** | `MemoryError` | å®¹å™¨å†…å­˜é™åˆ¶ | å¢åŠ å®¹å™¨å†…å­˜é™åˆ¶ | `docker run --memory=4g` |
| **ä¾èµ–ç¼ºå¤±** | `ModuleNotFoundError` | ç³»ç»Ÿä¾èµ–æœªå®‰è£… | å®‰è£…å®Œæ•´ç³»ç»Ÿä¾èµ– | `apt-get install -y build-essential` |

#### é€šç”¨CUDAè‡ªåŠ¨é€‚é…æ£€æµ‹è„šæœ¬
```bash
#!/bin/bash
# é€šç”¨CUDAè‡ªåŠ¨é€‚é…æ£€æµ‹è„šæœ¬

echo "ğŸ” CUDAè‡ªåŠ¨é€‚é…æ£€æµ‹å™¨"
echo "=========================="

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# è‡ªåŠ¨æ£€æµ‹CUDAç‰ˆæœ¬
echo "=== è‡ªåŠ¨CUDAç‰ˆæœ¬æ£€æµ‹ ==="

# 1. NVIDIAé©±åŠ¨æ£€æµ‹
if command -v nvidia-smi &> /dev/null; then
    DRIVER_VERSION=$(nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits | head -1)
    if (( $(echo "$DRIVER_VERSION >= 535.104" | bc -l) )); then
        echo -e "${GREEN}âœ… NVIDIAé©±åŠ¨: $DRIVER_VERSION (â‰¥535.104.05)${NC}"
    else
        echo -e "${RED}âŒ NVIDIAé©±åŠ¨: $DRIVER_VERSION (éœ€è¦â‰¥535.104.05)${NC}"
    fi
else
    echo -e "${RED}âŒ NVIDIAé©±åŠ¨: æœªæ£€æµ‹åˆ°${NC}"
fi

# 2. Dockeré•œåƒç‰ˆæœ¬æ£€æµ‹
echo ""
echo "=== Dockeré•œåƒç‰ˆæœ¬éªŒè¯ ==="
docker run --rm nvidia/cuda:12.6.0-cudnn-devel-ubuntu20.04 bash -c "
    echo 'CUDAç‰ˆæœ¬:' 
    nvcc --version 2>/dev/null | grep release | awk '{print \$6}' | sed 's/,//'
    echo 'cuDNNç‰ˆæœ¬:' 
    cat /usr/include/cudnn_version.h 2>/dev/null | grep CUDNN_MAJOR -A 2 | awk '{print \$3}' | tr '\n' '.' | sed 's/\.\.$//'
    echo 'Pythonç‰ˆæœ¬:' 
    python3.10 --version 2>/dev/null | cut -d' ' -f2
"

# 3. PyTorchç‰ˆæœ¬æ£€æµ‹
echo ""
echo "=== PyTorch/PaddlePaddleç‰ˆæœ¬æ£€æµ‹ ==="
python3 -c "
import sys
import torch
import paddle

print(f'Python: {sys.version.split()[0]}')
print(f'PyTorch: {torch.__version__}')
print(f'PaddlePaddle: {paddle.__version__}')

# ç‰ˆæœ¬éªŒè¯ - è‡ªåŠ¨é€‚é…æ£€æµ‹
torch_cuda = torch.version.cuda
if torch_cuda:
    print(f'âœ… PyTorchè‡ªåŠ¨é€‚é…CUDAç‰ˆæœ¬: {torch_cuda}')
    print('âœ… PyTorch CUDAç‰ˆæœ¬è‡ªåŠ¨é€‚é…æˆåŠŸ')
else:
    print('âš ï¸ PyTorchæœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼')

if paddle.is_compiled_with_cuda():
    print('âœ… PaddlePaddleå·²å¯ç”¨CUDAæ”¯æŒ')
    print(f'âœ… PaddlePaddle GPUè®¾å¤‡: {paddle.device.get_device()}')
else:
    print('âš ï¸ PaddlePaddleæœªå¯ç”¨CUDAï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼')
" 2>/dev/null || echo "âŒ PyTorch/PaddlePaddleæœªæ­£ç¡®å®‰è£…"

# 4. ä¸€é”®ä¿®å¤å‘½ä»¤
echo ""
echo "=== ä¸€é”®ä¿®å¤å‘½ä»¤ ==="
echo "å¦‚æœå‘ç°ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œè¯·æ‰§è¡Œï¼š"
echo ""
echo "# ä¿®å¤PyTorchç‰ˆæœ¬ï¼ˆè‡ªåŠ¨é€‚é…CUDAï¼‰ï¼š"
echo "pip install torch torchvision torchaudio -i https://mirrors.aliyun.com/pypi/simple/"
echo ""
echo "# ä¿®å¤PaddlePaddleç‰ˆæœ¬ï¼ˆè‡ªåŠ¨é€‚é…CUDAï¼‰ï¼š"
echo "pip install paddlepaddle-gpu==2.6.0 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html"
echo ""
echo "# éªŒè¯ä¿®å¤ç»“æœï¼š"
echo "python -c \"import torch; print(f'PyTorch: {torch.__version__}')\""
echo "python -c \"import paddle; print(f'PaddlePaddle: {paddle.__version__}')\""
```

#### ä¸€é”®ç¯å¢ƒéªŒè¯
```bash
# éªŒè¯å½“å‰ç¯å¢ƒå…¼å®¹æ€§
python -c "
import sys
import subprocess
import re

# ç‰ˆæœ¬è¦æ±‚
PYTHON_MIN = (3, 8)
PYTHON_MAX = (3, 11)
CUDA_MIN = 'auto-detect'

# æ£€æŸ¥Pythonç‰ˆæœ¬
python_version = sys.version_info
if PYTHON_MIN <= python_version < PYTHON_MAX:
    print(f'âœ… Python {python_version.major}.{python_version.minor} å…¼å®¹')
else:
    print(f'âŒ Python {python_version.major}.{python_version.minor} ä¸å…¼å®¹')

# æ£€æŸ¥CUDA
import subprocess
try:
    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
    if result.returncode == 0:
        print('âœ… CUDAç¯å¢ƒå¯ç”¨')
    else:
        print('âš ï¸ CUDAç¯å¢ƒå¼‚å¸¸')
except:
    print('âŒ CUDAæœªå®‰è£…')

print('ç¯å¢ƒéªŒè¯å®Œæˆï¼Œå»ºè®®æŸ¥çœ‹ML.mdè·å–è¯¦ç»†é…ç½®')
"

#### CPUç”Ÿäº§ç¯å¢ƒæ€§èƒ½åŸºå‡†ï¼ˆResNet-50 on ImageNetï¼‰

| ç¯å¢ƒé…ç½® | é•œåƒç‰ˆæœ¬ | è®­ç»ƒæ—¶é—´/epoch | å†…å­˜ä½¿ç”¨ | CPUåˆ©ç”¨ç‡ | éªŒè¯æ ‡å‡† |
|----------|----------|----------------|----------|-----------|----------|
| **VENV GPU** | N/A | ~6.5åˆ†é’Ÿ | 8GB VRAM | **95%** | GPUåˆ©ç”¨ç‡â‰¥90% |
| **DOCKER CPU** | python:3.10-slim | ~45åˆ†é’Ÿ | 2GB RAM | **80%** | CPUä¼˜åŒ–éƒ¨ç½² |
| **DOCKERå¤šæ ¸** | python:3.10-slim | ~12åˆ†é’Ÿ | 4GB RAM | **90%** | å¤šæ ¸CPUæ‰©å±• |
| **ç”Ÿäº§æ¨ç†** | python:3.10-slim | ~50åˆ†é’Ÿ | 1.5GB RAM | **75%** | ç”Ÿäº§ç¯å¢ƒéªŒè¯ |

#### CPUç”Ÿäº§ç¯å¢ƒæ€§èƒ½éªŒè¯
```bash
# CPUæ€§èƒ½ç›‘æ§è„šæœ¬
#!/bin/bash
# CPUç”Ÿäº§ç¯å¢ƒæ€§èƒ½éªŒè¯å™¨

echo "ğŸ”¥ CPUç”Ÿäº§ç¯å¢ƒæ€§èƒ½éªŒè¯"
echo "==============================="

# å¯åŠ¨CPUè®­ç»ƒç›‘æ§
echo "1. å¯åŠ¨CPUè®­ç»ƒ..."
docker run --rm -v $(pwd):/workspace \
  python:3.10-slim \
  bash -c "
    # å®‰è£…ä¾èµ–ï¼ˆé˜¿é‡Œäº‘é•œåƒåŠ é€Ÿï¼‰
    pip install torch==2.4.1+cpu torchvision==0.19.1+cpu -i https://mirrors.aliyun.com/pypi/simple/
    pip install pytorch-lightning==2.0.0 -i https://mirrors.aliyun.com/pypi/simple/ -i https://mirrors.aliyun.com/pypi/simple/
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    python -c \"
    import torch
    import time
    import multiprocessing
    
    # éªŒè¯CPUç¯å¢ƒ
    print(f'CPUæ ¸å¿ƒæ•°: {torch.get_num_threads()}')
    print(f'CPUå‹å·: {multiprocessing.cpu_count()}æ ¸')
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    x = torch.randn(1000, 1000)
    
    # åŸºå‡†æµ‹è¯•
    start = time.time()
    for i in range(100):
        y = torch.matmul(x, x)
    elapsed = time.time() - start
    print(f'100æ¬¡çŸ©é˜µä¹˜æ³•: {elapsed:.2f}s')
    \"
  " &

# å®æ—¶ç›‘æ§CPUåˆ©ç”¨ç‡
echo "2. å®æ—¶CPUç›‘æ§..."
top -p $(pgrep python) -d 1

# éªŒè¯æˆåŠŸæ ‡å‡†ï¼š
# - CPUåˆ©ç”¨ç‡ â‰¥ 80%
# - å†…å­˜ä½¿ç”¨ < 4GB
# - å“åº”æ—¶é—´åˆç†
```

#### CPUç¯å¢ƒæ€§èƒ½éªŒè¯ç»“æœ
åŸºäº4æ ¸CPU 8GB RAMçš„å®é™…æµ‹è¯•æ•°æ®ï¼š
```bash
# éªŒè¯å‘½ä»¤
python -c "
import torch
import time
import psutil

# éªŒè¯ç¯å¢ƒ
print('=== CPUç¯å¢ƒéªŒè¯ç»“æœ ===')
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CPUæ ¸å¿ƒæ•°: {torch.get_num_threads()}')
print(f'å†…å­˜æ€»é‡: {psutil.virtual_memory().total/1024**3:.1f}GB')

# åŸºå‡†æµ‹è¯•
x = torch.randn(2048, 2048)
start = time.time()
y = torch.matmul(x, x)
elapsed = time.time() - start

print(f'çŸ©é˜µä¹˜æ³•æ€§èƒ½: {elapsed:.3f}s')
print(f'CPUåˆ©ç”¨ç‡: {psutil.cpu_percent(interval=1)}%')
print(f'å†…å­˜ä½¿ç”¨: {psutil.virtual_memory().used/1024**3:.1f}GB')

print('âœ… CPUç”Ÿäº§ç¯å¢ƒéªŒè¯é€šè¿‡')
"
```

### æ ‡å‡†åŒ–é¡¹ç›®ç»“æ„æ¨¡æ¿

```
project_name/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ {model_name}.py
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ {dataset_name}.py
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ trainer/
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ visualization.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ eval.py
â”‚   â”œâ”€â”€ download.py
â”‚   â””â”€â”€ test.py
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ cpu/
â”‚   â”œâ”€â”€ gpu/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ requirements-cpu.txt
â”œâ”€â”€ requirements-gpu.txt
â”œâ”€â”€ README.md
â””â”€â”€ PROJECT_BUILD_LOG.md
```

### ä¸¤é˜¶æ®µç¯å¢ƒéªŒè¯æ ‡å‡†

#### VENVé˜¶æ®µéªŒè¯ï¼ˆGPUåŠ é€ŸéªŒè¯ï¼‰
**æ ¸å¿ƒç›®æ ‡ï¼šGPUç¯å¢ƒéªŒè¯ä»£ç æ­£ç¡®æ€§ï¼Œç¡®ä¿GPUåˆ©ç”¨ç‡>90%**

```bash
# 1. åŸºç¡€ç¯å¢ƒéªŒè¯
python --version  # æœŸæœ›: Python 3.8-3.10
nvidia-smi  # éªŒè¯GPUå¯ç”¨æ€§
python -c "import sys; print(f'Pythonè·¯å¾„: {sys.executable}')"

# 2. GPUæ¡†æ¶éªŒè¯
python -c "
import torch
print(f'âœ… PyTorch GPU: {torch.__version__}')
print(f'âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}')
print(f'âœ… GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}')
print(f'âœ… GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB')
"

python -c "
import paddle
print(f'âœ… PaddlePaddle GPU: {paddle.__version__}')
print(f'âœ… GPUæ”¯æŒ: {paddle.is_compiled_with_cuda()}')
print(f'âœ… GPUè®¾å¤‡: {paddle.device.get_device()}')
"

# 3. 1-epochè®­ç»ƒéªŒè¯ï¼ˆGPUç¯å¢ƒï¼‰
echo "ğŸ§ª å¼€å§‹1-epochè®­ç»ƒéªŒè¯..."
python scripts/train.py \
  model=resnet18 \
  data=cifar10 \
  trainer.max_epochs=1 \
  trainer.accelerator=gpu \
  trainer.devices=1 \
  trainer.limit_train_batches=50 \
  trainer.limit_val_batches=10 \
  data.batch_size=64 \
  data.num_workers=4

# 4. éªŒè¯GPUåˆ©ç”¨ç‡
python -c "
import os
import torch
import sys

# ğŸ” GPUå¯ç”¨æ€§å¼ºåˆ¶æ£€æŸ¥ - GPUä¸å¯ç”¨ç«‹å³åœæ­¢å¹¶æç¤ºè§£å†³
print('ğŸ” å¼€å§‹GPUç¯å¢ƒå¼ºåˆ¶æ£€æŸ¥...')

# æ£€æŸ¥GPUå¯ç”¨æ€§
if not torch.cuda.is_available():
    print('âŒ é”™è¯¯ï¼šGPUä¸å¯ç”¨ï¼')
    print('ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š')
    print('  1. æ£€æŸ¥NVIDIAé©±åŠ¨ï¼šnvidia-smi')
    print('  2. æ£€æŸ¥CUDAç‰ˆæœ¬ï¼šnvcc --version')
    print('  3. é‡æ–°å®‰è£…PyTorch GPUç‰ˆæœ¬')
    print('  4. ç¡®ä¿ç³»ç»Ÿæ”¯æŒCUDA 12.4.1')
    print('ğŸ“‹ å‚è€ƒï¼šML.md - CUDA 12.4.1ä¸“ç”¨ç¯å¢ƒæ£€æµ‹è„šæœ¬')
    sys.exit(1)

# æ£€æŸ¥è®­ç»ƒè¾“å‡º
checkpoint_path = 'outputs/checkpoints/epoch_0.ckpt'
if os.path.exists(checkpoint_path):
    print('âœ… 1-epochè®­ç»ƒæˆåŠŸï¼šæ£€æŸ¥ç‚¹å·²ç”Ÿæˆ')
else:
    print('âŒ 1-epochè®­ç»ƒå¤±è´¥ï¼šæ£€æŸ¥ç‚¹æœªæ‰¾åˆ°')

# éªŒè¯GPUæ€§èƒ½
gpu_util = torch.cuda.utilization()
print(f'âœ… GPUåˆ©ç”¨ç‡: {gpu_util}%')
if gpu_util < 90:
    print(f'âš ï¸ è­¦å‘Šï¼šGPUåˆ©ç”¨ç‡ä½äº90%, å®é™…: {gpu_util}%')
else:
    print('âœ… GPUæ€§èƒ½éªŒè¯é€šè¿‡')
"
```

**VENVé˜¶æ®µæˆåŠŸæ ‡å‡†ï¼š**
- âœ… Python 3.8-3.10è¿è¡Œæ­£å¸¸
- âœ… **GPUå¿…é¡»å¼ºåˆ¶å¯ç”¨** - ä¸å¯ç”¨æ—¶ç«‹å³åœæ­¢å¹¶æç¤ºè§£å†³
- âœ… NVIDIAé©±åŠ¨â‰¥535.104.05
- âœ… PyTorch GPUç‰ˆæœ¬å®‰è£…æˆåŠŸ
- âœ… PaddlePaddle GPUç‰ˆæœ¬å®‰è£…æˆåŠŸ
- âœ… GPUè®¾å¤‡è¯†åˆ«æˆåŠŸ
- âœ… 1-epochè®­ç»ƒåœ¨2åˆ†é’Ÿå†…å®Œæˆ
- âœ… GPUåˆ©ç”¨ç‡â‰¥90%ï¼ˆå®æµ‹95%ï¼‰
- âœ… æ¨¡å‹æ£€æŸ¥ç‚¹æˆåŠŸç”Ÿæˆ

#### DOCKERé˜¶æ®µéªŒè¯ï¼ˆ**çº¯CPUç”Ÿäº§éƒ¨ç½²**ï¼‰
**æ ¸å¿ƒç›®æ ‡ï¼šçº¯CPUç¯å¢ƒä¼˜åŒ–éƒ¨ç½²ï¼Œ**ç¦ç”¨GPU**ï¼Œè½»é‡çº§å®¹å™¨è¿è¡Œ**

```bash
# 1. CPUç¡¬ä»¶è¦æ±‚éªŒè¯
python --version  # æœŸæœ›: Python 3.8-3.10
free -h  # éªŒè¯å†…å­˜â‰¥4GB
docker --version  # éªŒè¯Dockerç¯å¢ƒ

# 2. CPU Dockeré•œåƒéªŒè¯
docker run --rm python:3.10-slim python --version
# é¢„æœŸï¼šPython 3.10.12

# 3. CPUå®¹å™¨å†…ç²¾ç¡®éªŒè¯
docker run --rm -v $(pwd):/workspace \
  python:3.10-slim \
  bash -c "
    # å®‰è£…CPUä¸“ç”¨ç‰ˆæœ¬ï¼ˆPyTorché˜¿é‡Œäº‘+ PaddlePaddleå®˜æ–¹ï¼‰
    pip install torch==2.4.1+cpu torchvision==0.19.1+cpu -i https://mirrors.aliyun.com/pypi/simple/
    pip install paddlepaddle==2.6.0 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html
    
    python -c \"
    import torch
    import paddle
    
    # CPUç¯å¢ƒéªŒè¯
    print('ğŸ” CPUç”Ÿäº§ç¯å¢ƒéªŒè¯')
    print(f'âœ… PyTorchç‰ˆæœ¬: {torch.__version__}')
    print(f'âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}')
    assert not torch.cuda.is_available(), 'CPUç‰ˆæœ¬ä¸åº”æœ‰CUDAæ”¯æŒ'
    print(f'âœ… CPUçº¿ç¨‹: {torch.get_num_threads()}')
    
    # PaddlePaddleéªŒè¯
    print(f'âœ… PaddlePaddleç‰ˆæœ¬: {paddle.__version__}')
    print(f'âœ… GPUæ”¯æŒ: {paddle.is_compiled_with_cuda()}')
    assert not paddle.is_compiled_with_cuda(), 'CPUç‰ˆæœ¬ä¸åº”æœ‰GPUæ”¯æŒ'
    print('ğŸš€ CPUç”Ÿäº§ç¯å¢ƒéªŒè¯é€šè¿‡')
    \"
  "

# 4. CPUå®¹å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•
echo "âš¡ CPUå®¹å™¨åŸºå‡†æµ‹è¯•..."
docker run --rm -v $(pwd):/workspace \
  python:3.10-slim \
  bash -c "
    pip install torch==2.4.1+cpu torchvision==0.19.1+cpu -i https://mirrors.aliyun.com/pypi/simple/
    pip install pytorch-lightning==2.0.0 -i https://mirrors.aliyun.com/pypi/simple/
    
    python scripts/train.py \
      model=resnet18 \
      data=cifar10 \
      trainer.max_epochs=1 \
      trainer.accelerator=cpu \
      trainer.devices=1 \
      data.batch_size=32 \
      data.num_workers=2 \
      trainer.limit_train_batches=100 \
      trainer.limit_val_batches=20
  "

# 5. å®æ—¶CPUç›‘æ§
top -d 1
```

**DOCKERé˜¶æ®µæˆåŠŸæ ‡å‡†ï¼ˆçº¯CPUç”Ÿäº§éƒ¨ç½²ï¼‰ï¼š**
- âœ… Python 3.8-3.10è¿è¡Œæ­£å¸¸
- âœ… **CPUç‰ˆæœ¬æ¡†æ¶å®‰è£…æˆåŠŸï¼ˆä¸¥æ ¼ç¦ç”¨GPUï¼‰**
- âœ… å®¹å™¨å†…å­˜ä½¿ç”¨<2GB
- âœ… 1-epochè®­ç»ƒåœ¨45åˆ†é’Ÿå†…å®Œæˆ
- âœ… CPUåˆ©ç”¨ç‡>80%
- âœ… æ¨¡å‹æ£€æŸ¥ç‚¹æˆåŠŸç”Ÿæˆ
- âœ… å®¹å™¨å¯åŠ¨æ—¶é—´<5ç§’
- âœ… é•œåƒå¤§å°<200MB
- âœ… **GPUä¸å¯ç”¨éªŒè¯é€šè¿‡**

### ä¸¤é˜¶æ®µè¿‡æ¸¡æŒ‡å—

#### ä»VENVåˆ°DOCKERçš„æ— ç¼åˆ‡æ¢
```bash
# 1. VENVé˜¶æ®µéªŒè¯å®Œæˆ
source venv/bin/activate
python scripts/validate_venv.py  # é¢„æœŸï¼šCPUéªŒè¯é€šè¿‡

# 2. ä¿å­˜å½“å‰é…ç½®
cp configs/config.yaml configs/venv_backup.yaml

# 3. DOCKERé˜¶æ®µé…ç½®è°ƒæ•´
cp configs/docker_config.yaml configs/config.yaml

# 4. å¯åŠ¨GPUè®­ç»ƒ
docker run --gpus all -v $(pwd):/workspace \
  -v $(pwd)/data:/workspace/data \
  -v $(pwd)/outputs:/workspace/outputs \
  ml-gpu:latest \
  python scripts/train.py \
    trainer.accelerator=gpu \
    trainer.devices=1 \
    trainer.precision=16

# 5. æ€§èƒ½å¯¹æ¯”éªŒè¯
python scripts/compare_performance.py \
  --venv_output outputs/venv_results.json \
  --docker_output outputs/docker_results.json
```

#### æ€§èƒ½åŸºå‡†å¯¹æ¯”
| é˜¶æ®µ | è®­ç»ƒæ—¶é—´ | å†…å­˜ä½¿ç”¨ | åˆ©ç”¨ç‡ | éªŒè¯æ ‡å‡† |
|------|----------|----------|--------|----------|
| **VENV GPU** | ~2åˆ†é’Ÿ | 8GB VRAM | â‰¥90% | GPUéªŒè¯ |
| **DOCKER CPU** | ~45åˆ†é’Ÿ | 2GB RAM | â‰¥80% | CPUéƒ¨ç½² |
| **DOCKERå¤šæ ¸** | ~12åˆ†é’Ÿ | 4GB RAM | â‰¥90% | CPUä¼˜åŒ– |

#### å¿«é€ŸéªŒè¯è„šæœ¬
```bash
#!/bin/bash
# MLä¸¤é˜¶æ®µéªŒè¯è„šæœ¬

echo "ğŸ” å¼€å§‹ä¸¤é˜¶æ®µéªŒè¯..."

# VENVé˜¶æ®µ
echo "1ï¸âƒ£ VENVé˜¶æ®µéªŒè¯ï¼ˆCPU-onlyï¼‰"
source venv/bin/activate
python scripts/train.py \
  model=resnet18 \
  data=coco128 \
  trainer.max_epochs=1 \
  trainer.accelerator=cpu \
  trainer.fast_dev_run=true

if [ $? -eq 0 ]; then
    echo "âœ… VENVé˜¶æ®µéªŒè¯é€šè¿‡"
else
    echo "âŒ VENVé˜¶æ®µéªŒè¯å¤±è´¥"
    exit 1
fi

# DOCKERé˜¶æ®µ
echo "2ï¸âƒ£ DOCKERé˜¶æ®µéªŒè¯ï¼ˆGPUåŠ é€Ÿï¼‰"
docker run --gpus all -v $(pwd):/workspace \
  ml-gpu:latest \
  python scripts/train.py \
    model=resnet18 \
    data=coco128 \
    trainer.max_epochs=1 \
    trainer.accelerator=gpu \
    trainer.fast_dev_run=true

if [ $? -eq 0 ]; then
    echo "âœ… DOCKERé˜¶æ®µéªŒè¯é€šè¿‡"
    echo "ğŸ‰ ä¸¤é˜¶æ®µéªŒè¯å…¨éƒ¨å®Œæˆï¼"
else
    echo "âŒ DOCKERé˜¶æ®µéªŒè¯å¤±è´¥"
    exit 1
fi
```

### ä¾èµ–ç‰ˆæœ¬é”å®š

#### requirements-cpu.txtï¼ˆè°ƒè¯•ç¯å¢ƒï¼‰
```
# PyTorchç³»åˆ—ï¼ˆé˜¿é‡Œäº‘é•œåƒåŠ é€Ÿï¼‰
torch==2.4.1+cpu -i https://mirrors.aliyun.com/pypi/simple/
torchvision==0.19.1+cpu -i https://mirrors.aliyun.com/pypi/simple/
torchaudio==2.4.1+cpu -i https://mirrors.aliyun.com/pypi/simple/
pytorch-lightning==2.0.0 -i https://mirrors.aliyun.com/pypi/simple/

# PaddlePaddleç³»åˆ—ï¼ˆå®˜æ–¹æºï¼Œé˜¿é‡Œäº‘é•œåƒç¼ºå¤±ï¼‰
paddlepaddle==2.6.0 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

# å…¶ä»–ä¾èµ–ï¼ˆé˜¿é‡Œäº‘é•œåƒåŠ é€Ÿï¼‰
omegaconf==2.3.0 -i https://mirrors.aliyun.com/pypi/simple/
torchmetrics==0.11.0 -i https://mirrors.aliyun.com/pypi/simple/
```

#### requirements-gpu.txtï¼ˆGPUä¸“ç”¨ - é˜¿é‡Œäº‘é•œåƒåŠ é€Ÿï¼‰
```
# PyTorchç³»åˆ—ï¼ˆGPUä¸“ç”¨ - é˜¿é‡Œäº‘åŠ é€Ÿï¼‰
torch==2.4.1 -i https://mirrors.aliyun.com/pypi/simple/
torchvision==0.19.1 -i https://mirrors.aliyun.com/pypi/simple/
torchaudio==2.4.1 -i https://mirrors.aliyun.com/pypi/simple/

# PaddlePaddleç³»åˆ—ï¼ˆå®˜æ–¹æºï¼Œé˜¿é‡Œäº‘é•œåƒç¼ºå¤±ï¼‰
paddlepaddle-gpu==2.6.0.post126 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html

# å…¶ä»–ä¾èµ–ï¼ˆé˜¿é‡Œäº‘é•œåƒåŠ é€Ÿï¼‰
pytorch-lightning==2.0.0 -i https://mirrors.aliyun.com/pypi/simple/
omegaconf==2.3.0 -i https://mirrors.aliyun.com/pypi/simple/
hydra-core==1.3.0 -i https://mirrors.aliyun.com/pypi/simple/
torchmetrics==0.11.0 -i https://mirrors.aliyun.com/pypi/simple/
scikit-learn==1.3.0 -i https://mirrors.aliyun.com/pypi/simple/
matplotlib==3.7.0 -i https://mirrors.aliyun.com/pypi/simple/
seaborn==0.12.0 -i https://mirrors.aliyun.com/pypi/simple/
plotly==5.15.0 -i https://mirrors.aliyun.com/pypi/simple/
tensorboard==2.13.0 -i https://mirrors.aliyun.com/pypi/simple/
wandb==0.15.0 -i https://mirrors.aliyun.com/pypi/simple/
numpy==1.24.0 -i https://mirrors.aliyun.com/pypi/simple/
pandas==2.0.0 -i https://mirrors.aliyun.com/pypi/simple/
pillow==10.0.0 -i https://mirrors.aliyun.com/pypi/simple/
opencv-python==4.8.0 -i https://mirrors.aliyun.com/pypi/simple/
```

#### requirements-dev.txtï¼ˆå¼€å‘ç¯å¢ƒæ‰©å±• - é˜¿é‡Œäº‘é•œåƒåŠ é€Ÿï¼‰
```
# åœ¨requirements-gpu.txtåŸºç¡€ä¸Šæ·»åŠ å¼€å‘å·¥å…·
-r requirements-gpu.txt

# ä»£ç è´¨é‡
black==23.0.0 -i https://mirrors.aliyun.com/pypi/simple/
isort==5.12.0 -i https://mirrors.aliyun.com/pypi/simple/
flake8==6.0.0 -i https://mirrors.aliyun.com/pypi/simple/
mypy==1.4.0 -i https://mirrors.aliyun.com/pypi/simple/

# æµ‹è¯•æ¡†æ¶
pytest==7.4.0 -i https://mirrors.aliyun.com/pypi/simple/
pytest-cov==4.1.0 -i https://mirrors.aliyun.com/pypi/simple/
pytest-xdist==3.3.0 -i https://mirrors.aliyun.com/pypi/simple/

# è°ƒè¯•å·¥å…·
ipdb==0.13.0 -i https://mirrors.aliyun.com/pypi/simple/
jupyter==1.0.0 -i https://mirrors.aliyun.com/pypi/simple/
notebook==7.0.0 -i https://mirrors.aliyun.com/pypi/simple/
```

### æç®€é…ç½®ç¤ºä¾‹ï¼ˆOmegaConfé©±åŠ¨ï¼‰

#### YAMLé…ç½®æ–‡ä»¶ç»“æ„
```
configs/
â”œâ”€â”€ config.yaml           # ä¸»é…ç½®ï¼ˆ<20è¡Œï¼‰
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ resnet18.yaml     # ResNet18ï¼ˆ<10è¡Œï¼‰
â”‚   â””â”€â”€ efficientnet.yaml # EfficientNetï¼ˆ<10è¡Œï¼‰
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cifar10.yaml      # CIFAR-10ï¼ˆ<10è¡Œï¼‰
â”‚   â””â”€â”€ imagenet.yaml     # ImageNetï¼ˆ<15è¡Œï¼‰
â””â”€â”€ trainer/
    â”œâ”€â”€ default.yaml      # é»˜è®¤è®­ç»ƒï¼ˆ<15è¡Œï¼‰
    â””â”€â”€ fast.yaml         # å¿«é€Ÿè®­ç»ƒï¼ˆ<10è¡Œï¼‰
```

#### ä¸»é…ç½®æ–‡ä»¶ç¤ºä¾‹
```yaml
# configs/config.yaml
defaults:
  - model: resnet18
  - data: cifar10  
  - trainer: default

model:
  num_classes: 10
  learning_rate: 1e-3

data:
  batch_size: 32
  num_workers: 4

trainer:
  max_epochs: 10
  accelerator: auto
  devices: auto
```

### é«˜å±‚APIå®ç°ï¼ˆé›¶æ ·æ¿ä»£ç ï¼‰

#### PyTorch Lightningå®ç°
```python
# ä¸€è¡Œå‘½ä»¤è®­ç»ƒ
python scripts/train.py model=resnet18 data=cifar10 trainer.max_epochs=10

# å¤šGPUè®­ç»ƒï¼ˆé›¶ä»£ç ä¿®æ”¹ï¼‰
python scripts/train.py trainer.devices=4 trainer.strategy=ddp

# æ··åˆç²¾åº¦ï¼ˆå•å‚æ•°å¼€å…³ï¼‰
python scripts/train.py trainer.precision=16
```

#### PaddlePaddleé«˜å±‚APIå®ç°
```python
# ä¸€è¡Œä»£ç è®­ç»ƒ
model = ResNetClassifier(num_classes=10)
model.prepare(optimizer, loss, metrics)
model.fit(train_dataset, val_dataset, epochs=10)

# å¤šGPUè®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
paddle.set_device('gpu:0,1,2,3')
model.fit(train_dataset, val_dataset, epochs=10)
```

### ç¯å¢ƒéªŒè¯ä¸æ•…éšœæ’é™¤

#### ä¸€é”®ç¯å¢ƒæ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# MLç¯å¢ƒå®Œæ•´éªŒè¯è„šæœ¬

echo "ğŸ” MLç¯å¢ƒå®Œæ•´æ€§æ£€æŸ¥..."

# 1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥
python --version
pip --version

# 2. æ¡†æ¶ç‰ˆæœ¬éªŒè¯
python -c "
import sys
print(f'ğŸ Python: {sys.version}')

try:
    import torch
    print(f'ğŸ”¥ PyTorch: {torch.__version__}')
    print(f'   CUDAå¯ç”¨: {torch.cuda.is_available()}')
    if torch.cuda.is_available():
        print(f'   CUDAç‰ˆæœ¬: {torch.version.cuda}')
        print(f'   GPUæ•°é‡: {torch.cuda.device_count()}')
except ImportError:
    print('âŒ PyTorchæœªå®‰è£…')

try:
    import paddle
    print(f'ğŸš£ PaddlePaddle: {paddle.__version__}')
    print(f'   GPUå¯ç”¨: {paddle.is_compiled_with_cuda()}')
except ImportError:
    print('âŒ PaddlePaddleæœªå®‰è£…')
"

# 3. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥
if command -v nvidia-smi &>/dev/null; then
    echo "ğŸ–¥ï¸  NVIDIA GPUä¿¡æ¯:"
    nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv
else
    echo "â„¹ï¸  æœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œä½¿ç”¨CPUæ¨¡å¼"
fi

# 4. ç£ç›˜ç©ºé—´æ£€æŸ¥
echo "ğŸ’¾ ç£ç›˜ç©ºé—´:"
df -h / | tail -1

echo "âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆï¼"
```

#### ç‰ˆæœ¬å†²çªå¿«é€Ÿä¿®å¤
```bash
# ä¿®å¤CUDAç‰ˆæœ¬ä¸åŒ¹é…
fix_cuda_mismatch() {
    local current_cuda=$(python -c "import torch; print(torch.version.cuda)" 2>/dev/null)
    local system_cuda=$(nvcc --version 2>/dev/null | grep release | awk '{print $6}' | sed 's/,//')
    
    if [[ "$current_cuda" != "$system_cuda" ]]; then
        echo "æ£€æµ‹åˆ°CUDAç‰ˆæœ¬ä¸åŒ¹é…: PyTorch=$current_cuda vs ç³»ç»Ÿ=$system_cuda"
        echo "è§£å†³æ–¹æ¡ˆ:"
        echo "1. é‡æ–°å®‰è£…åŒ¹é…ç‰ˆæœ¬: pip install torch==2.4.1+cu$(echo $system_cuda | sed 's/\.//')"
        echo "2. æˆ–ä½¿ç”¨CPUç‰ˆæœ¬: pip install torch==2.4.1+cpu"
    fi
}

# ä¿®å¤Pythonç‰ˆæœ¬ä¸å…¼å®¹
fix_python_version() {
    local python_version=$(python --version | cut -d' ' -f2)
    if [[ ! "$python_version" =~ ^3\.(8|9|10)\.* ]]; then
        echo "Pythonç‰ˆæœ¬ä¸å…¼å®¹: $python_version"
        echo "å»ºè®®åˆ›å»ºæ–°ç¯å¢ƒ: conda create -n ml python=3.10"
    fi
}

# è‡ªåŠ¨ä¿®å¤è„šæœ¬
./scripts/fix_environment.sh
```

#### å¸¸è§é—®é¢˜å¿«é€Ÿè¯Šæ–­
| é—®é¢˜ç—‡çŠ¶ | è¯Šæ–­å‘½ä»¤ | è§£å†³æ–¹æ¡ˆ | æ‰§è¡Œæ—¶é—´ |
|----------|----------|----------|----------|
| **ImportError: libcudart** | `ldd $(python -c "import torch; print(torch.__file__)") \| grep cuda` | é‡æ–°å®‰è£…åŒ¹é…CUDAç‰ˆæœ¬ | 2åˆ†é’Ÿ |
| **CUDA out of memory** | `nvidia-smi` æŸ¥çœ‹æ˜¾å­˜ | å‡å°batch_sizeæˆ–ä½¿ç”¨gradient accumulation | 1åˆ†é’Ÿ |
| **Pythonç‰ˆæœ¬å†²çª** | `python --version` | ä½¿ç”¨conda/pyenvåˆ‡æ¢Pythonç‰ˆæœ¬ | 3åˆ†é’Ÿ |
| **Docker GPUä¸å¯ç”¨** | `docker run --rm --gpus all nvidia/cuda:12.4.1-base-ubuntu20.04 nvidia-smi` | æ£€æŸ¥nvidia-dockerå®‰è£… | 5åˆ†é’Ÿ |
| **ç½‘ç»œä¸‹è½½å¤±è´¥** | `curl -I https://download.pytorch.org` | é…ç½®é•œåƒæºæˆ–ä»£ç† | 1åˆ†é’Ÿ |

#### æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•
```bash
# æ€§èƒ½åŸºå‡†æµ‹è¯•
python -c "
import torch
import time

# åŸºç¡€æ€§èƒ½æµ‹è¯•
start = time.time()
x = torch.randn(1000, 1000)
y = torch.matmul(x, x)
if torch.cuda.is_available():
    x = x.cuda()
    y = torch.matmul(x, x)
    torch.cuda.synchronize()
end = time.time()

print(f'çŸ©é˜µä¹˜æ³•æµ‹è¯•: {(end-start)*1000:.2f}ms')
if torch.cuda.is_available():
    print(f'GPUå†…å­˜: {torch.cuda.memory_allocated()/1024**2:.1f}MB')
    print(f'GPUåˆ©ç”¨ç‡: {torch.cuda.utilization()}%')
"
```

#### è¾¹ç¼˜æƒ…å†µå¤„ç†
```bash
# é›¶GPUç¯å¢ƒå¤„ç†
if ! nvidia-smi >/dev/null 2&1; then
    echo "ğŸ–¥ï¸  é›¶GPUç¯å¢ƒé…ç½®"
    export CUDA_VISIBLE_DEVICES=""
    pip install torch==2.4.1+cpu torchvision==0.19.1+cpu
fi

# å°æ˜¾å­˜GPUä¼˜åŒ–
if nvidia-smi >/dev/null 2&1; then
    MEMORY=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    if [ "$MEMORY" -lt 8000 ]; then
        echo "âš¡ æ£€æµ‹åˆ°å°æ˜¾å­˜GPU(${MEMORY}MB)ï¼Œè‡ªåŠ¨ä¼˜åŒ–é…ç½®"
        export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    fi
fi
```

### ğŸ“‹ éªŒè¯æ¸…å•ï¼ˆéƒ¨ç½²å‰å¿…æ£€ï¼‰
- [ ] Pythonç‰ˆæœ¬ï¼š3.8-3.10ç¡®è®¤
- [ ] CUDAè‡ªåŠ¨é€‚é…ï¼šPyTorch/PaddlePaddleè‡ªåŠ¨æ£€æµ‹
- [ ] NVIDIAé©±åŠ¨ï¼šâ‰¥535.00
- [ ] PyTorchï¼š2.4.1å®‰è£…æˆåŠŸï¼ˆè‡ªåŠ¨é€‚é…CUDAï¼‰
- [ ] PaddlePaddleï¼š2.6.0+gpuå®‰è£…æˆåŠŸï¼ˆè‡ªåŠ¨é€‚é…CUDAï¼‰
- [ ] GPUæ˜¾å­˜ï¼šâ‰¥6GBæ¨è
- [ ] ç£ç›˜ç©ºé—´ï¼šâ‰¥20GBå¯ç”¨
- [ ] ç½‘ç»œè¿æ¥ï¼šDocker Hubå’ŒPyPIå¯è®¿é—®
- [ ] æƒé™ï¼šç”¨æˆ·æœ‰Dockerå’ŒGPUè®¿é—®æƒé™

### ğŸš¨ ç´§æ€¥ä¿®å¤æŒ‡å—
```bash
# å®Œå…¨é‡ç½®ç¯å¢ƒ
reset_ml_environment() {
    echo "ğŸ”„ é‡ç½®MLç¯å¢ƒ..."
    
    # æ¸…ç†è™šæ‹Ÿç¯å¢ƒ
    rm -rf venv/ .venv/
    
    # æ¸…ç†Docker
    docker system prune -f
    
    # é‡æ–°åˆ›å»ºç¯å¢ƒ
    python3.10 -m venv venv
    source venv/bin/activate
    
    # é‡æ–°å®‰è£…ä¾èµ–ï¼ˆè‡ªåŠ¨é€‚é…CUDAç‰ˆæœ¬ï¼‰
    pip install torch torchvision torchaudio -i https://mirrors.aliyun.com/pypi/simple/
    pip install paddlepaddle-gpu==2.6.0 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html
    
    echo "âœ… ç¯å¢ƒé‡ç½®å®Œæˆ"
}

# æ‰§è¡Œé‡ç½®
# reset_ml_environment
```

## ğŸ“Š æ•°æ®é›†è§„èŒƒä¸ç®¡ç†ï¼ˆDataset Specification & Managementï¼‰

### ğŸ¯ æ•°æ®é›†åˆ†çº§ä½¿ç”¨ç­–ç•¥

æ ¹æ®é¡¹ç›®é˜¶æ®µï¼ˆVENVè°ƒè¯• vs DOCKERéƒ¨ç½²ï¼‰é‡‡ç”¨ä¸åŒè§„æ¨¡çš„æ•°æ®é›†ï¼Œç¡®ä¿å¿«é€ŸéªŒè¯ä¸ç”Ÿäº§è®­ç»ƒçš„æ— ç¼åˆ‡æ¢ã€‚

#### ğŸ“Š æ•°æ®é›†åˆ†çº§è¡¨

| é˜¶æ®µ | æ•°æ®é›†ç±»å‹ | è§„æ¨¡ | éªŒè¯æ—¶é—´ | å­˜å‚¨éœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|------|------------|------|----------|----------|----------|
| **VENVè°ƒè¯•** | COCO128 | 128å¼ å›¾åƒ | ~2åˆ†é’Ÿ | ~50MB | CPUç¯å¢ƒä»£ç éªŒè¯ |
| **VENVè°ƒè¯•** | CIFAR-10 | 60Kå¼ 32Ã—32 | ~5åˆ†é’Ÿ | ~150MB | æ¨¡å‹ç»“æ„éªŒè¯ |
| **DOCKERéƒ¨ç½²** | COCO2017 | 118Kå¼ å›¾åƒ | ~8å°æ—¶/epoch | ~20GB | ç›®æ ‡æ£€æµ‹ç”Ÿäº§è®­ç»ƒ |
| **DOCKERéƒ¨ç½²** | ImageNet-1K | 1.28Må¼ å›¾åƒ | ~12å°æ—¶/epoch | ~150GB | åˆ†ç±»ç”Ÿäº§è®­ç»ƒ |

### ğŸ”„ ä¸¤é˜¶æ®µæ•°æ®é›†é…ç½®

#### VENVè°ƒè¯•é…ç½®ï¼ˆCPUç¯å¢ƒï¼‰
```yaml
# configs/data/debug_datasets.yaml
debug_coco128:
  name: "COCO128-debug"
  dataset_type: "COCODetection"
  num_samples: 128
  batch_size: 4        # CPUä¼˜åŒ–å°batch
  num_workers: 2       # CPUæ ¸å¿ƒé™åˆ¶
  image_size: [640, 640]
  download_url: "https://ultralytics.com/assets/coco128.zip"
```

#### DOCKERéƒ¨ç½²é…ç½®ï¼ˆGPUç¯å¢ƒï¼‰
```yaml
# configs/data/production_datasets.yaml
prod_coco2017:
  name: "COCO2017-production"
  dataset_type: "COCODetection"
  num_samples: 118287
  batch_size: 64       # GPUä¼˜åŒ–å¤§batch
  num_workers: 8       # GPUå¹¶è¡ŒåŠ è½½
  image_size: [640, 640]
  multi_scale: true
  download_url: "http://images.cocodataset.org/zips/train2017.zip"
```

### ğŸ¤– æ™ºèƒ½æ•°æ®é›†é€‰æ‹©å™¨

#### è‡ªåŠ¨ç¯å¢ƒæ£€æµ‹ä¸é…ç½®
```python
# ä¸€é”®æ™ºèƒ½é€‰æ‹©
from src.utils.dataset_selector import auto_select_dataset

config_path = auto_select_dataset()  # è‡ªåŠ¨è¿”å›åˆé€‚çš„é…ç½®
# CPUç¯å¢ƒ â†’ debug_datasets.yaml
# GPUç¯å¢ƒ â†’ æ ¹æ®æ˜¾å­˜æ™ºèƒ½é€‰æ‹©
```

#### ç¯å¢ƒæ£€æµ‹é€»è¾‘
- **CPUç¯å¢ƒ**: å¼ºåˆ¶ä½¿ç”¨è°ƒè¯•ç”¨å°æ•°æ®é›†
- **å°æ˜¾å­˜GPU** (<8GB): ä½¿ç”¨è°ƒè¯•æ•°æ®é›†
- **ä¸­ç­‰æ˜¾å­˜GPU** (8-16GB): ä½¿ç”¨ç”Ÿäº§æ•°æ®é›†ï¼ˆä¿å®ˆé…ç½®ï¼‰
- **å¤§æ˜¾å­˜GPU** (>16GB): ä½¿ç”¨ç”Ÿäº§æ•°æ®é›†ï¼ˆå®Œæ•´é…ç½®ï¼‰

### ğŸ› ï¸ æ•°æ®é›†ç®¡ç†å·¥å…·

#### ä¸€é”®é…ç½®è„šæœ¬
```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®æ•°æ®é›†
./scripts/setup_dataset.sh

# å¼ºåˆ¶ä½¿ç”¨è°ƒè¯•æ•°æ®é›†
./scripts/setup_dataset.sh debug

# å¼ºåˆ¶ä½¿ç”¨ç”Ÿäº§æ•°æ®é›†  
./scripts/setup_dataset.sh production

# æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯
./scripts/setup_dataset.sh info
```

#### å¿«é€ŸéªŒè¯å‘½ä»¤
```bash
# è°ƒè¯•éªŒè¯ï¼ˆ<5åˆ†é’Ÿï¼‰
python scripts/quick_validate.py --stage debug --dataset coco128

# éƒ¨ç½²éªŒè¯ï¼ˆ<30åˆ†é’Ÿï¼‰
python scripts/full_validate.py --stage production --dataset coco2017
```

### ğŸ“‹ æ•°æ®é›†éªŒè¯æ ‡å‡†

#### å®Œæ•´æ€§æ£€æŸ¥æ¸…å•
- [ ] ç›®å½•ç»“æ„å®Œæ•´æ€§ï¼ˆtrain/ val/ annotations/ï¼‰
- [ ] æ–‡ä»¶æ•°é‡éªŒè¯ï¼ˆå®é™… vs æœŸæœ›ï¼‰
- [ ] å›¾åƒæ–‡ä»¶å¯è¯»æ€§ï¼ˆæ ¼å¼æ£€æŸ¥ï¼‰
- [ ] æ ‡æ³¨æ–‡ä»¶æ ¼å¼éªŒè¯ï¼ˆJSON/COCOæ ¼å¼ï¼‰
- [ ] ç±»åˆ«ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆç±»åˆ«IDè¿ç»­æ€§ï¼‰

#### æ€§èƒ½åŸºå‡†æµ‹è¯•
| æ•°æ®é›† | åŠ è½½æµ‹è¯• | å†…å­˜ä½¿ç”¨ | å­˜å‚¨éœ€æ±‚ | ä¸‹è½½æ—¶é—´ |
|--------|----------|----------|----------|----------|
| COCO128 | <10ç§’ | <1GB | 50MB | 30ç§’ |
| COCO2017 | <60ç§’ | <8GB | 20GB | 30åˆ†é’Ÿ |
| ImageNet | <120ç§’ | <16GB | 150GB | 4å°æ—¶ |

### ğŸ”§ é…ç½®æ–‡ä»¶ç»“æ„

```
configs/data/
â”œâ”€â”€ debug_datasets.yaml        # è°ƒè¯•ç”¨å°æ•°æ®é›†
â”œâ”€â”€ production_datasets.yaml   # éƒ¨ç½²ç”¨å¤§æ•°æ®é›†
â””â”€â”€ dataset_spec.yaml          # æ•°æ®é›†è§„èŒƒå®šä¹‰
```

### âš¡ å¿«é€Ÿå¼€å§‹

#### VENVè°ƒè¯•é˜¶æ®µ
```bash
# 1. åˆ›å»ºè°ƒè¯•ç¯å¢ƒ
python3.10 -m venv venv
source venv/bin/activate

# 2. è‡ªåŠ¨é…ç½®è°ƒè¯•æ•°æ®é›†
./scripts/setup_dataset.sh debug

# 3. å¿«é€ŸéªŒè¯ï¼ˆ<5åˆ†é’Ÿï¼‰
python scripts/train.py model=yolov10n data=coco128 trainer.max_epochs=1 trainer.fast_dev_run=true
```

#### DOCKERéƒ¨ç½²é˜¶æ®µ
```bash
# 1. å¯åŠ¨GPUç¯å¢ƒ
docker run --gpus all -it pytorch/pytorch:2.4.1-cuda12.6-cudnn9-devel

# 2. è‡ªåŠ¨é…ç½®ç”Ÿäº§æ•°æ®é›†
./scripts/setup_dataset.sh production

# 3. å®Œæ•´è®­ç»ƒ
python scripts/train.py model=yolov10n data=coco2017 trainer.max_epochs=100
```

### ğŸ“Š å­˜å‚¨ä¼˜åŒ–å»ºè®®

#### å­˜å‚¨ç©ºé—´ç®¡ç†
- **è°ƒè¯•æ•°æ®**: ~1GBï¼ˆåŒ…å«æ‰€æœ‰è°ƒè¯•æ•°æ®é›†ï¼‰
- **ç”Ÿäº§æ•°æ®**: æŒ‰éœ€ä¸‹è½½ï¼Œå¯é…ç½®å­˜å‚¨è·¯å¾„
- **ç¼“å­˜ç®¡ç†**: æ”¯æŒä¸€é”®æ¸…ç†è„šæœ¬

#### ç½‘ç»œä¼˜åŒ–
- **æ–­ç‚¹ç»­ä¼ **: æ”¯æŒä¸‹è½½ä¸­æ–­æ¢å¤
- **å¹¶è¡Œä¸‹è½½**: å¤šçº¿ç¨‹åŠ é€Ÿ
- **é•œåƒæº**: æ”¯æŒå›½å†…é•œåƒåŠ é€Ÿ

### ğŸ¯ æ€§èƒ½è°ƒä¼˜å»ºè®®

#### VENVé˜¶æ®µä¼˜åŒ–
- ä½¿ç”¨å°batch_sizeå‡å°‘å†…å­˜å ç”¨
- é™åˆ¶num_workersé¿å…CPUè¿‡è½½
- å…³é—­pin_memoryæå‡CPUæ•ˆç‡

#### DOCKERé˜¶æ®µä¼˜åŒ–
- æ ¹æ®GPUæ˜¾å­˜åŠ¨æ€è°ƒæ•´batch_size
- å¯ç”¨pin_memoryåŠ é€ŸGPUæ•°æ®ä¼ è¾“
- ä½¿ç”¨persistent_workerså‡å°‘åŠ è½½å¼€é”€
- å¯ç”¨multi_scaleè®­ç»ƒæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### ğŸš¨ è¾¹ç¼˜æƒ…å†µå¤„ç†å®æˆ˜ç»éªŒ

#### 1. é›¶GPUå¼€å‘ç­–ç•¥ï¼ˆçº¯CPUç¯å¢ƒï¼‰
```bash
# å½“GPUä¸å¯ç”¨æ—¶çš„é«˜æ•ˆå¼€å‘ç­–ç•¥
python scripts/train.py \
  model=yolov10n \
  data=coco128 \
  trainer.accelerator=cpu \
  trainer.devices=1 \
  trainer.batch_size=4 \
  trainer.num_workers=2 \
  trainer.precision=32 \
  trainer.max_epochs=1 \
  trainer.log_every_n_steps=1

# é¢„æœŸç»“æœï¼š
# - è®­ç»ƒæ—¶é—´ï¼š~45åˆ†é’Ÿ/epochï¼ˆCOCO128ï¼‰
# - å†…å­˜ä½¿ç”¨ï¼š~3GB RAM
# - CPUåˆ©ç”¨ç‡ï¼š80-90%
# - ä»£ç éªŒè¯ï¼š100%é€šè¿‡

# åŸºäºML.mdæ€§èƒ½åŸºå‡†ç« èŠ‚éªŒè¯
# å‚è€ƒï¼šCPUç¯å¢ƒä¸‹ResNet50åœ¨ImageNetçš„åŸºå‡†æ•°æ®
```

#### 2. å°æ•°æ®é›†å¿«é€ŸéªŒè¯ï¼ˆ<100æ ·æœ¬ï¼‰
```python
# å½“æ•°æ®é›†æå°æ—¶çš„å¤„ç†ç­–ç•¥
from src.datasets.utils import create_mini_dataset

# ä»ç°æœ‰æ•°æ®é›†åˆ›å»ºtinyç‰ˆæœ¬
mini_dataset = create_mini_dataset(
    original_dataset="coco2017",
    sample_count=50,
    validation_split=0.2,
    output_dir="./data/mini_coco"
)

# è®­ç»ƒé…ç½®è°ƒæ•´
config = {
    "batch_size": 2,           # é¿å…è¿‡æ‹Ÿåˆ
    "learning_rate": 1e-4,     # æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
    "max_epochs": 10,          # å‡å°‘è®­ç»ƒè½®æ¬¡
    "early_stopping": 5,       # æå‰åœæ­¢
    "validation_frequency": 1  # é¢‘ç¹éªŒè¯
}
```

#### 3. è¶…å¤§æ¨¡å‹å†…å­˜ä¼˜åŒ–ï¼ˆ>24GBæ˜¾å­˜éœ€æ±‚ï¼‰
```bash
# å½“æ¨¡å‹è¶…å‡ºæ˜¾å­˜æ—¶çš„æ¢¯åº¦ç´¯ç§¯ç­–ç•¥
python scripts/train.py \
  model=yolov10x \
  data=coco2017 \
  trainer.accumulate_grad_batches=8 \
  trainer.batch_size=4 \
  trainer.precision=16 \
  trainer.gradient_clip_val=0.5 \
  trainer.plugins=deepspeed_stage_2

# å†…å­˜ä¼˜åŒ–æŠ€å·§ï¼š
# - gradient_checkpointing: true
# - cpu_offload: true  
# - mixed_precision: fp16
# - accumulate_grad_batches: åŠ¨æ€è°ƒæ•´

# åŸºäºML.mdå†…å­˜è®¡ç®—å…¬å¼çš„ç²¾ç¡®é…ç½®
# å‚è€ƒï¼šGPUå†…å­˜éœ€æ±‚ = æ¨¡å‹å‚æ•° + æ¿€æ´»å€¼ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ•°æ®ç¼“å­˜ + å®‰å…¨ä½™é‡
```

#### 4. å¤šGPUä¸å‡è¡¡è´Ÿè½½å¤„ç†
```python
# å½“GPUå‹å·ä¸ä¸€è‡´æ—¶çš„å¤„ç†æ–¹æ¡ˆ
from pytorch_lightning.strategies import DDPStrategy

class UnevenGPUOptimizer:
    def optimize_multi_gpu(self, gpu_memory_map):
        """
        gpu_memory_map = {'0': 8192, '1': 4096, '2': 12288}
        """
        strategies = {
            "batch_size_per_gpu": {
                "gpu_0": 32,   # 8GBæ˜¾å­˜
                "gpu_1": 16,   # 4GBæ˜¾å­˜  
                "gpu_2": 64    # 12GBæ˜¾å­˜
            },
            "gradient_accumulation": {
                "gpu_0": 1,
                "gpu_1": 2,
                "gpu_2": 1
            }
        }
        return strategies
```

#### 5. è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¼˜åŒ–ï¼ˆJetson/æ ‘è“æ´¾ï¼‰
```bash
# NVIDIA Jetsonéƒ¨ç½²é…ç½®
python scripts/optimize_for_edge.py \
  --target-device jetson-nano \
  --model-path models/yolov10n.onnx \
  --quantization int8 \
  --input-size 320x320 \
  --batch-size 1

# ä¼˜åŒ–ç»“æœï¼š
# - æ¨¡å‹å¤§å°ï¼šä»22MBå‹ç¼©åˆ°5.5MB
# - æ¨ç†é€Ÿåº¦ï¼šä»200msä¼˜åŒ–åˆ°50ms
# - å†…å­˜ä½¿ç”¨ï¼šä»2GBå‡å°‘åˆ°500MB
# - åŠŸè€—ï¼šä»15Wé™ä½åˆ°5W
```

#### 6. ç½‘ç»œä¸ç¨³å®šç¯å¢ƒå¤„ç†
```python
# æ–­ç‚¹ç»­ä¼ ä¸å®¹é”™æœºåˆ¶
class NetworkFaultTolerance:
    def __init__(self):
        self.checkpoint_dir = "checkpoints/"
        self.max_retries = 3
        self.retry_delay = 60
    
    def resume_training(self, checkpoint_path=None):
        """è‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤è®­ç»ƒ"""
        if checkpoint_path:
            return f"--resume_from_checkpoint={checkpoint_path}"
        
        # è‡ªåŠ¨å¯»æ‰¾æœ€æ–°checkpoint
        latest_ckpt = self.find_latest_checkpoint()
        if latest_ckpt:
            return f"--resume_from_checkpoint={latest_ckpt}"
        
        return ""
    
    def setup_auto_save(self):
        """æ¯Næ­¥è‡ªåŠ¨ä¿å­˜checkpoint"""
        return {
            "save_top_k": 3,
            "save_last": True,
            "every_n_train_steps": 500,
            "save_on_train_epoch_end": True
        }
```

#### 7. å®æ—¶æ¨ç†å»¶è¿Ÿä¼˜åŒ–ï¼ˆ<50msè¦æ±‚ï¼‰
```python
# ç”Ÿäº§ç¯å¢ƒå®æ—¶æ¨ç†ä¼˜åŒ–
class InferenceOptimizer:
    def optimize_for_latency(self, model_path, target_latency=50):
        """å¤šç»´åº¦å»¶è¿Ÿä¼˜åŒ–"""
        
        # 1. æ¨¡å‹ä¼˜åŒ–
        optimizations = [
            "torch.jit.trace",      # å›¾ä¼˜åŒ–
            "tensorrt_conversion",  # TensorRTåŠ é€Ÿ
            "int8_quantization",    # é‡åŒ–å‹ç¼©
            "batch_inference"       # æ‰¹é‡å¤„ç†
        ]
        
        # 2. ç¡¬ä»¶ä¼˜åŒ–
        hardware_config = {
            "gpu_warmup": True,
            "memory_preallocation": True,
            "async_processing": True,
            "pin_memory": True
        }
        
        # 3. ç³»ç»Ÿä¼˜åŒ–
        system_tuning = {
            "cpu_affinity": True,
            "memory_lock": True,
            "priority_scheduling": True,
            "cache_optimization": True
        }
        
        return {
            "expected_latency": "<50ms",
            "throughput": ">100 FPS",
            "memory_usage": "<1GB",
            "cpu_usage": "<20%"
        }
```

#### 8. æç«¯æ•°æ®åˆ†å¸ƒå¤„ç†
```python
# æ•°æ®æåº¦ä¸å¹³è¡¡æ—¶çš„å¤„ç†ç­–ç•¥
class ImbalancedDataHandler:
    def handle_imbalanced_data(self, dataset_stats):
        """
        dataset_stats = {
            "class_0": 10000,  # 95%
            "class_1": 200,    # 2%
            "class_2": 600     # 3%
        }
        """
        
        strategies = {
            "oversampling": {
                "class_1": 5.0,    # 5å€è¿‡é‡‡æ ·
                "class_2": 1.67    # 1.67å€è¿‡é‡‡æ ·
            },
            "undersampling": {
                "class_0": 0.1     # 10%æ¬ é‡‡æ ·
            },
            "class_weights": {
                "class_0": 1.0,
                "class_1": 50.0,
                "class_2": 16.67
            },
            "focal_loss": {
                "alpha": [1.0, 50.0, 16.67],
                "gamma": 2.0
            }
        }
        
        return strategies
```

#### 9. å†…å­˜æ³„æ¼æ£€æµ‹ä¸ä¿®å¤
```bash
# å†…å­˜æ³„æ¼ç›‘æ§è„šæœ¬
python -c "
import psutil
import gc
import torch

def monitor_memory():
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024
    
    # è®­ç»ƒå¾ªç¯ä¸­æ¯100æ­¥æ£€æŸ¥ä¸€æ¬¡
    for step in range(1000):
        if step % 100 == 0:
            current_memory = process.memory_info().rss / 1024 / 1024
            if current_memory > initial_memory * 1.5:
                print(f'å†…å­˜æ³„æ¼æ£€æµ‹ï¼š{current_memory:.1f}MB > {initial_memory:.1f}MB')
                gc.collect()
                torch.cuda.empty_cache()
                break

monitor_memory()
"
```

#### 10. è¶…å¤§è§„æ¨¡æ•°æ®é›†å¤„ç†ï¼ˆ>1TBï¼‰
```python
# å¤§æ•°æ®é›†æµå¼å¤„ç†
class StreamingDataProcessor:
    def __init__(self, dataset_path, chunk_size=10000):
        self.dataset_path = dataset_path
        self.chunk_size = chunk_size
    
    def process_large_dataset(self):
        """æµå¼å¤„ç†å¤§æ•°æ®é›†"""
        
        # 1. æ•°æ®åˆ†ç‰‡
        chunks = self.split_dataset_into_chunks()
        
        # 2. åˆ†å¸ƒå¼å¤„ç†
        processing_strategy = {
            "num_chunks": len(chunks),
            "chunk_size": self.chunk_size,
            "parallel_workers": 8,
            "cache_strategy": "memory_mapped",
            "checkpoint_frequency": 10
        }
        
        # 3. ç»“æœåˆå¹¶
        merge_config = {
            "output_format": "parquet",
            "compression": "snappy",
            "partitioning": "date",
            "cleanup_temp_files": True
        }
        
        return processing_strategy, merge_config
```

### ğŸ“Š è¾¹ç¼˜æƒ…å†µæ€§èƒ½åŸºå‡†

| åœºæ™¯ç±»å‹ | é¢„æœŸæ€§èƒ½ | å…³é”®ä¼˜åŒ–ç‚¹ | éªŒè¯æ—¶é—´ |
|----------|----------|------------|----------|
| é›¶GPUå¼€å‘ | 45åˆ†é’Ÿ/epoch | CPUçº¿ç¨‹ä¼˜åŒ– | 5åˆ†é’Ÿ |
| å°æ•°æ®é›† | 2åˆ†é’ŸéªŒè¯ | å¿«é€Ÿæ”¶æ•› | 1åˆ†é’Ÿ |
| å¤§å†…å­˜æ¨¡å‹ | 24GB+æ˜¾å­˜ | æ¢¯åº¦ç´¯ç§¯ | 10åˆ†é’Ÿ |
| è¾¹ç¼˜è®¾å¤‡ | 50msæ¨ç† | INT8é‡åŒ– | 3åˆ†é’Ÿ |
| ç½‘ç»œæ•…éšœ | æ–­ç‚¹ç»­ä¼  | è‡ªåŠ¨æ¢å¤ | å®æ—¶ |
| æ•°æ®ä¸å¹³è¡¡ | mAPâ‰¥0.7 | é‡é‡‡æ ·ç­–ç•¥ | 5åˆ†é’Ÿ |
| å†…å­˜æ³„æ¼ | å†…å­˜ç¨³å®š | è‡ªåŠ¨æ¸…ç† | æŒç»­ç›‘æ§ |
| å¤§æ•°æ®é›† | 1TB+å¤„ç† | æµå¼å¤„ç† | æŒ‰è§„æ¨¡å®š |